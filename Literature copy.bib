@online{alamaRayFrontsOpenSetSemantic2025,
  title = {{{RayFronts}}: {{Open-Set Semantic Ray Frontiers}} for {{Online Scene Understanding}} and {{Exploration}}},
  shorttitle = {{{RayFronts}}},
  author = {Alama, Omar and Bhattacharya, Avigyan and He, Haoyang and Kim, Seungchan and Qiu, Yuheng and Wang, Wenshan and Ho, Cherie and Keetha, Nikhil and Scherer, Sebastian},
  date = {2025-04-09},
  eprint = {2504.06994},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.06994},
  url = {http://arxiv.org/abs/2504.06994},
  urldate = {2025-12-11},
  abstract = {Open-set semantic mapping is crucial for open-world robots. Current mapping approaches either are limited by the depth range or only map beyond-range entities in constrained settings, where overall they fail to combine within-range and beyond-range observations. Furthermore, these methods make a trade-off between fine-grained semantics and efficiency. We introduce RayFronts, a unified representation that enables both dense and beyond-range efficient semantic mapping. RayFronts encodes task-agnostic open-set semantics to both in-range voxels and beyond-range rays encoded at map boundaries, empowering the robot to reduce search volumes significantly and make informed decisions both within \& beyond sensory range, while running at 8.84 Hz on an Orin AGX. Benchmarking the within-range semantics shows that RayFronts's fine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation performance while improving throughput by 16.5x. Traditionally, online mapping performance is entangled with other system components, complicating evaluation. We propose a planner-agnostic evaluation framework that captures the utility for online beyond-range search and exploration, and show RayFronts reduces search volume 2.2x more efficiently than the closest online baselines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/9YK7FELL/Alama et al. - 2025 - RayFronts Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/IUS9YCRU/2504.html}
}

@article{almadhounSurveyInspectingStructures2016,
  title = {A Survey on Inspecting Structures Using Robotic Systems},
  author = {Almadhoun, Randa and Taha, Tarek and Seneviratne, Lakmal and Dias, Jorge and Cai, Guowei},
  date = {2016-11-28},
  journaltitle = {International Journal of Advanced Robotic Systems},
  shortjournal = {International Journal of Advanced Robotic Systems},
  volume = {13},
  doi = {10.1177/1729881416663664},
  abstract = {Advancements in robotics and autonomous systems are being deployed nowadays in many application domains such as search and rescue, industrial automation, domestic services and healthcare. These systems are developed to tackle tasks in some of the most challenging, labour intensive and dangerous environments. Inspecting structures (e.g. bridges, buildings, ships, wind turbines and aircrafts) is considered a hard task for humans to perform and of critical importance since missing any details could affect the structure’s performance and integrity. Additionally, structure inspection is time and resource intensive and should be performed as efficiently and accurately as possible. Inspecting various structures has been reported in the literature using different robotic platforms to: inspect difficult to reach areas and detect various types of faults and anomalies. Typically, inspection missions involve performing three main tasks: coverage path planning, shape, model or surface reconstruction and the actual inspection of the structure. Coverage path planning ensures the generation of an optimized path that guarantees the complete coverage of the structure of interest in order to gather highly accurate information to be used for shape/model reconstruction. This article aims to provide an overview of the recent work and breakthroughs in the field of coverage path planning and model reconstruction, with focus on 3D reconstruction, for the purpose of robotic inspection.},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/UCI3I6EJ/Almadhoun et al. - 2016 - A survey on inspecting structures using robotic systems.pdf}
}

@inproceedings{bourgaultInformationBasedAdaptive2002,
  title = {Information {{Based Adaptive Robotic Exploration}}},
  author = {Bourgault, Frédéric and Makarenko, Alexei and Williams, Stefan and Grocholsky, Ben and Durrant-Whyte, Hugh},
  date = {2002-02-01},
  volume = {1},
  pages = {540-545 vol.1},
  doi = {10.1109/IRDS.2002.1041446},
  abstract = {Exploration involving mapping and concurrent localization in an unknown environment is a pervasive task in mobile robotics. In general, the accuracy of the mapping process depends directly on the accuracy of the localization process. This paper address the problem of maximizing the accuracy of the map building process during exploration by adaptively selecting control actions that maximize localisation accuracy. The map building and exploration task is modeled using an Occupancy Grid (OG) with concurrent localisation performed using a feature-based Simultaneous Localisation And Mapping (SLAM) algorithm. Adaptive sensing aims at maximizing the map information by simultaneously maximizing the expected Shannon information gain (Mutual Information) on the OG map and minimizing the uncertainty of the vehicle pose and map feature uncertainty in the SLAM process. The resulting map building system is demonstrated in an indoor environment using data from a laser scanner mounted on a mobile platform.},
  isbn = {978-0-7803-7398-3},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/C3L3R6FA/Bourgault et al. - 2002 - Information Based Adaptive Robotic Exploration.pdf}
}

@online{brohanRT2VisionLanguageActionModels2023,
  title = {{{RT-2}}: {{Vision-Language-Action Models Transfer Web Knowledge}} to {{Robotic Control}}},
  shorttitle = {{{RT-2}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and Florence, Pete and Fu, Chuyuan and Arenas, Montse Gonzalez and Gopalakrishnan, Keerthana and Han, Kehang and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ichter, Brian and Irpan, Alex and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Lisa and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Michalewski, Henryk and Mordatch, Igor and Pertsch, Karl and Rao, Kanishka and Reymann, Krista and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sermanet, Pierre and Singh, Jaspiar and Singh, Anikait and Soricut, Radu and Tran, Huong and Vanhoucke, Vincent and Vuong, Quan and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Wu, Jialin and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  date = {2023-07-28},
  eprint = {2307.15818},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.15818},
  url = {http://arxiv.org/abs/2307.15818},
  urldate = {2026-01-02},
  abstract = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Website: https://robotics-transformer.github.io/},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WF5428KW/Brohan et al. - 2023 - RT-2 Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/U3KCFQT5/2307.html}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2026-01-02},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 40+32 pages},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/VFZNX4TP/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/FJG4YT6J/2005.html}
}

@online{buschOneMapFind2025,
  title = {One {{Map}} to {{Find Them All}}: {{Real-time Open-Vocabulary Mapping}} for {{Zero-shot Multi-Object Navigation}}},
  shorttitle = {One {{Map}} to {{Find Them All}}},
  author = {Busch, Finn Lukas and Homberger, Timon and Ortega-Peimbert, Jesús and Yang, Quantao and Andersson, Olov},
  date = {2025-03-03},
  eprint = {2409.11764},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.11764},
  url = {http://arxiv.org/abs/2409.11764},
  urldate = {2025-12-11},
  abstract = {The capability to efficiently search for objects in complex environments is fundamental for many real-world robot applications. Recent advances in open-vocabulary vision models have resulted in semantically-informed object navigation methods that allow a robot to search for an arbitrary object without prior training. However, these zero-shot methods have so far treated the environment as unknown for each consecutive query. In this paper we introduce a new benchmark for zero-shot multi-object navigation, allowing the robot to leverage information gathered from previous searches to more efficiently find new objects. To address this problem we build a reusable open-vocabulary feature map tailored for real-time object search. We further propose a probabilistic-semantic map update that mitigates common sources of errors in semantic feature extraction and leverage this semantic uncertainty for informed multi-object exploration. We evaluate our method on a set of object navigation tasks in both simulation as well as with a real robot, running in real-time on a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art approaches both on single and multi-object navigation tasks. Additional videos, code and the multi-object navigation benchmark will be available on https://finnbsch.github.io/OneMap.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5TN5PZLW/Busch et al. - 2025 - One Map to Find Them All Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/U2WXYRMQ/2409.html}
}

@online{changMatterport3DLearningRGBD2017,
  title = {{{Matterport3D}}: {{Learning}} from {{RGB-D Data}} in {{Indoor Environments}}},
  shorttitle = {{{Matterport3D}}},
  author = {Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Nießner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
  date = {2017-09-18},
  eprint = {1709.06158},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1709.06158},
  url = {http://arxiv.org/abs/1709.06158},
  urldate = {2026-01-06},
  abstract = {Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/73GM9BQP/Chang et al. - 2017 - Matterport3D Learning from RGB-D Data in Indoor Environments.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/JGMFFWZP/1709.html}
}

@online{chaplotObjectGoalNavigation2020,
  title = {Object {{Goal Navigation}} Using {{Goal-Oriented Semantic Exploration}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
  date = {2020-07-02},
  eprint = {2007.00643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.00643},
  url = {http://arxiv.org/abs/2007.00643},
  urldate = {2025-12-11},
  abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Winner of the CVPR 2020 AI-Habitat Object Goal Navigation Challenge. See the project webpage at https://devendrachaplot.github.io/projects/semantic-exploration.html},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/S9CD9YI9/Chaplot et al. - 2020 - Object Goal Navigation using Goal-Oriented Semantic Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DGZ6KSBI/2007.html}
}

@online{chengYOLOWorldRealTimeOpenVocabulary2024,
  title = {{{YOLO-World}}: {{Real-Time Open-Vocabulary Object Detection}}},
  shorttitle = {{{YOLO-World}}},
  author = {Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
  date = {2024-02-22},
  eprint = {2401.17270},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.17270},
  url = {http://arxiv.org/abs/2401.17270},
  urldate = {2026-01-08},
  abstract = {The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Work still in progress. Code \& models are available at: https://github.com/AILab-CVC/YOLO-World},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/KSH65BFR/Cheng et al. - 2024 - YOLO-World Real-Time Open-Vocabulary Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/PUH7ALVF/2401.html}
}

@online{chenHowNotTrain2023,
  title = {How {{To Not Train Your Dragon}}: {{Training-free Embodied Object Goal Navigation}} with {{Semantic Frontiers}}},
  shorttitle = {How {{To Not Train Your Dragon}}},
  author = {Chen, Junting and Li, Guohao and Kumar, Suryansh and Ghanem, Bernard and Yu, Fisher},
  date = {2023-05-26},
  eprint = {2305.16925},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.16925},
  url = {http://arxiv.org/abs/2305.16925},
  urldate = {2025-12-11},
  abstract = {Object goal navigation is an important problem in Embodied AI that involves guiding the agent to navigate to an instance of the object category in an unknown environment -- typically an indoor scene. Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches, \textbackslash eg, end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are typically costly to train and difficult to debug, leading to a lack of transferability and explainability. Inspired by recent successes in combining classical and learning methods, we present a modular and training-free solution, which embraces more classic approaches, to tackle the object goal navigation problem. Our method builds a structured scene representation based on the classic visual simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based frontier exploration to reason about promising areas to search for a goal object. Our structured scene representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph. Our method propagates semantics on the scene graphs based on language priors and scene statistics to introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can reason about the most promising frontier to explore. The proposed pipeline shows strong experimental performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in the object navigation task.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Accepted by/To be published in Robotics: Science and Systems (RSS) 2023; 11 pages, 5 figures},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/RGE5D2PZ/Chen et al. - 2023 - How To Not Train Your Dragon Training-free Embodied Object Goal Navigation with Semantic Frontiers.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/K42WJCBZ/2305.html}
}

@online{chertiReproducibleScalingLaws2022,
  title = {Reproducible Scaling Laws for Contrastive Language-Image Learning},
  author = {Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  date = {2022-12-14},
  doi = {10.1109/CVPR52729.2023.00276},
  url = {https://arxiv.org/abs/2212.07143v2},
  urldate = {2026-01-08},
  abstract = {Scaling up neural networks has led to remarkable performance across a wide range of tasks. Moreover, performance often follows reliable scaling laws as a function of training set size, model size, and compute, which offers valuable guidance as large-scale experiments are becoming increasingly expensive. However, previous work on scaling laws has primarily used private data \textbackslash\& models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository. Our large-scale experiments involve models trained on up to two billion image-text pairs and identify power law scaling for multiple downstream tasks including zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. We find that the training distribution plays a key role in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling behavior despite identical model architectures and similar training recipes. We open-source our evaluation workflow and all models, including the largest public CLIP models, to ensure reproducibility and make scaling laws research more accessible. Source code and instructions to reproduce this study will be available at https://github.com/LAION-AI/scaling-laws-openclip},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/AT493AAL/Cherti et al. - 2022 - Reproducible scaling laws for contrastive language-image learning.pdf}
}

@article{colledanchiseImplementationBehaviorTrees2021,
  title = {On the {{Implementation}} of {{Behavior Trees}} in {{Robotics}}},
  author = {Colledanchise, Michele and Natale, Lorenzo},
  date = {2021-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {3},
  eprint = {2106.15227},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {5929--5936},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3087442},
  url = {http://arxiv.org/abs/2106.15227},
  urldate = {2026-01-19},
  abstract = {There is a growing interest in Behavior Trees (BTs) as a tool to describe and implement robot behaviors. BTs were devised in the video game industry and their adoption in robotics resulted in the development of ad-hoc libraries to design and execute BTs that fit complex robotics software architectures. While there is broad consensus on how BTs work, some characteristics rely on the implementation choices done in the specific software library used. In this letter, we outline practical aspects in the adoption of BTs and the solutions devised by the robotics community to fully exploit the advantages of BTs in real robots. We also overview the solutions proposed in open-source libraries used in robotics, we show how BTs fit in robotic software architecture, and we present a use case example.},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WWJGMZV6/Colledanchise and Natale - 2021 - On the Implementation of Behavior Trees in Robotics.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/YM7Q549Z/2106.html}
}

@book{corkeRoboticsVisionControl2023,
  title = {Robotics, {{Vision}} and {{Control}}: {{Fundamental Algorithms}} in {{Python}}},
  shorttitle = {Robotics, {{Vision}} and {{Control}}},
  author = {Corke, Peter},
  date = {2023},
  series = {Springer {{Tracts}} in {{Advanced Robotics}}},
  volume = {146},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-06469-2},
  url = {https://link.springer.com/10.1007/978-3-031-06469-2},
  urldate = {2026-01-18},
  isbn = {978-3-031-06468-5 978-3-031-06469-2},
  langid = {english},
  keywords = {Color,Control,Dynamics,Geometric Control Theory,Image Formation,Image Processing,Inertial,Kinematics,Localization,Mapping,Mobile Robots,Motion Control,Navigation,Optimal Control,Path Planning,Projection Models,Rigid Body Transformation,Robotics,Simultaneous Localization and Mapping,Trajectory}
}

@article{crouseImplementing2DRectangular2016,
  title = {On Implementing {{2D}} Rectangular Assignment Algorithms},
  author = {Crouse, David F.},
  date = {2016-08},
  journaltitle = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {52},
  number = {4},
  pages = {1679--1696},
  issn = {1557-9603},
  doi = {10.1109/TAES.2016.140952},
  url = {https://ieeexplore.ieee.org/document/7738348},
  urldate = {2026-01-11},
  abstract = {This paper reviews research into solving the two-dimensional (2D) rectangular assignment problem and combines the best methods to implement a k-best 2D rectangular assignment algorithm with bounded runtime. This paper condenses numerous results as an understanding of the "best" algorithm, a strong polynomial-time algorithm with a low polynomial order (a shortest augmenting path approach), would require assimilating information from many separate papers, each making a small contribution. 2D rectangular assignment Matlab code is provided.},
  keywords = {Approximation algorithms,Complexity theory,Cost function,MATLAB,Minimization,Two dimensional displays},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/APY2689T/7738348.html}
}

@online{daiScanNetRichlyannotated3D2017,
  title = {{{ScanNet}}: {{Richly-annotated 3D Reconstructions}} of {{Indoor Scenes}}},
  shorttitle = {{{ScanNet}}},
  author = {Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nießner, Matthias},
  date = {2017-04-11},
  eprint = {1702.04405},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1702.04405},
  url = {http://arxiv.org/abs/1702.04405},
  urldate = {2026-01-16},
  abstract = {A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at http://www.scan-net.org.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/XKZFT2MN/Dai et al. - 2017 - ScanNet Richly-annotated 3D Reconstructions of Indoor Scenes.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/3RN4L2S3/1702.html}
}

@online{dedieuLearningNoisyORBayesian2023,
  title = {Learning Noisy-{{OR Bayesian Networks}} with {{Max-Product Belief Propagation}}},
  author = {Dedieu, Antoine and Zhou, Guangyao and George, Dileep and Lazaro-Gredilla, Miguel},
  date = {2023-01-31},
  eprint = {2302.00099},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.00099},
  url = {http://arxiv.org/abs/2302.00099},
  urldate = {2026-01-19},
  abstract = {Noisy-OR Bayesian Networks (BNs) are a family of probabilistic graphical models which express rich statistical dependencies in binary data. Variational inference (VI) has been the main method proposed to learn noisy-OR BNs with complex latent structures (Jaakkola \& Jordan, 1999; Ji et al., 2020; Buhai et al., 2020). However, the proposed VI approaches either (a) use a recognition network with standard amortized inference that cannot induce ``explaining-away''; or (b) assume a simple mean-field (MF) posterior which is vulnerable to bad local optima. Existing MF VI methods also update the MF parameters sequentially which makes them inherently slow. In this paper, we propose parallel max-product as an alternative algorithm for learning noisy-OR BNs with complex latent structures and we derive a fast stochastic training scheme that scales to large datasets. We evaluate both approaches on several benchmarks where VI is the state-of-the-art and show that our method (a) achieves better test performance than Ji et al. (2020) for learning noisy-OR BNs with hierarchical latent structures on large sparse real datasets; (b) recovers a higher number of ground truth parameters than Buhai et al. (2020) from cluttered synthetic scenes; and (c) solves the 2D blind deconvolution problem from Lazaro-Gredilla et al. (2021) and variant - including binary matrix factorization - while VI catastrophically fails and is up to two orders of magnitude slower.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/XJAJJY37/Dedieu et al. - 2023 - Learning noisy-OR Bayesian Networks with Max-Product Belief Propagation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/D5G9E4QV/2302.html}
}

@online{devlinBERTPretrainingDeep2018,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2018-10-11},
  url = {https://arxiv.org/abs/1810.04805v2},
  urldate = {2026-01-08},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/HQUNA5YD/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf}
}

@article{dorbalaCanEmbodiedAgent2024,
  title = {Can an {{Embodied Agent Find Your}} "{{Cat-shaped Mug}}"? {{LLM-Guided Exploration}} for {{Zero-Shot Object Navigation}}},
  shorttitle = {Can an {{Embodied Agent Find Your}} "{{Cat-shaped Mug}}"?},
  author = {Dorbala, Vishnu Sashank and Mullen, James F. and Manocha, Dinesh},
  date = {2024-05},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {9},
  number = {5},
  eprint = {2303.03480},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {4083--4090},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2023.3346800},
  url = {http://arxiv.org/abs/2303.03480},
  urldate = {2025-12-11},
  abstract = {We present LGX (Language-guided Exploration), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to a uniquely described target object in a previously unseen environment. Our approach makes use of Large Language Models (LLMs) for this task by leveraging the LLM's commonsense reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27\% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via \textbackslash textit\{real-world\} experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Robotics},
  note = {Comment: 10 pages},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8PEIEHVD/Dorbala et al. - 2024 - Can an Embodied Agent Find Your Cat-shaped Mug LLM-Guided Exploration for Zero-Shot Object Naviga.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/CR35QE3J/2303.html}
}

@online{Figure2Downsampling,
  title = {Figure 2. {{Downsampling}} a Point Cloud with Voxel Grid Filtering.},
  url = {https://www.researchgate.net/figure/Downsampling-a-point-cloud-with-voxel-grid-filtering_fig2_346331400},
  urldate = {2026-01-16},
  abstract = {Download scientific diagram | Downsampling a point cloud with voxel grid filtering. from publication: Pose Invariant People Detection in Point Clouds for Mobile Robots | Point Clouds | ResearchGate, the professional network for scientists.},
  langid = {english},
  organization = {ResearchGate},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/DEZG98FD/Downsampling-a-point-cloud-with-voxel-grid-filtering_fig2_346331400.html}
}

@online{FrontierbasedApproachAutonomous,
  title = {A Frontier-Based Approach for Autonomous Exploration},
  url = {https://ieeexplore-1ieee-1org-100033czj0337.han.technikum-wien.at/document/613851},
  urldate = {2025-12-12},
  abstract = {We introduce a new approach for exploration based on the concept of frontiers, regions on the boundary between open space and unexplored space. By moving to new frontiers, a mobile robot can extend its map into new territory until the entire environment has been explored. We describe a method for detecting frontiers in evidence grids and navigating to these frontiers. We also introduce a technique for minimizing specular reflections in evidence grids using laser-limited sonar. We have tested this approach with a real mobile robot, exploring real-world office environments cluttered with a variety of obstacles. An advantage of our approach is its ability to explore both large open spaces and narrow cluttered spaces, with walls and obstacles in arbitrary orientation.},
  langid = {american},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5FXT4Y7A/613851.html}
}

@online{gadreCoWsPastureBaselines2022,
  title = {{{CoWs}} on {{Pasture}}: {{Baselines}} and {{Benchmarks}} for {{Language-Driven Zero-Shot Object Navigation}}},
  shorttitle = {{{CoWs}} on {{Pasture}}},
  author = {Gadre, Samir Yitzhak and Wortsman, Mitchell and Ilharco, Gabriel and Schmidt, Ludwig and Song, Shuran},
  date = {2022-12-14},
  eprint = {2203.10421},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.10421},
  url = {http://arxiv.org/abs/2203.10421},
  urldate = {2025-12-11},
  abstract = {For robots to be generally useful, they must be able to find arbitrary objects described by people (i.e., be language-driven) even without expensive navigation training on in-domain data (i.e., perform zero-shot inference). We explore these capabilities in a unified setting: language-driven zero-shot object navigation (L-ZSON). Inspired by the recent success of open-vocabulary models for image classification, we investigate a straightforward framework, CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without fine-tuning. To better evaluate L-ZSON, we introduce the Pasture benchmark, which considers finding uncommon objects, objects described by spatial and appearance attributes, and hidden objects described relative to visible objects. We conduct an in-depth empirical study by directly deploying 21 CoW baselines across Habitat, RoboTHOR, and Pasture. In total, we evaluate over 90k navigation episodes and find that (1) CoW baselines often struggle to leverage language descriptions, but are proficient at finding uncommon objects. (2) A simple CoW, with CLIP-based object localization and classical exploration -- and no additional training -- matches the navigation efficiency of a state-of-the-art ZSON method trained for 500M steps on Habitat MP3D data. This same CoW provides a 15.6 percentage point improvement in success over a state-of-the-art RoboTHOR ZSON model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/68M88XPA/Gadre et al. - 2022 - CoWs on Pasture Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/HV8RG95V/2203.html}
}

@article{gargSemanticsRoboticMapping2020,
  title = {Semantics for {{Robotic Mapping}}, {{Perception}} and {{Interaction}}: {{A Survey}}},
  shorttitle = {Semantics for {{Robotic Mapping}}, {{Perception}} and {{Interaction}}},
  author = {Garg, Sourav and Sünderhauf, Niko and Dayoub, Feras and Morrison, Douglas and Cosgun, Akansel and Carneiro, Gustavo and Wu, Qi and Chin, Tat-Jun and Reid, Ian and Gould, Stephen and Corke, Peter and Milford, Michael},
  date = {2020},
  journaltitle = {Foundations and Trends® in Robotics},
  shortjournal = {FNT in Robotics},
  volume = {8},
  number = {1--2},
  eprint = {2101.00443},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--224},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000059},
  url = {http://arxiv.org/abs/2101.00443},
  urldate = {2025-12-11},
  abstract = {For robots to navigate and interact more richly with the world around them, they will likely require a deeper understanding of the world in which they operate. In robotics and related research fields, the study of understanding is often referred to as semantics, which dictates what does the world "mean" to a robot, and is strongly tied to the question of how to represent that meaning. With humans and robots increasingly operating in the same world, the prospects of human-robot interaction also bring semantics and ontology of natural language into the picture. Driven by need, as well as by enablers like increasing availability of training data and computational resources, semantics is a rapidly growing research area in robotics. The field has received significant attention in the research literature to date, but most reviews and surveys have focused on particular aspects of the topic: the technical research issues regarding its use in specific robotic topics like mapping or segmentation, or its relevance to one particular application domain like autonomous driving. A new treatment is therefore required, and is also timely because so much relevant research has occurred since many of the key surveys were published. This survey therefore provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity, in which semantics are extracted, used, or both. Within these broad categories we survey dozens of major topics including fundamentals from the computer vision field and key robotics research areas utilizing semantics, including mapping, navigation and interaction with the world. The survey also covers key practical considerations, including enablers like increased data availability and improved computational hardware, and major application areas where...},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 81 pages, 1 figure, published in Foundations and Trends in Robotics, 2020},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/C2YANZ7K/Garg et al. - 2020 - Semantics for Robotic Mapping, Perception and Interaction A Survey.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/NFYR4VZ2/2101.html}
}

@online{ghasemiComprehensiveSurveyReinforcement2025,
  title = {A {{Comprehensive Survey}} of {{Reinforcement Learning}}: {{From Algorithms}} to {{Practical Challenges}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Reinforcement Learning}}},
  author = {Ghasemi, Majid and Moosavi, Amir Hossein and Ebrahimi, Dariush},
  date = {2025-02-01},
  eprint = {2411.18892},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.18892},
  url = {http://arxiv.org/abs/2411.18892},
  urldate = {2026-01-05},
  abstract = {Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: 79 pages},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/9RBBBNKJ/Ghasemi et al. - 2025 - A Comprehensive Survey of Reinforcement Learning From Algorithms to Practical Challenges.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/F6GZQZYS/2411.html}
}

@online{guConceptGraphsOpenVocabulary3D2023,
  title = {{{ConceptGraphs}}: {{Open-Vocabulary 3D Scene Graphs}} for {{Perception}} and {{Planning}}},
  shorttitle = {{{ConceptGraphs}}},
  author = {Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and family=Melo, given=Celso Miguel, prefix=de, useprefix=false and Tenenbaum, Joshua B. and Torralba, Antonio and Shkurti, Florian and Paull, Liam},
  date = {2023-09-28},
  eprint = {2309.16650},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16650},
  url = {http://arxiv.org/abs/2309.16650},
  urldate = {2025-12-11},
  abstract = {For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc )},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/2ZBH5Z2M/Gu et al. - 2023 - ConceptGraphs Open-Vocabulary 3D Scene Graphs for Perception and Planning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/XFV25V23/2309.html}
}

@online{guptaLVISDatasetLarge2019,
  title = {{{LVIS}}: {{A Dataset}} for {{Large Vocabulary Instance Segmentation}}},
  shorttitle = {{{LVIS}}},
  author = {Gupta, Agrim and Dollár, Piotr and Girshick, Ross},
  date = {2019-09-15},
  eprint = {1908.03195},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.03195},
  url = {http://arxiv.org/abs/1908.03195},
  urldate = {2026-01-17},
  abstract = {Progress on object detection is enabled by datasets that focus the research community's attention on open challenges. This process led us from simple images to complex scenes and from bounding boxes to segmentation masks. In this work, we introduce LVIS (pronounced `el-vis'): a new dataset for Large Vocabulary Instance Segmentation. We plan to collect \textasciitilde 2 million high-quality instance segmentation masks for over 1000 entry-level object categories in 164k images. Due to the Zipfian distribution of categories in natural images, LVIS naturally has a long tail of categories with few training samples. Given that state-of-the-art deep learning methods for object detection perform poorly in the low-sample regime, we believe that our dataset poses an important and exciting new scientific challenge. LVIS is available at http://www.lvisdataset.org.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Extension of the CVPR'19 paper describing release v0.5, the LVIS Challenge, and baseline results},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/VVKQZY3T/Gupta et al. - 2019 - LVIS A Dataset for Large Vocabulary Instance Segmentation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/AX3ILXJ8/1908.html}
}

@article{hacinecipogluPoseInvariantPeople2020,
  title = {Pose {{Invariant People Detection}} in {{Point Clouds}} for {{Mobile Robots}}},
  author = {Hacinecipoglu, Akif and Konukseven, E. and Koku, Ahmet},
  date = {2020-01-01},
  journaltitle = {International Journal of Mechanical Engineering and Robotics Research},
  shortjournal = {International Journal of Mechanical Engineering and Robotics Research},
  pages = {709--715},
  doi = {10.18178/ijmerr.9.5.709-715},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/EXI4NGP9/Hacinecipoglu et al. - 2020 - Pose Invariant People Detection in Point Clouds for Mobile Robots.pdf}
}

@online{hanFetchBenchSimulationBenchmark2024,
  title = {{{FetchBench}}: {{A Simulation Benchmark}} for {{Robot Fetching}}},
  shorttitle = {{{FetchBench}}},
  author = {Han, Beining and Parakh, Meenal and Geng, Derek and Defay, Jack A. and Luyang, Gan and Deng, Jia},
  date = {2024-10-18},
  eprint = {2406.11793},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.11793},
  url = {http://arxiv.org/abs/2406.11793},
  urldate = {2026-01-19},
  abstract = {Fetching, which includes approaching, grasping, and retrieving, is a critical challenge for robot manipulation tasks. Existing methods primarily focus on table-top scenarios, which do not adequately capture the complexities of environments where both grasping and planning are essential. To address this gap, we propose a new benchmark FetchBench, featuring diverse procedural scenes that integrate both grasping and motion planning challenges. Additionally, FetchBench includes a data generation pipeline that collects successful fetch trajectories for use in imitation learning methods. We implement multiple baselines from the traditional sense-plan-act pipeline to end-to-end behavior models. Our empirical analysis reveals that these methods achieve a maximum success rate of only 20\%, indicating substantial room for improvement. Additionally, we identify key bottlenecks within the sense-plan-act pipeline and make recommendations based on the systematic analysis.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/YP8V48X6/Han et al. - 2024 - FetchBench A Simulation Benchmark for Robot Fetching.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/HEC553U6/2406.html}
}

@online{heMaskRCNN2018,
  title = {Mask {{R-CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  date = {2018-01-24},
  eprint = {1703.06870},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1703.06870},
  url = {http://arxiv.org/abs/1703.06870},
  urldate = {2026-01-06},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: open source; appendix on more results},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/6SXVS3Q2/He et al. - 2018 - Mask R-CNN.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/Q6GCME8B/1703.html}
}

@online{heMaskRCNN2018a,
  title = {Mask {{R-CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  date = {2018-01-24},
  eprint = {1703.06870},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1703.06870},
  url = {http://arxiv.org/abs/1703.06870},
  urldate = {2026-01-17},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: open source; appendix on more results},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ELS8YLUM/He et al. - 2018 - Mask R-CNN.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/L2ETQVXQ/1703.html}
}

@online{huangVisualLanguageMaps2023,
  title = {Visual {{Language Maps}} for {{Robot Navigation}}},
  author = {Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  date = {2023-03-08},
  eprint = {2210.05714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.05714},
  url = {http://arxiv.org/abs/2210.05714},
  urldate = {2025-12-11},
  abstract = {Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., "in between the sofa and TV" or "three meters to the right of the chair") directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Accepted at the 2023 IEEE International Conference on Robotics and Automation (ICRA). Project page: https://vlmaps.github.io},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/G4DABBR7/Huang et al. - 2023 - Visual Language Maps for Robot Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DVA7KMXT/2210.html}
}

@online{jatavallabhulaConceptFusionOpensetMultimodal2023,
  title = {{{ConceptFusion}}: {{Open-set Multimodal 3D Mapping}}},
  shorttitle = {{{ConceptFusion}}},
  author = {Jatavallabhula, Krishna Murthy and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Maalouf, Alaa and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and Tewari, Ayush and Tenenbaum, Joshua B. and family=Melo, given=Celso Miguel, prefix=de, useprefix=false and Krishna, Madhava and Paull, Liam and Shkurti, Florian and Torralba, Antonio},
  date = {2023-10-23},
  eprint = {2302.07241},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.07241},
  url = {http://arxiv.org/abs/2302.07241},
  urldate = {2026-01-03},
  abstract = {Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts. We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today's foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40\% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping. For more information, visit our project page https://concept-fusion.github.io or watch our 5-minute explainer video https://www.youtube.com/watch?v=rkXgws8fiDs},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: RSS 2023. Project page: https://concept-fusion.github.io Explainer video: https://www.youtube.com/watch?v=rkXgws8fiDs Code: https://github.com/concept-fusion/concept-fusion},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5FVLK83F/Jatavallabhula et al. - 2023 - ConceptFusion Open-set Multimodal 3D Mapping.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/9222GTMG/2302.html}
}

@online{jiangCLIPDINOVisual2024,
  title = {From {{CLIP}} to {{DINO}}: {{Visual Encoders Shout}} in {{Multi-modal Large Language Models}}},
  shorttitle = {From {{CLIP}} to {{DINO}}},
  author = {Jiang, Dongsheng and Liu, Yuchen and Liu, Songlin and Zhao, Jin'e and Zhang, Hao and Gao, Zhen and Zhang, Xiaopeng and Li, Jin and Xiong, Hongkai},
  date = {2024-03-08},
  eprint = {2310.08825},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.08825},
  url = {http://arxiv.org/abs/2310.08825},
  urldate = {2025-12-14},
  abstract = {Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named COMM, that integrates CLIP and DINO with Multi-level features Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of COMM compared to existing methods, showcasing its enhanced visual capabilities within MLLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5GPWXAI2/Jiang et al. - 2024 - From CLIP to DINO Visual Encoders Shout in Multi-modal Large Language Models.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/5BJ9BFIZ/2310.html}
}

@article{jiangDualMapOnlineOpenVocabulary2025,
  title = {{{DualMap}}: {{Online Open-Vocabulary Semantic Mapping}} for {{Natural Language Navigation}} in {{Dynamic Changing Scenes}}},
  shorttitle = {{{DualMap}}},
  author = {Jiang, Jiajun and Zhu, Yiming and Wu, Zirui and Song, Jie},
  date = {2025-12},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {10},
  number = {12},
  eprint = {2506.01950},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {12612--12619},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2025.3621942},
  url = {http://arxiv.org/abs/2506.01950},
  urldate = {2026-01-07},
  abstract = {We introduce DualMap, an online open-vocabulary mapping system that enables robots to understand and navigate dynamically changing environments through natural language queries. Designed for efficient semantic mapping and adaptability to changing environments, DualMap meets the essential requirements for real-world robot navigation applications. Our proposed hybrid segmentation frontend and object-level status check eliminate the costly 3D object merging required by prior methods, enabling efficient online scene mapping. The dual-map representation combines a global abstract map for high-level candidate selection with a local concrete map for precise goal-reaching, effectively managing and updating dynamic changes in the environment. Through extensive experiments in both simulation and real-world scenarios, we demonstrate state-of-the-art performance in 3D open-vocabulary segmentation, efficient scene mapping, and online language-guided navigation. Project page: https://eku127.github.io/DualMap/},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: 14 pages, 14 figures. Published in IEEE Robotics and Automation Letters (RA-L), 2025. Code: https://github.com/Eku127/DualMap Project page: https://eku127.github.io/DualMap/},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5LUYVIA7/Jiang et al. - 2025 - DualMap Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/8DQ4K2RH/2506.html}
}

@online{kabirEnhancedRobotMotion2023,
  title = {Enhanced {{Robot Motion Block}} of {{A-star Algorithm}} for {{Robotic Path Planning}}},
  author = {Kabir, Raihan and Watanobe, Yutaka and Islam, Md Rashedul and Naruse, Keitaro},
  date = {2023-12-25},
  url = {https://arxiv.org/abs/2312.15738v1},
  urldate = {2026-01-08},
  abstract = {An efficient robot path-planning model is vulnerable to the number of search nodes, path cost, and time complexity. The conventional A-star (A*) algorithm outperforms other grid-based algorithms for its heuristic search. However it shows suboptimal performance for the time, space, and number of search nodes, depending on the robot motion block (RMB). To address this challenge, this study proposes an optimal RMB for the A* path-planning algorithm to enhance the performance, where the robot movement costs are calculated by the proposed adaptive cost function. Also, a selection process is proposed to select the optimal RMB size. In this proposed model, grid-based maps are used, where the robot's next move is determined based on the adaptive cost function by searching among surrounding octet neighborhood grid cells. The cumulative value from the output data arrays is used to determine the optimal motion block size, which is formulated based on parameters. The proposed RMB significantly affects the searching time complexity and number of search nodes of the A* algorithm while maintaining almost the same path cost to find the goal position by avoiding obstacles. For the experiment, a benchmarked online dataset is used and prepared three different dimensional maps. The proposed approach is validated using approximately 7000 different grid maps with various dimensions and obstacle environments. The proposed model with an optimal RMB demonstrated a remarkable improvement of 93.98\% in the number of search cells and 98.94\% in time complexity compared to the conventional A* algorithm. Path cost for the proposed model remained largely comparable to other state-of-the-art algorithms. Also, the proposed model outperforms other state-of-the-art algorithms.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/SAHXHELV/Kabir et al. - 2023 - Enhanced Robot Motion Block of A-star Algorithm for Robotic Path Planning.pdf}
}

@article{kansoSemanticSLAMComprehensive2025,
  title = {Semantic {{SLAM}}: {{A}} Comprehensive Survey of Methods and Applications},
  shorttitle = {Semantic {{SLAM}}},
  author = {Kanso, Houssein and Singh, Abhilasha and Zarif, Etaf El and Almohammed, Nooruldeen and Mounsef, Jinane and Maalouf, Noel and Arain, Bilal},
  date = {2025-12-01},
  journaltitle = {Intelligent Systems with Applications},
  shortjournal = {Intelligent Systems with Applications},
  volume = {28},
  pages = {200591},
  issn = {2667-3053},
  doi = {10.1016/j.iswa.2025.200591},
  url = {https://www.sciencedirect.com/science/article/pii/S2667305325001176},
  urldate = {2025-12-11},
  abstract = {This paper surveys the different approaches in semantic Simultaneous Localization and Mapping (SLAM), exploring how the incorporation of semantic information has enhanced performance in both indoor and outdoor settings, while highlighting key advancements in the field. It also identifies existing gaps and proposes potential directions for future improvements to address these issues. We provide a detailed review of the fundamentals of semantic SLAM, illustrating how incorporating semantic data enhances scene understanding and mapping accuracy. The paper presents semantic SLAM methods and core techniques that contribute to improved robustness and precision in mapping. A comprehensive overview of commonly used datasets for evaluating semantic SLAM systems is provided, along with a discussion of performance metrics used to assess their efficiency and accuracy. To demonstrate the reliability of semantic SLAM methodologies, we reproduce selected results from existing studies offering insights into the reproducibility of these approaches. The paper also addresses key challenges such as real-time processing, dynamic scene adaptation, and scalability while highlighting future research directions. Unlike prior surveys, this paper uniquely combines (i) a systematic taxonomy of semantic SLAM approaches across different sensing modalities and environments, (ii) a comparative review of datasets and evaluation metrics, and (iii) a reproducibility study of selected methods. To our knowledge, this is the first survey that integrates methods, datasets, evaluation practices, and application insights into a single comprehensive review, thereby offering a unified reference for researchers and practitioners. In conclusion, this review underscores the vital role of semantic SLAM in driving advancements in autonomous systems and intelligent navigation by analyzing recent developments, validating findings, and highlighting future research directions.},
  keywords = {Autonomous systems,Dynamic environments,Object-level SLAM,Semantic mapping,Semantic SLAM,Visual SLAM},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JYLEZSMP/Kanso et al. - 2025 - Semantic SLAM A comprehensive survey of methods and applications.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/2C2DQTME/S2667305325001176.html}
}

@online{kerrLERFLanguageEmbedded2023,
  title = {{{LERF}}: {{Language Embedded Radiance Fields}}},
  shorttitle = {{{LERF}}},
  author = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  date = {2023-03-16},
  eprint = {2303.09553},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.09553},
  url = {http://arxiv.org/abs/2303.09553},
  urldate = {2026-01-08},
  abstract = {Humans describe the physical world using natural language to refer to specific 3D locations based on a vast range of properties: visual appearance, semantics, abstract associations, or actionable affordances. In this work we propose Language Embedded Radiance Fields (LERFs), a method for grounding language embeddings from off-the-shelf models like CLIP into NeRF, which enable these types of open-ended language queries in 3D. LERF learns a dense, multi-scale language field inside NeRF by volume rendering CLIP embeddings along training rays, supervising these embeddings across training views to provide multi-view consistency and smooth the underlying language field. After optimization, LERF can extract 3D relevancy maps for a broad range of language prompts interactively in real-time, which has potential use cases in robotics, understanding vision-language models, and interacting with 3D scenes. LERF enables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings without relying on region proposals or masks, supporting long-tail open-vocabulary queries hierarchically across the volume. The project website can be found at https://lerf.io .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  note = {Comment: Project website can be found at https://lerf.io},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JC7EXDM4/Kerr et al. - 2023 - LERF Language Embedded Radiance Fields.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/E9Z6W722/2303.html}
}

@online{kerrLERFLanguageEmbedded2023a,
  title = {{{LERF}}: {{Language Embedded Radiance Fields}}},
  shorttitle = {{{LERF}}},
  author = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  date = {2023-03-16},
  eprint = {2303.09553},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.09553},
  url = {http://arxiv.org/abs/2303.09553},
  urldate = {2026-01-09},
  abstract = {Humans describe the physical world using natural language to refer to specific 3D locations based on a vast range of properties: visual appearance, semantics, abstract associations, or actionable affordances. In this work we propose Language Embedded Radiance Fields (LERFs), a method for grounding language embeddings from off-the-shelf models like CLIP into NeRF, which enable these types of open-ended language queries in 3D. LERF learns a dense, multi-scale language field inside NeRF by volume rendering CLIP embeddings along training rays, supervising these embeddings across training views to provide multi-view consistency and smooth the underlying language field. After optimization, LERF can extract 3D relevancy maps for a broad range of language prompts interactively in real-time, which has potential use cases in robotics, understanding vision-language models, and interacting with 3D scenes. LERF enables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings without relying on region proposals or masks, supporting long-tail open-vocabulary queries hierarchically across the volume. The project website can be found at https://lerf.io .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  note = {Comment: Project website can be found at https://lerf.io},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/E37SDX9J/Kerr et al. - 2023 - LERF Language Embedded Radiance Fields.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DCWZB68F/2303.html}
}

@online{kerrLERFLanguageEmbedded2023b,
  title = {{{LERF}}: {{Language Embedded Radiance Fields}}},
  shorttitle = {{{LERF}}},
  author = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  date = {2023-03-16},
  eprint = {2303.09553},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.09553},
  url = {http://arxiv.org/abs/2303.09553},
  urldate = {2026-01-11},
  abstract = {Humans describe the physical world using natural language to refer to specific 3D locations based on a vast range of properties: visual appearance, semantics, abstract associations, or actionable affordances. In this work we propose Language Embedded Radiance Fields (LERFs), a method for grounding language embeddings from off-the-shelf models like CLIP into NeRF, which enable these types of open-ended language queries in 3D. LERF learns a dense, multi-scale language field inside NeRF by volume rendering CLIP embeddings along training rays, supervising these embeddings across training views to provide multi-view consistency and smooth the underlying language field. After optimization, LERF can extract 3D relevancy maps for a broad range of language prompts interactively in real-time, which has potential use cases in robotics, understanding vision-language models, and interacting with 3D scenes. LERF enables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings without relying on region proposals or masks, supporting long-tail open-vocabulary queries hierarchically across the volume. The project website can be found at https://lerf.io .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  note = {Comment: Project website can be found at https://lerf.io},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/YJ2GATU7/Kerr et al. - 2023 - LERF Language Embedded Radiance Fields.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/VAS5A32F/2303.html}
}

@online{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
  date = {2023-04-05},
  eprint = {2304.02643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.02643},
  url = {http://arxiv.org/abs/2304.02643},
  urldate = {2025-12-15},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Project web-page: https://segment-anything.com},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/TWTA6NG6/Kirillov et al. - 2023 - Segment Anything.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/ZTAF3529/2304.html}
}

@inproceedings{koenigDesignUseParadigms2004,
  title = {Design and Use Paradigms for {{Gazebo}}, an Open-Source Multi-Robot Simulator},
  booktitle = {2004 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Koenig, N. and Howard, A.},
  date = {2004-09},
  volume = {3},
  pages = {2149-2154 vol.3},
  doi = {10.1109/IROS.2004.1389727},
  url = {https://ieeexplore.ieee.org/document/1389727},
  urldate = {2026-01-19},
  abstract = {Simulators have played a critical role in robotics research as tools for quick and efficient testing of new concepts, strategies, and algorithms. To date, most simulators have been restricted to 2D worlds, and few have matured to the point where they are both highly capable and easily adaptable. Gazebo is designed to fill this niche by creating a 3D dynamic multi-robot environment capable of recreating the complex worlds that would be encountered by the next generation of mobile robots. Its open source status, fine grained control, and high fidelity place Gazebo in a unique position to become more than just a stepping stone between the drawing board and real hardware: data visualization, simulation of remote environments, and even reverse engineering of blackbox systems are all possible applications. Gazebo is developed in cooperation with the Player and Stage projects (Gerkey, B. P., et al., July 2003), (Gerkey, B. P., et al., May 2001), (Vaughan, R. T., et al., Oct. 2003), and is available from http://playerstage.sourceforge.net/gazebo/ gazebo.html.},
  eventtitle = {2004 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Computational modeling,Educational robots,Friction,Mobile robots,Open source software,Packaging,Robot sensing systems,Service robots,Testing,Vehicle dynamics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5RXSJZSF/1389727.html}
}

@online{kuJoint3DProposal2018,
  title = {Joint {{3D Proposal Generation}} and {{Object Detection}} from {{View Aggregation}}},
  author = {Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven},
  date = {2018-07-12},
  eprint = {1712.02294},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1712.02294},
  url = {http://arxiv.org/abs/1712.02294},
  urldate = {2026-01-17},
  abstract = {We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: For any inquiries contact aharakeh(at)uwaterloo(dot)ca},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/9AQ793RQ/Ku et al. - 2018 - Joint 3D Proposal Generation and Object Detection from View Aggregation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/555233B9/1712.html}
}

@online{leeOptimizingROS22025,
  title = {Optimizing {{ROS}} 2 {{Communication}} for {{Wireless Robotic Systems}}},
  author = {Lee, Sanghoon and Kim, Taehun and Chae, Jiyeong and Park, Kyung-Joon},
  date = {2025-08-15},
  eprint = {2508.11366},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2508.11366},
  url = {http://arxiv.org/abs/2508.11366},
  urldate = {2026-01-21},
  abstract = {Wireless transmission of large payloads, such as high-resolution images and LiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source robotics middleware. The default Data Distribution Service (DDS) communication stack in ROS 2 exhibits significant performance degradation over lossy wireless links. Despite the widespread use of ROS 2, the underlying causes of these wireless communication challenges remain unexplored. In this paper, we present the first in-depth network-layer analysis of ROS 2's DDS stack under wireless conditions with large payloads. We identify the following three key issues: excessive IP fragmentation, inefficient retransmission timing, and congestive buffer bursts. To address these issues, we propose a lightweight and fully compatible DDS optimization framework that tunes communication parameters based on link and payload characteristics. Our solution can be seamlessly applied through the standard ROS 2 application interface via simple XML-based QoS configuration, requiring no protocol modifications, no additional components, and virtually no integration efforts. Extensive experiments across various wireless scenarios demonstrate that our framework successfully delivers large payloads in conditions where existing DDS modes fail, while maintaining low end-to-end latency.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Networking and Internet Architecture,Computer Science - Robotics},
  note = {Comment: 10 pages, 8 figures},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/M3FT5QMN/Lee et al. - 2025 - Optimizing ROS 2 Communication for Wireless Robotic Systems.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/XJGETQ7S/2508.html}
}

@online{liBLIP2BootstrappingLanguageImage2023,
  title = {{{BLIP-2}}: {{Bootstrapping Language-Image Pre-training}} with {{Frozen Image Encoders}} and {{Large Language Models}}},
  shorttitle = {{{BLIP-2}}},
  author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  date = {2023-06-15},
  eprint = {2301.12597},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.12597},
  url = {http://arxiv.org/abs/2301.12597},
  urldate = {2025-12-15},
  abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/CK7EWWUD/Li et al. - 2023 - BLIP-2 Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Mode.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/EA8SKWTP/2301.html}
}

@online{liGroundedLanguageImagePretraining2022,
  title = {Grounded {{Language-Image Pre-training}}},
  author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
  date = {2022-06-17},
  eprint = {2112.03857},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.03857},
  url = {http://arxiv.org/abs/2112.03857},
  urldate = {2025-12-15},
  abstract = {This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  note = {Comment: CVPR 2022; updated visualizations; fixed hyper-parameters in Appendix C.1},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/EGSYAAGC/Li et al. - 2022 - Grounded Language-Image Pre-training.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/8V9KT4RJ/2112.html}
}

@online{liGroundedLanguageImagePretraining2022a,
  title = {Grounded {{Language-Image Pre-training}}},
  author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
  date = {2022-06-17},
  eprint = {2112.03857},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.03857},
  url = {http://arxiv.org/abs/2112.03857},
  urldate = {2026-01-05},
  abstract = {This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  note = {Comment: CVPR 2022; updated visualizations; fixed hyper-parameters in Appendix C.1},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/PLKLSXTB/Li et al. - 2022 - Grounded Language-Image Pre-training.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/QF4RG72G/2112.html}
}

@online{liLanguagedrivenSemanticSegmentation2022,
  title = {Language-Driven {{Semantic Segmentation}}},
  author = {Li, Boyi and Weinberger, Kilian Q. and Belongie, Serge and Koltun, Vladlen and Ranftl, René},
  date = {2022-04-03},
  eprint = {2201.03546},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.03546},
  url = {http://arxiv.org/abs/2201.03546},
  urldate = {2026-01-07},
  abstract = {We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., "grass" or "building") together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class. The text embeddings provide a flexible label representation in which semantically similar labels map to similar regions in the embedding space (e.g., "cat" and "furry"). This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample. We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided. Code and demo are available at https://github.com/isl-org/lang-seg.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: ICLR 2022},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/IGJ4LZK9/Li et al. - 2022 - Language-driven Semantic Segmentation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/EPE7TZNN/2201.html}
}

@online{liLAVISLibraryLanguageVision2022,
  title = {{{LAVIS}}: {{A Library}} for {{Language-Vision Intelligence}}},
  shorttitle = {{{LAVIS}}},
  author = {Li, Dongxu and Li, Junnan and Le, Hung and Wang, Guangsen and Savarese, Silvio and Hoi, Steven C. H.},
  date = {2022-09-15},
  eprint = {2209.09019},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.09019},
  url = {http://arxiv.org/abs/2209.09019},
  urldate = {2026-01-20},
  abstract = {We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks. The library is available at: https://github.com/salesforce/LAVIS.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Preprint of LAVIS technical report},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/J6X5VDS3/Li et al. - 2022 - LAVIS A Library for Language-Vision Intelligence.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/RP2GNIZF/2209.html}
}

@online{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2015-02-21},
  eprint = {1405.0312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1405.0312},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2025-12-11},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/I5ATQM2D/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/RZ8JVYSE/1405.html}
}

@book{liu3DPointCloud2021,
  title = {{{3D Point Cloud Analysis}}: {{Traditional}}, {{Deep Learning}}, and {{Explainable Machine Learning Methods}}},
  shorttitle = {{{3D Point Cloud Analysis}}},
  author = {Liu, Shan and Zhang, Min and Kadam, Pranav and Kuo, C.-C. Jay},
  date = {2021},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-89180-0},
  url = {https://link.springer.com/10.1007/978-3-030-89180-0},
  urldate = {2026-01-03},
  isbn = {978-3-030-89179-4 978-3-030-89180-0},
  langid = {english},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/G3D7SSDB/Liu et al. - 2021 - 3D Point Cloud Analysis Traditional, Deep Learning, and Explainable Machine Learning Methods.pdf}
}

@online{liuGroundingDINOMarrying2024,
  title = {Grounding {{DINO}}: {{Marrying DINO}} with {{Grounded Pre-Training}} for {{Open-Set Object Detection}}},
  shorttitle = {Grounding {{DINO}}},
  author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
  date = {2024-07-19},
  eprint = {2303.05499},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.05499},
  url = {http://arxiv.org/abs/2303.05499},
  urldate = {2025-12-15},
  abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at \textbackslash url\{https://github.com/IDEA-Research/GroundingDINO\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Code will be available at https://github.com/IDEA-Research/GroundingDINO},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/TB8JHP9D/Liu et al. - 2024 - Grounding DINO Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/JM9R998B/2303.html}
}

@online{liuGroundingDINOMarrying2024a,
  title = {Grounding {{DINO}}: {{Marrying DINO}} with {{Grounded Pre-Training}} for {{Open-Set Object Detection}}},
  shorttitle = {Grounding {{DINO}}},
  author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
  date = {2024-07-19},
  eprint = {2303.05499},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.05499},
  url = {http://arxiv.org/abs/2303.05499},
  urldate = {2026-01-03},
  abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at \textbackslash url\{https://github.com/IDEA-Research/GroundingDINO\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Code will be available at https://github.com/IDEA-Research/GroundingDINO},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/DW4ZL8U7/Liu et al. - 2024 - Grounding DINO Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WDYGMMKN/2303.html}
}

@online{liuGroundingDINOMarrying2024b,
  title = {Grounding {{DINO}}: {{Marrying DINO}} with {{Grounded Pre-Training}} for {{Open-Set Object Detection}}},
  shorttitle = {Grounding {{DINO}}},
  author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
  date = {2024-07-19},
  eprint = {2303.05499},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.05499},
  url = {http://arxiv.org/abs/2303.05499},
  urldate = {2026-01-05},
  abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at \textbackslash url\{https://github.com/IDEA-Research/GroundingDINO\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Code will be available at https://github.com/IDEA-Research/GroundingDINO},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/24KD9Y9N/Liu et al. - 2024 - Grounding DINO Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/FWPTPB62/2303.html}
}

@article{lluviaActiveMappingRobot2021,
  title = {Active {{Mapping}} and {{Robot Exploration}}: {{A Survey}}},
  shorttitle = {Active {{Mapping}} and {{Robot Exploration}}},
  author = {Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander and Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander},
  date = {2021-04-02},
  journaltitle = {Sensors},
  volume = {21},
  number = {7},
  publisher = {publisher},
  issn = {1424-8220},
  doi = {10.3390/s21072445},
  url = {https://www.mdpi.com/1424-8220/21/7/2445},
  urldate = {2025-12-12},
  abstract = {Simultaneous localization and mapping responds to the problem of building a map of the environment without any prior information and based on the data...},
  langid = {english},
  keywords = {exploration,frontiers,mapping,mobile robots,next best view,path planning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WJMBQ9GT/Lluvia et al. - 2021 - Active Mapping and Robot Exploration A Survey.pdf}
}

@article{macenskiDesksROSMaintainers2023,
  title = {From the {{Desks}} of {{ROS Maintainers}}: {{A Survey}} of {{Modern}} \& {{Capable Mobile Robotics Algorithms}} in the {{Robot Operating System}} 2},
  shorttitle = {From the {{Desks}} of {{ROS Maintainers}}},
  author = {Macenski, Steve and Moore, Tom and Lu, David and Merzlyakov, Alexey and Ferguson, Michael},
  date = {2023-10},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {168},
  eprint = {2307.15236},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {104493},
  issn = {09218890},
  doi = {10.1016/j.robot.2023.104493},
  url = {http://arxiv.org/abs/2307.15236},
  urldate = {2026-01-13},
  abstract = {The Robot Operating System 2 (ROS 2) is rapidly impacting the intelligent machines sector -- on space missions, large agriculture equipment, multi-robot fleets, and more. Its success derives from its focused design and improved capabilities targeting product-grade and modern robotic systems. Following ROS 2's example, the mobile robotics ecosystem has been fully redesigned based on the transformed needs of modern robots and is experiencing active development not seen since its inception. This paper comes from the desks of the key ROS Navigation maintainers to review and analyze the state of the art of robotics navigation in ROS 2. This includes new systems without parallel in ROS 1 or other similar mobile robotics frameworks. We discuss current research products and historically robust methods that provide differing behaviors and support for most every robot type. This survey consists of overviews, comparisons, and expert insights organized by the fundamental problems in the field. Some of these implementations have yet to be described in literature and many have not been benchmarked relative to others. We end by providing a glimpse into the future of the ROS 2 mobile robotics ecosystem.},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/QCGGKLKB/Macenski et al. - 2023 - From the Desks of ROS Maintainers A Survey of Modern & Capable Mobile Robotics Algorithms in the Ro.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/R86TLMXW/2307.html}
}

@online{macenskiImpactROS22023,
  title = {Impact of {{ROS}} 2 {{Node Composition}} in {{Robotic Systems}}},
  author = {Macenski, Steve and Soragna, Alberto and Carroll, Michael and Ge, Zhenpeng},
  date = {2023-05-17},
  eprint = {2305.09933},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.09933},
  url = {http://arxiv.org/abs/2305.09933},
  urldate = {2026-01-20},
  abstract = {The Robot Operating System 2 (ROS 2) is the second generation of ROS representing a step forward in the robotic framework. Several new types of nodes and executor models are integral to control where, how, and when information is processed in the computational graph. This paper explores and benchmarks one of these new node types -- the Component node -- which allows nodes to be composed manually or dynamically into processes while retaining separation of concerns in a codebase for distributed development. Composition is shown to achieve a high degree of performance optimization, particularly valuable for resource-constrained systems and sensor processing pipelines, enabling distributed tasks that would not be otherwise possible in ROS 2. In this work, we briefly introduce the significance and design of node composition, then our contribution of benchmarking is provided to analyze its impact on robotic systems. Its compelling influence on performance is shown through several experiments on the latest Long Term Support (LTS) ROS 2 distribution, Humble Hawksbill.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  note = {Comment: IEEE Robotics and Automation Letters, 2023},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/6YNXF7M9/Macenski et al. - 2023 - Impact of ROS 2 Node Composition in Robotic Systems.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/Z5H6Z5K4/2305.html}
}

@article{macenskiSLAMToolboxSLAM2021,
  title = {{{SLAM Toolbox}}: {{SLAM}} for the Dynamic World},
  shorttitle = {{{SLAM Toolbox}}},
  author = {Macenski, Steve and Jambrecic, Ivona},
  date = {2021-05-13},
  journaltitle = {Journal of Open Source Software},
  shortjournal = {Journal of Open Source Software},
  volume = {6},
  pages = {2783},
  doi = {10.21105/joss.02783},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5T82CIAR/Macenski and Jambrecic - 2021 - SLAM Toolbox SLAM for the dynamic world.pdf}
}

@online{maggioClioRealtimeTaskDriven2024,
  title = {Clio: {{Real-time Task-Driven Open-Set 3D Scene Graphs}}},
  shorttitle = {Clio},
  author = {Maggio, Dominic and Chang, Yun and Hughes, Nathan and Trang, Matthew and Griffith, Dan and Dougherty, Carlyn and Cristofalo, Eric and Schmid, Lukas and Carlone, Luca},
  date = {2024-04-29},
  eprint = {2404.13696},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.13696},
  url = {http://arxiv.org/abs/2404.13696},
  urldate = {2026-01-08},
  abstract = {Modern tools for class-agnostic image segmentation (e.g., SegmentAnything) and open-set semantic understanding (e.g., CLIP) provide unprecedented opportunities for robot perception and mapping. While traditional closed-set metric-semantic maps were restricted to tens or hundreds of semantic classes, we can now build maps with a plethora of objects and countless semantic variations. This leaves us with a fundamental question: what is the right granularity for the objects (and, more generally, for the semantic concepts) the robot has to include in its map representation? While related work implicitly chooses a level of granularity by tuning thresholds for object detection, we argue that such a choice is intrinsically task-dependent. The first contribution of this paper is to propose a task-driven 3D scene understanding problem, where the robot is given a list of tasks in natural language and has to select the granularity and the subset of objects and scene structure to retain in its map that is sufficient to complete the tasks. We show that this problem can be naturally formulated using the Information Bottleneck (IB), an established information-theoretic framework. The second contribution is an algorithm for task-driven 3D scene understanding based on an Agglomerative IB approach, that is able to cluster 3D primitives in the environment into task-relevant objects and regions and executes incrementally. The third contribution is to integrate our task-driven clustering algorithm into a real-time pipeline, named Clio, that constructs a hierarchical 3D scene graph of the environment online using only onboard compute, as the robot explores it. Our final contribution is an extensive experimental campaign showing that Clio not only allows real-time construction of compact open-set 3D scene graphs, but also improves the accuracy of task execution by limiting the map to relevant semantic concepts.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/IVQ9JLMR/Maggio et al. - 2024 - Clio Real-time Task-Driven Open-Set 3D Scene Graphs.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/VGJKVADR/2404.html}
}

@online{majumdarZSONZeroShotObjectGoal2023,
  title = {{{ZSON}}: {{Zero-Shot Object-Goal Navigation}} Using {{Multimodal Goal Embeddings}}},
  shorttitle = {{{ZSON}}},
  author = {Majumdar, Arjun and Aggarwal, Gunjan and Devnani, Bhavika and Hoffman, Judy and Batra, Dhruv},
  date = {2023-10-13},
  eprint = {2206.12403},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.12403},
  url = {http://arxiv.org/abs/2206.12403},
  urldate = {2025-12-11},
  abstract = {We present a scalable approach for learning open-world object-goal navigation (ObjectNav) -- the task of asking a virtual robot (agent) to find any instance of an object in an unexplored environment (e.g., "find a sink"). Our approach is entirely zero-shot -- i.e., it does not require ObjectNav rewards or demonstrations of any kind. Instead, we train on the image-goal navigation (ImageNav) task, in which agents find the location where a picture (i.e., goal image) was captured. Specifically, we encode goal images into a multimodal, semantic embedding space to enable training semantic-goal navigation (SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D). After training, SemanticNav agents can be instructed to find objects described in free-form natural language (e.g., "sink", "bathroom sink", etc.) by projecting language goals into the same multimodal, semantic embedding space. As a result, our approach enables open-world ObjectNav. We extensively evaluate our agents on three ObjectNav datasets (Gibson, HM3D, and MP3D) and observe absolute improvements in success of 4.2\% - 20.0\% over existing zero-shot methods. For reference, these gains are similar or better than the 5\% improvement in success between the Habitat 2020 and 2021 ObjectNav challenge winners. In an open-world setting, we discover that our agents can generalize to compound instructions with a room explicitly mentioned (e.g., "Find a kitchen sink") and when the target room can be inferred (e.g., "Find a sink and a stove").},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: code: https://github.com/gunagg/zson},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/Q9ILVZFE/Majumdar et al. - 2023 - ZSON Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/QJ3M4582/2206.html}
}

@online{makoviychukIsaacGymHigh2021,
  title = {Isaac {{Gym}}: {{High Performance GPU-Based Physics Simulation For Robot Learning}}},
  shorttitle = {Isaac {{Gym}}},
  author = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and State, Gavriel},
  date = {2021-08-25},
  eprint = {2108.10470},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2108.10470},
  url = {http://arxiv.org/abs/2108.10470},
  urldate = {2026-01-14},
  abstract = {Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at \textbackslash url\{https://sites.google.com/view/isaacgym-nvidia\} and isaac gym can be downloaded at \textbackslash url\{https://developer.nvidia.com/isaac-gym\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: tech report on isaac-gym},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5MHSVJZ6/Makoviychuk et al. - 2021 - Isaac Gym High Performance GPU-Based Physics Simulation For Robot Learning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/GYKAUXIB/2108.html}
}

@online{makoviychukIsaacGymHigh2021a,
  title = {Isaac {{Gym}}: {{High Performance GPU-Based Physics Simulation For Robot Learning}}},
  shorttitle = {Isaac {{Gym}}},
  author = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and State, Gavriel},
  date = {2021-08-25},
  eprint = {2108.10470},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2108.10470},
  url = {http://arxiv.org/abs/2108.10470},
  urldate = {2026-01-19},
  abstract = {Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at \textbackslash url\{https://sites.google.com/view/isaacgym-nvidia\} and isaac gym can be downloaded at \textbackslash url\{https://developer.nvidia.com/isaac-gym\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: tech report on isaac-gym},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/94YEUBF7/Makoviychuk et al. - 2021 - Isaac Gym High Performance GPU-Based Physics Simulation For Robot Learning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/59E9IUXV/2108.html}
}

@online{meierCARLADroneMonocular2024,
  title = {{{CARLA Drone}}: {{Monocular 3D Object Detection}} from a {{Different Perspective}}},
  shorttitle = {{{CARLA Drone}}},
  author = {Meier, Johannes and Scalerandi, Luca and Dhaouadi, Oussema and Kaiser, Jacques and Araslanov, Nikita and Cremers, Daniel},
  date = {2024-10-21},
  eprint = {2408.11958},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.11958},
  url = {http://arxiv.org/abs/2408.11958},
  urldate = {2026-01-17},
  abstract = {Existing techniques for monocular 3D detection have a serious restriction. They tend to perform well only on a limited set of benchmarks, faring well either on ego-centric car views or on traffic camera views, but rarely on both. To encourage progress, this work advocates for an extended evaluation of 3D detection frameworks across different camera perspectives. We make two key contributions. First, we introduce the CARLA Drone dataset, CDrone. Simulating drone views, it substantially expands the diversity of camera perspectives in existing benchmarks. Despite its synthetic nature, CDrone represents a real-world challenge. To show this, we confirm that previous techniques struggle to perform well both on CDrone and a real-world 3D drone dataset. Second, we develop an effective data augmentation pipeline called GroundMix. Its distinguishing element is the use of the ground for creating 3D-consistent augmentation of a training image. GroundMix significantly boosts the detection accuracy of a lightweight one-stage detector. In our expanded evaluation, we achieve the average precision on par with or substantially higher than the previous state of the art across all tested datasets.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/FD3CM8N2/Meier et al. - 2024 - CARLA Drone Monocular 3D Object Detection from a Different Perspective.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/P4LE3EH5/2408.html}
}

@article{miaoFrontierReviewSemantic2025,
  title = {A {{Frontier Review}} of {{Semantic SLAM Technologies Applied}} to the {{Open World}}},
  author = {Miao, Le and Liu, Wen and Deng, Zhongliang and Miao, Le and Liu, Wen and Deng, Zhongliang},
  date = {2025-08-12},
  journaltitle = {Sensors},
  volume = {25},
  number = {16},
  publisher = {publisher},
  issn = {1424-8220},
  doi = {10.3390/s25164994},
  url = {https://www.mdpi.com/1424-8220/25/16/4994},
  urldate = {2025-12-11},
  abstract = {With the growing demand for autonomous robotic operations in complex and unstructured environments, traditional semantic SLAM systems—which rely on cl...},
  langid = {english},
  keywords = {multimodal sensor fusion,open-world perception,robotic perception,semantic SLAM,zero-shot learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/NBHH6UZS/Miao et al. - 2025 - A Frontier Review of Semantic SLAM Technologies Applied to the Open World.pdf}
}

@online{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2020-08-03},
  eprint = {2003.08934},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.08934},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2026-01-11},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(θ, φ)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  note = {Comment: ECCV 2020 (oral). Project page with videos and code: http://tancik.com/nerf},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/BQGAGWUJ/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Fields for View Synthesis.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WUCI2C2W/2003.html}
}

@online{mindererScalingOpenVocabularyObject2024,
  title = {Scaling {{Open-Vocabulary Object Detection}}},
  author = {Minderer, Matthias and Gritsenko, Alexey and Houlsby, Neil},
  date = {2024-05-22},
  eprint = {2306.09683},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.09683},
  url = {http://arxiv.org/abs/2306.09683},
  urldate = {2026-01-17},
  abstract = {Open-vocabulary object detection has benefited greatly from pretrained vision-language models, but is still limited by the amount of available detection training data. While detection training data can be expanded by using Web image-text pairs as weak supervision, this has not been done at scales comparable to image-level pretraining. Here, we scale up detection data with self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. Major challenges in scaling self-training are the choice of label space, pseudo-annotation filtering, and training efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which address these challenges. OWLv2 surpasses the performance of previous state-of-the-art open-vocabulary detectors already at comparable training scales (\textasciitilde 10M examples). However, with OWL-ST, we can scale to over 1B examples, yielding further large improvement: With an L/14 architecture, OWL-ST improves AP on LVIS rare classes, for which the model has seen no human box annotations, from 31.2\% to 44.6\% (43\% relative improvement). OWL-ST unlocks Web-scale training for open-world localization, similar to what has been seen for image classification and language modelling.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ZJZ4I8WB/Minderer et al. - 2024 - Scaling Open-Vocabulary Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/EDNXTVJV/2306.html}
}

@online{oquabDINOv2LearningRobust2024,
  title = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  date = {2024-02-02},
  eprint = {2304.07193},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.07193},
  url = {http://arxiv.org/abs/2304.07193},
  urldate = {2026-01-08},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/7XXEHEJG/Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Supervision.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/J6ZATIPM/2304.html}
}

@online{PDFRobustStatistics,
  title = {({{PDF}}) {{Robust Statistics}}: {{The Approach Based}} on {{Influence Functions}}},
  shorttitle = {({{PDF}}) {{Robust Statistics}}},
  url = {https://www.researchgate.net/publication/346630632_Robust_Statistics_The_Approach_Based_on_Influence_Functions},
  urldate = {2026-01-17},
  abstract = {PDF | On Mar 22, 1986, Frank R. Hampel and others published Robust Statistics: The Approach Based on Influence Functions | Find, read and cite all the research you need on ResearchGate},
  isbn = {9781118186435},
  langid = {english},
  organization = {ResearchGate}
}

@online{PDFRobustStatisticsa,
  title = {({{PDF}}) {{Robust Statistics}}: {{The Approach Based}} on {{Influence Functions}}},
  shorttitle = {({{PDF}}) {{Robust Statistics}}},
  url = {https://www.researchgate.net/publication/346630632_Robust_Statistics_The_Approach_Based_on_Influence_Functions},
  urldate = {2026-01-17},
  abstract = {PDF | On Mar 22, 1986, Frank R. Hampel and others published Robust Statistics: The Approach Based on Influence Functions | Find, read and cite all the research you need on ResearchGate},
  isbn = {9781118186435},
  langid = {english},
  organization = {ResearchGate},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/MTU564GX/346630632_Robust_Statistics_The_Approach_Based_on_Influence_Functions.html}
}

@online{pengPIGEONVLMDrivenObject2025,
  title = {{{PIGEON}}: {{VLM-Driven Object Navigation}} via {{Points}} of {{Interest Selection}}},
  shorttitle = {{{PIGEON}}},
  author = {Peng, Cheng and Zhang, Zhenzhe and Chi, Cheng and Wei, Xiaobao and Zhang, Yanhao and Wang, Heng and Wang, Pengwei and Wang, Zhongyuan and Liu, Jing and Zhang, Shanghang},
  date = {2025-11-17},
  eprint = {2511.13207},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2511.13207},
  url = {http://arxiv.org/abs/2511.13207},
  urldate = {2026-01-28},
  abstract = {Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WL5S354M/Peng et al. - 2025 - PIGEON VLM-Driven Object Navigation via Points of Interest Selection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/ILYG4NZU/2511.html}
}

@online{ProbabilisticReasoningIntelligent,
  title = {Probabilistic {{Reasoning}} in {{Intelligent Systems}}},
  url = {http://www.sciencedirect.com:5070/book/monograph/9780080514895/probabilistic-reasoning-in-intelligent-systems},
  urldate = {2026-01-19},
  abstract = {Probabilistic Reasoning in Intelligent Systems is a complete and accessible account of the theoretical foundations and computational methods that unde...},
  isbn = {9780080514895},
  langid = {english},
  organization = {ScienceDirect},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/YJVJBR8S/probabilistic-reasoning-in-intelligent-systems.html}
}

@online{puigHabitat30CoHabitat2023,
  title = {Habitat 3.0: {{A Co-Habitat}} for {{Humans}}, {{Avatars}} and {{Robots}}},
  shorttitle = {Habitat 3.0},
  author = {Puig, Xavier and Undersander, Eric and Szot, Andrew and Cote, Mikael Dallaire and Yang, Tsung-Yen and Partsey, Ruslan and Desai, Ruta and Clegg, Alexander William and Hlavac, Michal and Min, So Yeon and Vondruš, Vladimír and Gervet, Theophile and Berges, Vincent-Pierre and Turner, John M. and Maksymets, Oleksandr and Kira, Zsolt and Kalakrishnan, Mrinal and Malik, Jitendra and Chaplot, Devendra Singh and Jain, Unnat and Batra, Dhruv and Rai, Akshara and Mottaghi, Roozbeh},
  date = {2023-10-19},
  eprint = {2310.13724},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.13724},
  url = {http://arxiv.org/abs/2310.13724},
  urldate = {2026-01-20},
  abstract = {We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments. Habitat 3.0 offers contributions across three dimensions: (1) Accurate humanoid simulation: addressing challenges in modeling complex deformable bodies and diversity in appearance and motion, all while ensuring high simulation speed. (2) Human-in-the-loop infrastructure: enabling real human interaction with simulated robots via mouse/keyboard or a VR interface, facilitating evaluation of robot policies with human input. (3) Collaborative tasks: studying two collaborative tasks, Social Navigation and Social Rearrangement. Social Navigation investigates a robot's ability to locate and follow humanoid avatars in unseen environments, whereas Social Rearrangement addresses collaboration between a humanoid and robot while rearranging a scene. These contributions allow us to study end-to-end learned and heuristic baselines for human-robot collaboration in-depth, as well as evaluate them with humans in the loop. Our experiments demonstrate that learned robot policies lead to efficient task completion when collaborating with unseen humanoid agents and human partners that might exhibit behaviors that the robot has not seen before. Additionally, we observe emergent behaviors during collaborative task execution, such as the robot yielding space when obstructing a humanoid agent, thereby allowing the effective completion of the task by the humanoid agent. Furthermore, our experiments using the human-in-the-loop tool demonstrate that our automated evaluation with humanoids can provide an indication of the relative ordering of different policies when evaluated with real human collaborators. Habitat 3.0 unlocks interesting new features in simulators for Embodied AI, and we hope it paves the way for a new frontier of embodied human-AI interaction capabilities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Human-Computer Interaction,Computer Science - Multiagent Systems,Computer Science - Robotics},
  note = {Comment: Project page: http://aihabitat.org/habitat3},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/QL4NTZBJ/Puig et al. - 2023 - Habitat 3.0 A Co-Habitat for Humans, Avatars and Robots.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/BM6ZP8JX/2310.html}
}

@online{puigHabitat30CoHabitat2023a,
  title = {Habitat 3.0: {{A Co-Habitat}} for {{Humans}}, {{Avatars}} and {{Robots}}},
  shorttitle = {Habitat 3.0},
  author = {Puig, Xavier and Undersander, Eric and Szot, Andrew and Cote, Mikael Dallaire and Yang, Tsung-Yen and Partsey, Ruslan and Desai, Ruta and Clegg, Alexander William and Hlavac, Michal and Min, So Yeon and Vondruš, Vladimír and Gervet, Theophile and Berges, Vincent-Pierre and Turner, John M. and Maksymets, Oleksandr and Kira, Zsolt and Kalakrishnan, Mrinal and Malik, Jitendra and Chaplot, Devendra Singh and Jain, Unnat and Batra, Dhruv and Rai, Akshara and Mottaghi, Roozbeh},
  date = {2023-10-19},
  eprint = {2310.13724},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.13724},
  url = {http://arxiv.org/abs/2310.13724},
  urldate = {2026-01-30},
  abstract = {We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments. Habitat 3.0 offers contributions across three dimensions: (1) Accurate humanoid simulation: addressing challenges in modeling complex deformable bodies and diversity in appearance and motion, all while ensuring high simulation speed. (2) Human-in-the-loop infrastructure: enabling real human interaction with simulated robots via mouse/keyboard or a VR interface, facilitating evaluation of robot policies with human input. (3) Collaborative tasks: studying two collaborative tasks, Social Navigation and Social Rearrangement. Social Navigation investigates a robot's ability to locate and follow humanoid avatars in unseen environments, whereas Social Rearrangement addresses collaboration between a humanoid and robot while rearranging a scene. These contributions allow us to study end-to-end learned and heuristic baselines for human-robot collaboration in-depth, as well as evaluate them with humans in the loop. Our experiments demonstrate that learned robot policies lead to efficient task completion when collaborating with unseen humanoid agents and human partners that might exhibit behaviors that the robot has not seen before. Additionally, we observe emergent behaviors during collaborative task execution, such as the robot yielding space when obstructing a humanoid agent, thereby allowing the effective completion of the task by the humanoid agent. Furthermore, our experiments using the human-in-the-loop tool demonstrate that our automated evaluation with humanoids can provide an indication of the relative ordering of different policies when evaluated with real human collaborators. Habitat 3.0 unlocks interesting new features in simulators for Embodied AI, and we hope it paves the way for a new frontier of embodied human-AI interaction capabilities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Human-Computer Interaction,Computer Science - Multiagent Systems,Computer Science - Robotics},
  note = {Comment: Project page: http://aihabitat.org/habitat3},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/FB8F2APE/Puig et al. - 2023 - Habitat 3.0 A Co-Habitat for Humans, Avatars and Robots.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DZUXVKTD/2310.html}
}

@inproceedings{putzMoveBaseFlex2018,
  title = {Move {{Base Flex A Highly Flexible Navigation Framework}} for {{Mobile Robots}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Pütz, Sebastian and Santos Simón, Jorge and Hertzberg, Joachim},
  date = {2018-10},
  pages = {3416--3421},
  issn = {2153-0866},
  doi = {10.1109/IROS.2018.8593829},
  url = {https://ieeexplore.ieee.org/document/8593829},
  urldate = {2026-01-29},
  abstract = {We present Move Base Flex (MBF), a highly flexible, modular, map-independent, open-source navigation framework for use in ROS. MBF provides modular actions for executing plugins for path planning, motion control, and recovery. These actions define interfaces for external executives to allow highly flexible navigation strategies, which can be intertwined with other robot tasks. MBF has been successfully deployed in a professional setting at customer facilities to control robots in highly dynamic environments. We compare MBF with the well-known move\_base and present the architecture as well as different deployment approaches, including how MBF can be used with different executives to perform complex navigation tasks interleaved with other robot operations.},
  eventtitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Computer architecture,Flexible printed circuits,Navigation,Planning,Robots,Servers,Task analysis},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/D3246MF4/8593829.html}
}

@online{qiuLearningGeneralizableFeature2024,
  title = {Learning {{Generalizable Feature Fields}} for {{Mobile Manipulation}}},
  author = {Qiu, Ri-Zhao and Hu, Yafei and Song, Yuchen and Yang, Ge and Fu, Yang and Ye, Jianglong and Mu, Jiteng and Yang, Ruihan and Atanasov, Nikolay and Scherer, Sebastian and Wang, Xiaolong},
  date = {2024-11-26},
  eprint = {2403.07563},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.07563},
  url = {http://arxiv.org/abs/2403.07563},
  urldate = {2025-12-11},
  abstract = {An open problem in mobile manipulation is how to represent objects and scenes in a unified manner so that robots can use both for navigation and manipulation. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherent at an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We quantitatively evaluate GeFF's ability for open-vocabulary object-/part-level manipulation and show that GeFF outperforms point-based baselines in runtime and storage-accuracy trade-offs, with qualitative examples of semantics-aware navigation and articulated object manipulation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Preprint. Project website is at: https://geff-b1.github.io/},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ZQ7PJHJN/Qiu et al. - 2024 - Learning Generalizable Feature Fields for Mobile Manipulation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/BFJCQT64/2403.html}
}

@article{quinApproachesEfficientlyDetecting2021,
  title = {Approaches for {{Efficiently Detecting Frontier Cells}} in {{Robotics Exploration}}},
  author = {Quin, Phillip and Nguyen, Dac and Vu, Thanh and Alempijevic, Alen and Paul, Gavin},
  date = {2021-02-25},
  journaltitle = {Frontiers in Robotics and AI},
  shortjournal = {Frontiers in Robotics and AI},
  volume = {8},
  pages = {616470},
  doi = {10.3389/frobt.2021.616470},
  abstract = {Many robot exploration algorithms that are used to explore office, home, or outdoor environments, rely on the concept of frontier cells. Frontier cells define the border between known and unknown space. Frontier-based exploration is the process of repeatedly detecting frontiers and moving towards them, until there are no more frontiers and therefore no more unknown regions. The faster frontier cells can be detected, the more efficient exploration becomes. This paper proposes several algorithms for detecting frontiers. The first is called Naïve Active Area (NaïveAA) frontier detection and achieves frontier detection in constant time by only evaluating the cells in the active area defined by scans taken. The second algorithm is called Expanding-Wavefront Frontier Detection (EWFD) and uses frontiers from the previous timestep as a starting point for searching for frontiers in newly discovered space. The third approach is called Frontier-Tracing Frontier Detection (FTFD) and also uses the frontiers from the previous timestep as well as the endpoints of the scan, to determine the frontiers at the current timestep. Algorithms are compared to state-of-the-art algorithms such as Naïve, WFD, and WFD-INC. NaïveAA is shown to operate in constant time and therefore is suitable as a basic benchmark for frontier detection algorithms. EWFD and FTFD are found to be significantly faster than other algorithms.},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/A6KDFCUD/Quin et al. - 2021 - Approaches for Efficiently Detecting Frontier Cells in Robotics Exploration.pdf}
}

@online{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.00020},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2025-12-13},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WBDSAGZM/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/QHYY9RBJ/2103.html}
}

@online{radfordLearningTransferableVisual2021a,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.00020},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2026-01-02},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/XUIE3LLL/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/2A2BP49U/2103.html}
}

@online{ramakrishnanHabitatMatterport3DDataset2021,
  title = {Habitat-{{Matterport 3D Dataset}} ({{HM3D}}): 1000 {{Large-scale 3D Environments}} for {{Embodied AI}}},
  shorttitle = {Habitat-{{Matterport 3D Dataset}} ({{HM3D}})},
  author = {Ramakrishnan, Santhosh K. and Gokaslan, Aaron and Wijmans, Erik and Maksymets, Oleksandr and Clegg, Alex and Turner, John and Undersander, Eric and Galuba, Wojciech and Westbury, Andrew and Chang, Angel X. and Savva, Manolis and Zhao, Yili and Batra, Dhruv},
  date = {2021-09-16},
  eprint = {2109.08238},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.08238},
  url = {http://arxiv.org/abs/2109.08238},
  urldate = {2026-01-20},
  abstract = {We present the Habitat-Matterport 3D (HM3D) dataset. HM3D is a large-scale dataset of 1,000 building-scale 3D reconstructions from a diverse set of real-world locations. Each scene in the dataset consists of a textured 3D mesh reconstruction of interiors such as multi-floor residences, stores, and other private indoor spaces. HM3D surpasses existing datasets available for academic research in terms of physical scale, completeness of the reconstruction, and visual fidelity. HM3D contains 112.5k m\textasciicircum 2 of navigable space, which is 1.4 - 3.7x larger than other building-scale datasets such as MP3D and Gibson. When compared to existing photorealistic 3D datasets such as Replica, MP3D, Gibson, and ScanNet, images rendered from HM3D have 20 - 85\% higher visual fidelity w.r.t. counterpart images captured with real cameras, and HM3D meshes have 34 - 91\% fewer artifacts due to incomplete surface reconstruction. The increased scale, fidelity, and diversity of HM3D directly impacts the performance of embodied AI agents trained using it. In fact, we find that HM3D is `pareto optimal' in the following sense -- agents trained to perform PointGoal navigation on HM3D achieve the highest performance regardless of whether they are evaluated on HM3D, Gibson, or MP3D. No similar claim can be made about training on other datasets. HM3D-trained PointNav agents achieve 100\% performance on Gibson-test dataset, suggesting that it might be time to retire that episode dataset.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 21 pages, 14 figures},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/H6WQ8YPV/Ramakrishnan et al. - 2021 - Habitat-Matterport 3D Dataset (HM3D) 1000 Large-scale 3D Environments for Embodied AI.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/26HJ49FU/2109.html}
}

@online{ramakrishnanPONIPotentialFunctions2022,
  title = {{{PONI}}: {{Potential Functions}} for {{ObjectGoal Navigation}} with {{Interaction-free Learning}}},
  shorttitle = {{{PONI}}},
  author = {Ramakrishnan, Santhosh Kumar and Chaplot, Devendra Singh and Al-Halah, Ziad and Malik, Jitendra and Grauman, Kristen},
  date = {2022-06-17},
  eprint = {2201.10029},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.10029},
  url = {http://arxiv.org/abs/2201.10029},
  urldate = {2025-12-11},
  abstract = {State-of-the-art approaches to ObjectGoal navigation rely on reinforcement learning and typically require significant computational resources and time for learning. We propose Potential functions for ObjectGoal Navigation with Interaction-free learning (PONI), a modular approach that disentangles the skills of `where to look?' for an object and `how to navigate to (x, y)?'. Our key insight is that `where to look?' can be treated purely as a perception problem, and learned without environment interactions. To address this, we propose a network that predicts two complementary potential functions conditioned on a semantic map and uses them to decide where to look for an unseen object. We train the potential function network using supervised learning on a passive dataset of top-down semantic maps, and integrate it into a modular framework to perform ObjectGoal navigation. Experiments on Gibson and Matterport3D demonstrate that our method achieves the state-of-the-art for ObjectGoal navigation while incurring up to 1,600x less computational cost for training. Code and pre-trained models are available: https://vision.cs.utexas.edu/projects/poni/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 8 pages + supplementary. Accepted in CVPR 2022},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/SCUSH2CB/Ramakrishnan et al. - 2022 - PONI Potential Functions for ObjectGoal Navigation with Interaction-free Learning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/EY8AD5U3/2201.html}
}

@online{ramrakhyaPIRLNavPretrainingImitation2023,
  title = {{{PIRLNav}}: {{Pretraining}} with {{Imitation}} and {{RL Finetuning}} for {{ObjectNav}}},
  shorttitle = {{{PIRLNav}}},
  author = {Ramrakhya, Ram and Batra, Dhruv and Wijmans, Erik and Das, Abhishek},
  date = {2023-03-26},
  eprint = {2301.07302},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.07302},
  url = {http://arxiv.org/abs/2301.07302},
  urldate = {2025-12-11},
  abstract = {We study ObjectGoal Navigation -- where a virtual robot situated in a new environment is asked to navigate to an object. Prior work has shown that imitation learning (IL) using behavior cloning (BC) on a dataset of human demonstrations achieves promising results. However, this has limitations -- 1) BC policies generalize poorly to new states, since the training mimics actions not their consequences, and 2) collecting demonstrations is expensive. On the other hand, reinforcement learning (RL) is trivially scalable, but requires careful reward engineering to achieve desirable behavior. We present PIRLNav, a two-stage learning scheme for BC pretraining on human demonstrations followed by RL-finetuning. This leads to a policy that achieves a success rate of \$65.0\textbackslash\%\$ on ObjectNav (\$+5.0\textbackslash\%\$ absolute over previous state-of-the-art). Using this BC\$\textbackslash rightarrow\$RL training recipe, we present a rigorous empirical analysis of design choices. First, we investigate whether human demonstrations can be replaced with `free' (automatically generated) sources of demonstrations, e.g. shortest paths (SP) or task-agnostic frontier exploration (FE) trajectories. We find that BC\$\textbackslash rightarrow\$RL on human demonstrations outperforms BC\$\textbackslash rightarrow\$RL on SP and FE trajectories, even when controlled for same BC-pretraining success on train, and even on a subset of val episodes where BC-pretraining success favors the SP or FE policies. Next, we study how RL-finetuning performance scales with the size of the BC pretraining dataset. We find that as we increase the size of BC-pretraining dataset and get to high BC accuracies, improvements from RL-finetuning are smaller, and that \$90\textbackslash\%\$ of the performance of our best BC\$\textbackslash rightarrow\$RL policy can be achieved with less than half the number of BC demonstrations. Finally, we analyze failure modes of our ObjectNav policies, and present guidelines for further improving them.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 8 pages + supplement},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/44TK5T7E/Ramrakhya et al. - 2023 - PIRLNav Pretraining with Imitation and RL Finetuning for ObjectNav.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/KVXYYXVM/2301.html}
}

@online{ranzingerAMRADIOAgglomerativeVision2024,
  title = {{{AM-RADIO}}: {{Agglomerative Vision Foundation Model}} -- {{Reduce All Domains Into One}}},
  shorttitle = {{{AM-RADIO}}},
  author = {Ranzinger, Mike and Heinrich, Greg and Kautz, Jan and Molchanov, Pavlo},
  date = {2024-04-30},
  eprint = {2312.06709},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.06709},
  url = {http://arxiv.org/abs/2312.06709},
  urldate = {2026-01-11},
  abstract = {A handful of visual foundation models (VFMs) have recently emerged as the backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are trained with distinct objectives, exhibiting unique characteristics for various downstream tasks. We find that despite their conceptual differences, these models can be effectively merged into a unified model through multi-teacher distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All Domains Into One). This integrative approach not only surpasses the performance of individual teacher models but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel-level understanding, and open vocabulary segmentation capabilities. In pursuit of the most hardware-efficient backbone, we evaluated numerous architectures in our multi-teacher distillation pipeline using the same training recipe. This led to the development of a novel architecture (E-RADIO) that exceeds the performance of its predecessors and is at least 7x faster than the teacher models. Our comprehensive benchmarking process covers downstream tasks including ImageNet classification, ADE20k semantic segmentation, COCO object detection and LLaVa-1.5 framework. Code: https://github.com/NVlabs/RADIO},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: CVPR 2024 Version 3: CVPR Camera Ready, reconfigured full paper, table 1 is now more comprehensive Version 2: Added more acknowledgements and updated table 7 with more recent results. Ensured that the link in the abstract to our code is working properly Version 3: Fix broken hyperlinks},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/KF2D2HJ3/Ranzinger et al. - 2024 - AM-RADIO Agglomerative Vision Foundation Model -- Reduce All Domains Into One.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/THRNZT2S/2312.html}
}

@online{raviSAM2Segment2024,
  title = {{{SAM}} 2: {{Segment Anything}} in {{Images}} and {{Videos}}},
  shorttitle = {{{SAM}} 2},
  author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rädle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Dollár, Piotr and Feichtenhofer, Christoph},
  date = {2024-10-28},
  eprint = {2408.00714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.00714},
  url = {http://arxiv.org/abs/2408.00714},
  urldate = {2026-01-09},
  abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Website: https://ai.meta.com/sam2},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/Y8MABWVH/Ravi et al. - 2024 - SAM 2 Segment Anything in Images and Videos.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/545W6SES/2408.html}
}

@online{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-05-09},
  eprint = {1506.02640},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.02640},
  url = {http://arxiv.org/abs/1506.02640},
  urldate = {2026-01-17},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/Z99ZWK6S/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/3ADGBSVL/1506.html}
}

@online{RoboticExplorationUsing,
  title = {Robotic {{Exploration Using Generalized Behavioral Entropy}}},
  url = {https://ieeexplore-1ieee-1org-100033czj02b7.han.technikum-wien.at/document/10608408},
  urldate = {2025-12-11},
  abstract = {This letter presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception. To do this, we introduce a measure of uncertainty that we term “Behavioral entropy”, which builds on Prelec's probability weighting from Behavioral Economics. We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon's and Renyi's. In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here. Then we use Behavioral entropy to define a new type of utility function that can guide a frontier-based environment exploration process. The approach's benefits are illustrated and compared in a Proof-of-Concept and ROS-Unity simulation environment with a Clearpath Warthog robot. We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies.},
  langid = {american},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/MTWWXC4V/10608408.html}
}

@inproceedings{ruanTaxonomySemanticInformation2022,
  title = {A {{Taxonomy}} of {{Semantic Information}} in {{Robot-Assisted Disaster Response}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Safety}}, {{Security}}, and {{Rescue Robotics}} ({{SSRR}})},
  author = {Ruan, Tianshu and Wang, Hao and Stolkin, Rustam and Chiou, Manolis},
  date = {2022-11-08},
  eprint = {2210.00125},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {285--292},
  doi = {10.1109/SSRR56537.2022.10018727},
  url = {http://arxiv.org/abs/2210.00125},
  urldate = {2025-12-11},
  abstract = {This paper proposes a taxonomy of semantic information in robot-assisted disaster response. Robots are increasingly being used in hazardous environment industries and emergency response teams to perform various tasks. Operational decision-making in such applications requires a complex semantic understanding of environments that are remote from the human operator. Low-level sensory data from the robot is transformed into perception and informative cognition. Currently, such cognition is predominantly performed by a human expert, who monitors remote sensor data such as robot video feeds. This engenders a need for AI-generated semantic understanding capabilities on the robot itself. Current work on semantics and AI lies towards the relatively academic end of the research spectrum, hence relatively removed from the practical realities of first responder teams. We aim for this paper to be a step towards bridging this divide. We first review common robot tasks in disaster response and the types of information such robots must collect. We then organize the types of semantic features and understanding that may be useful in disaster operations into a taxonomy of semantic information. We also briefly review the current state-of-the-art semantic understanding techniques. We highlight potential synergies, but we also identify gaps that need to be bridged to apply these ideas. We aim to stimulate the research that is needed to adapt, robustify, and implement state-of-the-art AI semantics methods in the challenging conditions of disasters and first responder scenarios.},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/BF73MJ3E/Ruan et al. - 2022 - A Taxonomy of Semantic Information in Robot-Assisted Disaster Response.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/JBWAHDQ4/2210.html}
}

@article{rusu3DPointCloud2008,
  title = {Towards {{3D Point}} Cloud Based Object Maps for Household Environments},
  author = {Rusu, Radu Bogdan and Marton, Zoltan Csaba and Blodow, Nico and Dolha, Mihai and Beetz, Michael},
  date = {2008-11-30},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  series = {Semantic {{Knowledge}} in {{Robotics}}},
  volume = {56},
  number = {11},
  pages = {927--941},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2008.08.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889008001140},
  urldate = {2026-01-17},
  abstract = {This article investigates the problem of acquiring 3D object maps of indoor household environments, in particular kitchens. The objects modeled in these maps include cupboards, tables, drawers and shelves, which are of particular importance for a household robotic assistant. Our mapping approach is based on PCD (point cloud data) representations. Sophisticated interpretation methods operating on these representations eliminate noise and resample the data without deleting the important details, and interpret the improved point clouds in terms of rectangular planes and 3D geometric shapes. We detail the steps of our mapping approach and explain the key techniques that make it work. The novel techniques include statistical analysis, persistent histogram features estimation that allows for a consistent registration, resampling with additional robust fitting techniques, and segmentation of the environment into meaningful regions.},
  keywords = {Environment object model,Geometrical reasoning,Point cloud data},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ABMWKS56/S0921889008001140.html}
}

@online{salimpourSimtoRealTransferMobile2025,
  title = {Sim-to-{{Real Transfer}} for {{Mobile Robots}} with {{Reinforcement Learning}}: From {{NVIDIA Isaac Sim}} to {{Gazebo}} and {{Real ROS}} 2 {{Robots}}},
  shorttitle = {Sim-to-{{Real Transfer}} for {{Mobile Robots}} with {{Reinforcement Learning}}},
  author = {Salimpour, Sahar and Peña-Queralta, Jorge and Paez-Granados, Diego and Heikkonen, Jukka and Westerlund, Tomi},
  date = {2025-01-06},
  eprint = {2501.02902},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.02902},
  url = {http://arxiv.org/abs/2501.02902},
  urldate = {2026-01-19},
  abstract = {Unprecedented agility and dexterous manipulation have been demonstrated with controllers based on deep reinforcement learning (RL), with a significant impact on legged and humanoid robots. Modern tooling and simulation platforms, such as NVIDIA Isaac Sim, have been enabling such advances. This article focuses on demonstrating the applications of Isaac in local planning and obstacle avoidance as one of the most fundamental ways in which a mobile robot interacts with its environments. Although there is extensive research on proprioception-based RL policies, the article highlights less standardized and reproducible approaches to exteroception. At the same time, the article aims to provide a base framework for end-to-end local navigation policies and how a custom robot can be trained in such simulation environment. We benchmark end-to-end policies with the state-of-the-art Nav2, navigation stack in Robot Operating System (ROS). We also cover the sim-to-real transfer process by demonstrating zero-shot transferability of policies trained in the Isaac simulator to real-world robots. This is further evidenced by the tests with different simulated robots, which show the generalization of the learned policy. Finally, the benchmarks demonstrate comparable performance to Nav2, opening the door to quick deployment of state-of-the-art end-to-end local planners for custom robot platforms, but importantly furthering the possibilities by expanding the state and action spaces or task definitions for more complex missions. Overall, with this article we introduce the most important steps, and aspects to consider, in deploying RL policies for local path planning and obstacle avoidance with Isaac Sim training, Gazebo testing, and ROS 2 for real-time inference in real robots. The code is available at https://github.com/sahars93/RL-Navigation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/HP48Z8IH/Salimpour et al. - 2025 - Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning from NVIDIA Isaac Sim to Gazebo.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/5NUUFK9A/2501.html}
}

@online{sapkotaUltralyticsYOLOEvolution2025,
  title = {Ultralytics {{YOLO Evolution}}: {{An Overview}} of {{YOLO26}}, {{YOLO11}}, {{YOLOv8}} and {{YOLOv5 Object Detectors}} for {{Computer Vision}} and {{Pattern Recognition}}},
  shorttitle = {Ultralytics {{YOLO Evolution}}},
  author = {Sapkota, Ranjan and Karkee, Manoj},
  date = {2025-10-15},
  eprint = {2510.09653},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.09653},
  url = {http://arxiv.org/abs/2510.09653},
  urldate = {2026-01-20},
  abstract = {This paper presents a comprehensive overview of the Ultralytics YOLO(You Only Look Once) family of object detectors, focusing the architectural evolution, benchmarking, deployment perspectives, and future challenges. The review begins with the most recent release, YOLO26 (or YOLOv26), which introduces key innovations including Distribution Focal Loss (DFL) removal, native NMS-free inference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label Assignment (STAL), and the MuSGD optimizer for stable training. The progression is then traced through YOLO11, with its hybrid task assignment and efficiency-focused modules; YOLOv8, which advanced with a decoupled detection head and anchor-free predictions; and YOLOv5, which established the modular PyTorch foundation that enabled modern YOLO development. Benchmarking on the MS COCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8, YOLO11, and YOLO26 (YOLOv26), alongside cross-comparisons with YOLOv12, YOLOv13, RT-DETR, and DEIM(DETR with Improved Matching). Metrics including precision, recall, F1 score, mean Average Precision, and inference speed are analyzed to highlight trade-offs between accuracy and efficiency. Deployment and application perspectives are further discussed, covering export formats, quantization strategies, and real-world use in robotics, agriculture, surveillance, and manufacturing. Finally, the paper identifies challenges and future directions, including dense-scene limitations, hybrid CNN-Transformer integration, open-vocabulary detection, and edge-aware training approaches. (Object Detection, YOLOv26, YOLO)},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/MERJ2LZZ/Sapkota and Karkee - 2025 - Ultralytics YOLO Evolution An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Co.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/CBDF8KMM/2510.html}
}

@online{savvaHabitatPlatformEmbodied2019,
  title = {Habitat: {{A Platform}} for {{Embodied AI Research}}},
  shorttitle = {Habitat},
  author = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and Parikh, Devi and Batra, Dhruv},
  date = {2019-11-25},
  eprint = {1904.01201},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.01201},
  url = {http://arxiv.org/abs/1904.01201},
  urldate = {2026-01-05},
  abstract = {We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments \{train, test\} x \{Matterport3D, Gibson\} for multiple sensors \{blind, RGB, RGBD, D\} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: ICCV 2019},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/6QJVE7UY/Savva et al. - 2019 - Habitat A Platform for Embodied AI Research.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/T7RRTKK3/1904.html}
}

@online{savvaHabitatPlatformEmbodied2019a,
  title = {Habitat: {{A Platform}} for {{Embodied AI Research}}},
  shorttitle = {Habitat},
  author = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and Parikh, Devi and Batra, Dhruv},
  date = {2019-11-25},
  eprint = {1904.01201},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.01201},
  url = {http://arxiv.org/abs/1904.01201},
  urldate = {2026-01-19},
  abstract = {We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments \{train, test\} x \{Matterport3D, Gibson\} for multiple sensors \{blind, RGB, RGBD, D\} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: ICCV 2019},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/KDFYJ8YN/Savva et al. - 2019 - Habitat A Platform for Embodied AI Research.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/BVI76N5R/1904.html}
}

@online{schwaigerOTASOpenvocabularyToken2025,
  title = {{{OTAS}}: {{Open-vocabulary Token Alignment}} for {{Outdoor Segmentation}}},
  shorttitle = {{{OTAS}}},
  author = {Schwaiger, Simon and Thalhammer, Stefan and Wöber, Wilfried and Steinbauer-Wagner, Gerald},
  date = {2025-09-22},
  eprint = {2507.08851},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2507.08851},
  url = {http://arxiv.org/abs/2507.08851},
  urldate = {2026-01-09},
  abstract = {Understanding open-world semantics is critical for robotic planning and control, particularly in unstructured outdoor environments. Existing vision-language mapping approaches typically rely on object-centric segmentation priors, which often fail outdoors due to semantic ambiguities and indistinct class boundaries. We propose OTAS - an Open-vocabulary Token Alignment method for outdoor Segmentation. OTAS addresses the limitations of open-vocabulary segmentation models by extracting semantic structure directly from the output tokens of pre-trained vision models. By clustering semantically similar structures across single and multiple views and grounding them in language, OTAS reconstructs a geometrically consistent feature field that supports open-vocabulary segmentation queries. Our method operates in a zero-shot manner, without scene-specific fine-tuning, and achieves real-time performance of up to \textasciitilde 17 fps. On the Off-Road Freespace Detection dataset, OTAS yields a modest IoU improvement over fine-tuned and open-vocabulary 2D segmentation baselines. In 3D segmentation on TartanAir, it achieves up to a 151\% relative IoU improvement compared to existing open-vocabulary mapping methods. Real-world reconstructions further demonstrate OTAS' applicability to robotic deployment. Code and a ROS 2 node are available at https://otas-segmentation.github.io/.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/UZ9C3NTN/Schwaiger et al. - 2025 - OTAS Open-vocabulary Token Alignment for Outdoor Segmentation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/8CV89T6A/2507.html}
}

@online{schwaigerUGVCBRNUnmannedGround2024,
  title = {{{UGV-CBRN}}: {{An Unmanned Ground Vehicle}} for {{Chemical}}, {{Biological}}, {{Radiological}}, and {{Nuclear Disaster Response}}},
  shorttitle = {{{UGV-CBRN}}},
  author = {Schwaiger, Simon and Muster, Lucas and Novotny, Georg and Schebek, Michael and Wöber, Wilfried and Thalhammer, Stefan and Böhm, Christoph},
  date = {2024-09-20},
  eprint = {2406.14385},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.14385},
  url = {http://arxiv.org/abs/2406.14385},
  urldate = {2026-01-02},
  abstract = {Robotic search and rescue (SAR) supports response teams by accelerating disaster assessment and by keeping operators away from hazardous environments. In the event of a chemical, biological, radiological, and nuclear (CBRN) disaster, robots are deployed to identify and locate radiation sources. Human responders then assess the situation and neutralize the danger. The presented system takes a step toward enhanced integration of robots into SAR teams. Integrating autonomous radiation mapping with semi-autonomous substance sampling and online analysis of the CBRN threat lets the human operator localize and assess the threat from a safe distance. Two LiDARs, an IMU, and a Geiger counter are used for mapping the surrounding area and localizing potential radiation sources. A mobile manipulator with six Degrees of Freedom manipulates valves and samples substances that are analyzed by an onboard Raman spectrometer. The human operator monitors the mission's progression from a remote location defining target locations and directing the semi-autonomous manipulation processes. Diverse recovery behaviours aid robot deployment, system state monitoring, as well as recovery of hard- and software. Field tests showcase the capabilities of the presented system during trials at the CBRN disaster response challenge European Robotics Hackathon (EnRicH). We provide recorded sensor data and implemented software through a GitHub repository: https://github.com/TW-Robotics/search-and-rescue-robot-2024.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/SYKUFV7W/Schwaiger et al. - 2024 - UGV-CBRN An Unmanned Ground Vehicle for Chemical, Biological, Radiological, and Nuclear Disaster Re.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/IMLX46I7/2406.html}
}

@online{sharirImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}, {{What}} Is a {{Video Worth}}?},
  author = {Sharir, Gilad and Noy, Asaf and Zelnik-Manor, Lihi},
  date = {2021-05-27},
  eprint = {2103.13915},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.13915},
  url = {http://arxiv.org/abs/2103.13915},
  urldate = {2026-01-15},
  abstract = {Leading methods in the domain of action recognition try to distill information from both the spatial and temporal dimensions of an input video. Methods that reach State of the Art (SotA) accuracy, usually make use of 3D convolution layers as a way to abstract the temporal information from video frames. The use of such convolutions requires sampling short clips from the input video, where each clip is a collection of closely sampled frames. Since each short clip covers a small fraction of an input video, multiple clips are sampled at inference in order to cover the whole temporal length of the video. This leads to increased computational load and is impractical for real-world applications. We address the computational bottleneck by significantly reducing the number of frames required for inference. Our approach relies on a temporal transformer that applies global attention over video frames, and thus better exploits the salient information in each frame. Therefore our approach is very input efficient, and can achieve SotA results (on Kinetics dataset) with a fraction of the data (frames per video), computation and latency. Specifically on Kinetics-400, we reach \$80.5\$ top-1 accuracy with \$\textbackslash times 30\$ less frames per video, and \$\textbackslash times 40\$ faster inference than the current leading method. Code is available at: https://github.com/Alibaba-MIIL/STAM},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/RFLPCBEN/Sharir et al. - 2021 - An Image is Worth 16x16 Words, What is a Video Worth.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/M93QJ3SQ/2103.html}
}

@online{straubReplicaDatasetDigital2019,
  title = {The {{Replica Dataset}}: {{A Digital Replica}} of {{Indoor Spaces}}},
  shorttitle = {The {{Replica Dataset}}},
  author = {Straub, Julian and Whelan, Thomas and Ma, Lingni and Chen, Yufan and Wijmans, Erik and Green, Simon and Engel, Jakob J. and Mur-Artal, Raul and Ren, Carl and Verma, Shobhit and Clarkson, Anton and Yan, Mingfei and Budge, Brian and Yan, Yajie and Pan, Xiaqing and Yon, June and Zou, Yuyang and Leon, Kimberly and Carter, Nigel and Briales, Jesus and Gillingham, Tyler and Mueggler, Elias and Pesqueira, Luis and Savva, Manolis and Batra, Dhruv and Strasdat, Hauke M. and Nardi, Renzo De and Goesele, Michael and Lovegrove, Steven and Newcombe, Richard},
  date = {2019-06-13},
  eprint = {1906.05797},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.05797},
  url = {http://arxiv.org/abs/1906.05797},
  urldate = {2026-01-05},
  abstract = {We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/KJFXQ9KY/Straub et al. - 2019 - The Replica Dataset A Digital Replica of Indoor Spaces.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/62D6ZF7Q/1906.html}
}

@online{tellaroliFrontierBasedExplorationMultiRobot2024,
  title = {Frontier-{{Based Exploration}} for {{Multi-Robot Rendezvous}} in {{Communication-Restricted Unknown Environments}}},
  author = {Tellaroli, Mauro and Luperto, Matteo and Antonazzi, Michele and Basilico, Nicola},
  date = {2024-07-19},
  eprint = {2403.11617},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.11617},
  url = {http://arxiv.org/abs/2403.11617},
  urldate = {2026-01-03},
  abstract = {Multi-robot rendezvous and exploration are fundamental challenges in the domain of mobile robotic systems. This paper addresses multi-robot rendezvous within an initially unknown environment where communication is only possible after the rendezvous. Traditionally, exploration has been focused on rapidly mapping the environment, often leading to suboptimal rendezvous performance in later stages. We adapt a standard frontier-based exploration technique to integrate exploration and rendezvous into a unified strategy, with a mechanism that allows robots to re-visit previously explored regions thus enhancing rendezvous opportunities. We validate our approach in 3D realistic simulations using ROS, showcasing its effectiveness in achieving faster rendezvous times compared to exploration strategies.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/CXSQP73T/Tellaroli et al. - 2024 - Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environmen.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/XB62J83R/2403.html}
}

@online{thomasPolicyGradientMethods2017,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}} and {{Action-Dependent Baselines}}},
  author = {Thomas, Philip S. and Brunskill, Emma},
  date = {2017-06-20},
  eprint = {1706.06643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.06643},
  url = {http://arxiv.org/abs/1706.06643},
  urldate = {2026-01-06},
  abstract = {We show how an action-dependent baseline can be used by the policy gradient theorem using function approximation, originally presented with action-independent baselines by (Sutton et al. 2000).},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/B7SCR2PA/Thomas and Brunskill - 2017 - Policy Gradient Methods for Reinforcement Learning with Function Approximation and Action-Dependent.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/3MTWXN3R/1706.html}
}

@book{thrunProbabilisticRobotics2006,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  date = {2006},
  series = {Intelligent Robotics and Autonomous Agents},
  publisher = {MIT Press},
  location = {Cambridge, Massachusetts London},
  abstract = {An introduction to the techniques and algorithms of the newest field in robotics. Probabilistic robotics is a new and growing area in robotics, concerned with perception and control in the face of uncertainty. Building on the field of mathematical statistics, probabilistic robotics endows robots with a new level of robustness in real-world situations. This book introduces the reader to a wealth of techniques and algorithms in the field. All algorithms are based on a single overarching mathematical foundation. Each chapter provides example implementations in pseudo code, detailed mathematical derivations, discussions from a practitioner's perspective, and extensive lists of exercises and class projects. The book's Web site, www.probabilistic-robotics.org, has additional material. The book is relevant for anyone involved in robotic software development and scientific research. It will also be of interest to applied statisticians and engineers dealing with real-world sensor data},
  isbn = {978-0-262-20162-9 978-0-262-30380-4},
  langid = {english},
  pagetotal = {1},
  note = {Description based on publisher supplied metadata and other sources}
}

@online{topiwalaFrontierBasedExploration2018,
  title = {Frontier {{Based Exploration}} for {{Autonomous Robot}}},
  author = {Topiwala, Anirudh and Inani, Pranav and Kathpal, Abhishek},
  date = {2018-06-10},
  eprint = {1806.03581},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.03581},
  url = {http://arxiv.org/abs/1806.03581},
  urldate = {2025-12-11},
  abstract = {Exploration is process of selecting target points that yield the biggest contribution to a specific gain function at an initially unknown environment. Frontier-based exploration is the most common approach to exploration, wherein frontiers are regions on the boundary between open space and unexplored space. By moving to a new frontier, we can keep building the map of the environment, until there are no new frontiers left to detect. In this paper, an autonomous frontier-based exploration strategy, namely Wavefront Frontier Detector (WFD) is described and implemented on Gazebo Simulation Environment as well as on hardware platform, i.e. Kobuki TurtleBot using Robot Operating System (ROS). The advantage of this algorithm is that the robot can explore large open spaces as well as small cluttered spaces. Further, the map generated from this technique is compared and validated with the map generated using turtlebot\_teleop ROS Package.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8AUGCNII/Topiwala et al. - 2018 - Frontier Based Exploration for Autonomous Robot.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WIHJKPVN/1806.html}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-12-11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 15 pages, 5 figures},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8LCESDHM/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WHZ65CNY/1706.html}
}

@online{wangYOLOERealTimeSeeing2025,
  title = {{{YOLOE}}: {{Real-Time Seeing Anything}}},
  shorttitle = {{{YOLOE}}},
  author = {Wang, Ao and Liu, Lihao and Chen, Hui and Lin, Zijia and Han, Jungong and Ding, Guiguang},
  date = {2025-10-17},
  eprint = {2503.07465},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.07465},
  url = {http://arxiv.org/abs/2503.07465},
  urldate = {2026-01-05},
  abstract = {Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3\$\textbackslash times\$ less training cost and 1.4\$\textbackslash times\$ inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP\$\textasciicircum b\$ and 0.4 AP\$\textasciicircum m\$ gains over closed-set YOLOv8-L with nearly 4\$\textbackslash times\$ less training time. Code and models are available at https://github.com/THU-MIG/yoloe.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ICCV 2025 Camera-ready Version},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/XTZ6WDR7/Wang et al. - 2025 - YOLOE Real-Time Seeing Anything.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/CDRRU6C4/2503.html}
}

@online{wangYOLOv7TrainableBagoffreebies2022,
  title = {{{YOLOv7}}: {{Trainable}} Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors},
  shorttitle = {{{YOLOv7}}},
  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  date = {2022-07-06},
  eprint = {2207.02696},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.02696},
  url = {http://arxiv.org/abs/2207.02696},
  urldate = {2026-01-06},
  abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/K9WRM94X/Wang et al. - 2022 - YOLOv7 Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/K3HN4JHE/2207.html}
}

@online{westphalGeneralizedInformationBottleneck2025,
  title = {A {{Generalized Information Bottleneck Theory}} of {{Deep Learning}}},
  author = {Westphal, Charles and Hailes, Stephen and Musolesi, Mirco},
  date = {2025-10-14},
  eprint = {2509.26327},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2509.26327},
  url = {http://arxiv.org/abs/2509.26327},
  urldate = {2026-01-10},
  abstract = {The Information Bottleneck (IB) principle offers a compelling theoretical framework to understand how neural networks (NNs) learn. However, its practical utility has been constrained by unresolved theoretical ambiguities and significant challenges in accurate estimation. In this paper, we present a \textbackslash textit\{Generalized Information Bottleneck (GIB)\} framework that reformulates the original IB principle through the lens of synergy, i.e., the information obtainable only through joint processing of features. We provide theoretical and empirical evidence demonstrating that synergistic functions achieve superior generalization compared to their non-synergistic counterparts. Building on these foundations we re-formulate the IB using a computable definition of synergy based on the average interaction information (II) of each feature with those remaining. We demonstrate that the original IB objective is upper bounded by our GIB in the case of perfect estimation, ensuring compatibility with existing IB theory while addressing its limitations. Our experimental results demonstrate that GIB consistently exhibits compression phases across a wide range of architectures (including those with \textbackslash textit\{ReLU\} activations where the standard IB fails), while yielding interpretable dynamics in both CNNs and Transformers and aligning more closely with our understanding of adversarial robustness.},
  pubstate = {prepublished},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/DW6A3LBC/Westphal et al. - 2025 - A Generalized Information Bottleneck Theory of Deep Learning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/C97VB9RP/2509.html}
}

@online{xiaGibsonEnvRealWorld2018,
  title = {Gibson {{Env}}: {{Real-World Perception}} for {{Embodied Agents}}},
  shorttitle = {Gibson {{Env}}},
  author = {Xia, Fei and Zamir, Amir and He, Zhi-Yang and Sax, Alexander and Malik, Jitendra and Savarese, Silvio},
  date = {2018-08-31},
  eprint = {1808.10654},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1808.10654},
  url = {http://arxiv.org/abs/1808.10654},
  urldate = {2026-01-06},
  abstract = {Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, "Goggles", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Access the code, dataset, and project website at http://gibsonenv.vision/ . CVPR 2018},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ZNY4ZRRZ/Xia et al. - 2018 - Gibson Env Real-World Perception for Embodied Agents.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/8XISCSLS/1808.html}
}

@inproceedings{yamauchiFrontierbasedApproachAutonomous1997,
  title = {A Frontier-Based Approach for Autonomous Exploration},
  booktitle = {Proceedings 1997 {{IEEE International Symposium}} on {{Computational Intelligence}} in {{Robotics}} and {{Automation CIRA}}'97. '{{Towards New Computational Principles}} for {{Robotics}} and {{Automation}}'},
  author = {Yamauchi, B.},
  date = {1997-07},
  pages = {146--151},
  doi = {10.1109/CIRA.1997.613851},
  url = {https://ieeexplore.ieee.org/document/613851/authors},
  urldate = {2026-01-14},
  abstract = {We introduce a new approach for exploration based on the concept of frontiers, regions on the boundary between open space and unexplored space. By moving to new frontiers, a mobile robot can extend its map into new territory until the entire environment has been explored. We describe a method for detecting frontiers in evidence grids and navigating to these frontiers. We also introduce a technique for minimizing specular reflections in evidence grids using laser-limited sonar. We have tested this approach with a real mobile robot, exploring real-world office environments cluttered with a variety of obstacles. An advantage of our approach is its ability to explore both large open spaces and narrow cluttered spaces, with walls and obstacles in arbitrary orientation.},
  eventtitle = {1997 {{IEEE International Symposium}} on {{Computational Intelligence}} in {{Robotics}} and {{Automation CIRA}}'97. '{{Towards New Computational Principles}} for {{Robotics}} and {{Automation}}'},
  keywords = {Artificial intelligence,Humans,Indoor environments,Laboratories,Mobile robots,Orbital robotics,Sonar navigation,Space exploration,Testing},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/CYHBQADR/Yamauchi - 1997 - A frontier-based approach for autonomous exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/ECTAQ449/authors.html}
}

@online{yamazakiOpenFusionRealtimeOpenVocabulary2023,
  title = {Open-{{Fusion}}: {{Real-time Open-Vocabulary 3D Mapping}} and {{Queryable Scene Representation}}},
  shorttitle = {Open-{{Fusion}}},
  author = {Yamazaki, Kashu and Hanyu, Taisei and Vo, Khoa and Pham, Thang and Tran, Minh and Doretto, Gianfranco and Nguyen, Anh and Le, Ngan},
  date = {2023-10-05},
  eprint = {2310.03923},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.03923},
  url = {http://arxiv.org/abs/2310.03923},
  urldate = {2025-12-11},
  abstract = {Precise 3D environmental mapping is pivotal in robotics. Existing methods often rely on predefined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, a groundbreaking approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pre-trained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D segmentation for open-vocabulary without necessitating additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: https://uark-aicv.github.io/OpenFusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/TXJ928CY/Yamazaki et al. - 2023 - Open-Fusion Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/5UNB552N/2310.html}
}

@online{yokoyamaVLFMVisionLanguageFrontier2023,
  title = {{{VLFM}}: {{Vision-Language Frontier Maps}} for {{Zero-Shot Semantic Navigation}}},
  shorttitle = {{{VLFM}}},
  author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv and Wang, Jiuguang and Bucher, Bernadette},
  date = {2023},
  doi = {10.48550/ARXIV.2312.03275},
  url = {https://arxiv.org/abs/2312.03275},
  urldate = {2025-12-11},
  abstract = {Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Robotics (cs.RO)}
}

@online{zakkaMuJoCoPlayground2025,
  title = {{{MuJoCo Playground}}},
  author = {Zakka, Kevin and Tabanpour, Baruch and Liao, Qiayuan and Haiderbhai, Mustafa and Holt, Samuel and Luo, Jing Yuan and Allshire, Arthur and Frey, Erik and Sreenath, Koushil and Kahrs, Lueder A. and Sferrazza, Carmelo and Tassa, Yuval and Abbeel, Pieter},
  date = {2025-02-12},
  eprint = {2502.08844},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.08844},
  url = {http://arxiv.org/abs/2502.08844},
  urldate = {2026-01-19},
  abstract = {We introduce MuJoCo Playground, a fully open-source framework for robot learning built with MJX, with the express goal of streamlining simulation, training, and sim-to-real transfer onto robots. With a simple "pip install playground", researchers can train policies in minutes on a single GPU. Playground supports diverse robotic platforms, including quadrupeds, humanoids, dexterous hands, and robotic arms, enabling zero-shot sim-to-real transfer from both state and pixel inputs. This is achieved through an integrated stack comprising a physics engine, batch renderer, and training environments. Along with video results, the entire framework is freely available at playground.mujoco.org},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/6TCX9KQP/Zakka et al. - 2025 - MuJoCo Playground.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/PVIFQGS6/2502.html}
}

@online{zareianOpenVocabularyObjectDetection2021,
  title = {Open-{{Vocabulary Object Detection Using Captions}}},
  author = {Zareian, Alireza and Rosa, Kevin Dela and Hu, Derek Hao and Chang, Shih-Fu},
  date = {2021-03-14},
  eprint = {2011.10678},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.10678},
  url = {http://arxiv.org/abs/2011.10678},
  urldate = {2026-01-03},
  abstract = {Despite the remarkable accuracy of deep neural networks in object detection, they are costly to train and scale due to supervision requirements. Particularly, learning more object categories typically requires proportionally more bounding box annotations. Weakly supervised and zero-shot learning techniques have been explored to scale object detectors to more categories with less supervision, but they have not been as successful and widely adopted as supervised models. In this paper, we put forth a novel formulation of the object detection problem, namely open-vocabulary object detection, which is more general, more practical, and more effective than weakly supervised and zero-shot approaches. We propose a new method to train object detectors using bounding box annotations for a limited set of object categories, as well as image-caption pairs that cover a larger variety of objects at a significantly lower cost. We show that the proposed method can detect and localize objects for which no bounding box annotation is provided during training, at a significantly higher accuracy than zero-shot approaches. Meanwhile, objects with bounding box annotation can be detected almost as accurately as supervised methods, which is significantly better than weakly supervised baselines. Accordingly, we establish a new state of the art for scalable object detection.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: To be presented at CVPR 2021 (oral paper)},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/2Z46A547/Zareian et al. - 2021 - Open-Vocabulary Object Detection Using Captions.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/GNF5RLZW/2011.html}
}

@online{zhaiSigmoidLossLanguage2023,
  title = {Sigmoid {{Loss}} for {{Language Image Pre-Training}}},
  author = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  date = {2023-09-27},
  eprint = {2303.15343},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.15343},
  url = {http://arxiv.org/abs/2303.15343},
  urldate = {2026-01-11},
  abstract = {We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5\% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at https://github.com/google-research/big\_vision and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ICCV'23 Oral. arXiv v2: fix typo in pseudocode; v3: clarify t vs t' init; v4: add SigLIP Base, Large, Shape-Optimized 400M results. Models released at: https://github.com/google-research/big\_vision. Xiaohua and Lucas contributed equally},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/4VXPPNNE/Zhai et al. - 2023 - Sigmoid Loss for Language Image Pre-Training.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/AQT7E7FF/2303.html}
}

@article{zhangFlexibleNewTechnique2000,
  title = {A Flexible New Technique for Camera Calibration},
  author = {Zhang, Z.},
  date = {2000-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {11},
  pages = {1330--1334},
  issn = {1939-3539},
  doi = {10.1109/34.888718},
  url = {https://ieeexplore.ieee.org/document/888718/citations},
  urldate = {2026-01-18},
  abstract = {We propose a flexible technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use.},
  keywords = {Calibration,Cameras,Closed-form solution,Computer simulation,Computer vision,Layout,Lenses,Maximum likelihood estimation,Nonlinear distortion,Testing},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/QNNITHB8/citations.html}
}

@article{zhanSemanticExplorationDense2025,
  title = {Semantic {{Exploration}} and {{Dense Mapping}} of {{Complex Environments}} Using {{Ground Robot}} with {{Panoramic LiDAR-Camera Fusion}}},
  author = {Zhan, Xiaoyang and Zhou, Shixin and Yang, Qianqian and Zhao, Yixuan and Liu, Hao and Ramineni, Srinivas Chowdary and Shimada, Kenji},
  date = {2025-11},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {10},
  number = {11},
  eprint = {2505.22880},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {11196--11203},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2025.3609216},
  url = {http://arxiv.org/abs/2505.22880},
  urldate = {2025-12-11},
  abstract = {This paper presents a system for autonomous semantic exploration and dense semantic target mapping of a complex unknown environment using a ground robot equipped with a LiDAR-panoramic camera suite. Existing approaches often struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal. To fill this gap, we propose a complete system combining mapping and planning. We first redefine the task as completing both geometric coverage and semantic viewpoint observation. We then manage semantic and geometric viewpoints separately and propose a novel Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This enables explicit multi-view semantic inspection and voxel coverage without unnecessary repetition. Building on this, we develop a hierarchical planner to ensure efficient global coverage. In addition, we propose a Safe Aggressive Exploration State Machine, which allows aggressive exploration behavior while ensuring the robot's safety. Our system includes a plug-and-play semantic target mapping module that integrates seamlessly with state-of-the-art SLAM algorithms for pointcloud-level dense semantic target mapping. We validate our approach through extensive experiments in both realistic simulations and complex real-world environments. Simulation results show that our planner achieves faster exploration and shorter travel distances while guaranteeing a specified number of multi-view inspections. Real-world experiments further confirm the system's effectiveness in achieving accurate dense semantic object mapping of unstructured environments.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  note = {Comment: Accepted by IEEE Robotics and Automation Letters},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/869W3E5V/Zhan et al. - 2025 - Semantic Exploration and Dense Mapping of Complex Environments using Ground Robot with Panoramic LiD.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/V5CCNKQI/2505.html}
}

@inproceedings{zhouBuildingAICPSNVIDIA2024,
  title = {Towards {{Building AI-CPS}} with {{NVIDIA Isaac Sim}}: {{An Industrial Benchmark}} and {{Case Study}} for {{Robotics Manipulation}}},
  shorttitle = {Towards {{Building AI-CPS}} with {{NVIDIA Isaac Sim}}},
  booktitle = {Proceedings of the 46th {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}}},
  author = {Zhou, Zhehua and Song, Jiayang and Xie, Xuan and Shu, Zhan and Ma, Lei and Liu, Dikai and Yin, Jianxiong and See, Simon},
  date = {2024-04-14},
  eprint = {2308.00055},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {263--274},
  doi = {10.1145/3639477.3639740},
  url = {http://arxiv.org/abs/2308.00055},
  urldate = {2026-01-19},
  abstract = {As a representative cyber-physical system (CPS), robotic manipulator has been widely adopted in various academic research and industrial processes, indicating its potential to act as a universal interface between the cyber and the physical worlds. Recent studies in robotics manipulation have started employing artificial intelligence (AI) approaches as controllers to achieve better adaptability and performance. However, the inherent challenge of explaining AI components introduces uncertainty and unreliability to these AI-enabled robotics systems, necessitating a reliable development platform for system design and performance assessment. As a foundational step towards building reliable AI-enabled robotics systems, we propose a public industrial benchmark for robotics manipulation in this paper. It leverages NVIDIA Omniverse Isaac Sim as the simulation platform, encompassing eight representative manipulation tasks and multiple AI software controllers. An extensive evaluation is conducted to analyze the performance of AI controllers in solving robotics manipulation tasks, enabling a thorough understanding of their effectiveness. To further demonstrate the applicability of our benchmark, we develop a falsification framework that is compatible with physical simulators and OpenAI Gym environments. This framework bridges the gap between traditional testing methods and modern physics engine-based simulations. The effectiveness of different optimization methods in falsifying AI-enabled robotics manipulation with physical simulators is examined via a falsification test. Our work not only establishes a foundation for the design and development of AI-enabled robotics systems but also provides practical experience and guidance to practitioners in this field, promoting further research in this critical academic and industrial domain.},
  keywords = {Computer Science - Robotics,Computer Science - Software Engineering},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8U6HRQ3Z/Zhou et al. - 2024 - Towards Building AI-CPS with NVIDIA Isaac Sim An Industrial Benchmark and Case Study for Robotics M.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DQHNLS8T/2308.html}
}

@online{zhouESCExplorationSoft2023,
  title = {{{ESC}}: {{Exploration}} with {{Soft Commonsense Constraints}} for {{Zero-shot Object Navigation}}},
  shorttitle = {{{ESC}}},
  author = {Zhou, Kaiwen and Zheng, Kaizhi and Pryor, Connor and Shen, Yilin and Jin, Hongxia and Getoor, Lise and Wang, Xin Eric},
  date = {2023-07-06},
  eprint = {2301.13166},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.13166},
  url = {http://arxiv.org/abs/2301.13166},
  urldate = {2025-12-11},
  abstract = {The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method improves significantly over baselines, and achieves new state-of-the-art results for zero-shot object navigation (e.g., 288\% relative Success Rate improvement than CoW on MP3D).},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/LS4TJXLV/Zhou et al. - 2023 - ESC Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/AKX9RGIY/2301.html}
}

@online{zouSegmentEverythingEverywhere2023,
  title = {Segment {{Everything Everywhere All}} at {{Once}}},
  author = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  date = {2023-07-11},
  eprint = {2304.06718},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.06718},
  url = {http://arxiv.org/abs/2304.06718},
  urldate = {2025-12-11},
  abstract = {In this work, we present SEEM, a promptable and interactive model for segmenting everything everywhere all at once in an image, as shown in Fig.1. In SEEM, we propose a novel decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal segmentation interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata: i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks; iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from decoder to image features; and iv) Semantic-awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. Notably, our single SEEM model achieves competitive performance across interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity for generalization to novel prompts or their combinations, rendering it a readily universal image segmentation interface.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JK3PPLKG/Zou et al. - 2023 - Segment Everything Everywhere All at Once.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/ABMEYVHQ/2304.html}
}
