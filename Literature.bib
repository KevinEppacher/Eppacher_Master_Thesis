@online{alamaRayFrontsOpenSetSemantic2025,
  title = {{{RayFronts}}: {{Open-Set Semantic Ray Frontiers}} for {{Online Scene Understanding}} and {{Exploration}}},
  shorttitle = {{{RayFronts}}},
  author = {Alama, Omar and Bhattacharya, Avigyan and He, Haoyang and Kim, Seungchan and Qiu, Yuheng and Wang, Wenshan and Ho, Cherie and Keetha, Nikhil and Scherer, Sebastian},
  date = {2025-04-09},
  eprint = {2504.06994},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.06994},
  url = {http://arxiv.org/abs/2504.06994},
  urldate = {2025-12-11},
  abstract = {Open-set semantic mapping is crucial for open-world robots. Current mapping approaches either are limited by the depth range or only map beyond-range entities in constrained settings, where overall they fail to combine within-range and beyond-range observations. Furthermore, these methods make a trade-off between fine-grained semantics and efficiency. We introduce RayFronts, a unified representation that enables both dense and beyond-range efficient semantic mapping. RayFronts encodes task-agnostic open-set semantics to both in-range voxels and beyond-range rays encoded at map boundaries, empowering the robot to reduce search volumes significantly and make informed decisions both within \& beyond sensory range, while running at 8.84 Hz on an Orin AGX. Benchmarking the within-range semantics shows that RayFronts's fine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation performance while improving throughput by 16.5x. Traditionally, online mapping performance is entangled with other system components, complicating evaluation. We propose a planner-agnostic evaluation framework that captures the utility for online beyond-range search and exploration, and show RayFronts reduces search volume 2.2x more efficiently than the closest online baselines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/9YK7FELL/Alama et al. - 2025 - RayFronts Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/IUS9YCRU/2504.html}
}

@article{almadhounSurveyInspectingStructures2016,
  title = {A Survey on Inspecting Structures Using Robotic Systems},
  author = {Almadhoun, Randa and Taha, Tarek and Seneviratne, Lakmal and Dias, Jorge and Cai, Guowei},
  date = {2016-11-28},
  journaltitle = {International Journal of Advanced Robotic Systems},
  shortjournal = {International Journal of Advanced Robotic Systems},
  volume = {13},
  doi = {10.1177/1729881416663664},
  abstract = {Advancements in robotics and autonomous systems are being deployed nowadays in many application domains such as search and rescue, industrial automation, domestic services and healthcare. These systems are developed to tackle tasks in some of the most challenging, labour intensive and dangerous environments. Inspecting structures (e.g. bridges, buildings, ships, wind turbines and aircrafts) is considered a hard task for humans to perform and of critical importance since missing any details could affect the structure’s performance and integrity. Additionally, structure inspection is time and resource intensive and should be performed as efficiently and accurately as possible. Inspecting various structures has been reported in the literature using different robotic platforms to: inspect difficult to reach areas and detect various types of faults and anomalies. Typically, inspection missions involve performing three main tasks: coverage path planning, shape, model or surface reconstruction and the actual inspection of the structure. Coverage path planning ensures the generation of an optimized path that guarantees the complete coverage of the structure of interest in order to gather highly accurate information to be used for shape/model reconstruction. This article aims to provide an overview of the recent work and breakthroughs in the field of coverage path planning and model reconstruction, with focus on 3D reconstruction, for the purpose of robotic inspection.},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/UCI3I6EJ/Almadhoun et al. - 2016 - A survey on inspecting structures using robotic systems.pdf}
}

@inproceedings{bourgaultInformationBasedAdaptive2002,
  title = {Information {{Based Adaptive Robotic Exploration}}},
  author = {Bourgault, Frédéric and Makarenko, Alexei and Williams, Stefan and Grocholsky, Ben and Durrant-Whyte, Hugh},
  date = {2002-02-01},
  volume = {1},
  pages = {540-545 vol.1},
  doi = {10.1109/IRDS.2002.1041446},
  abstract = {Exploration involving mapping and concurrent localization in an unknown environment is a pervasive task in mobile robotics. In general, the accuracy of the mapping process depends directly on the accuracy of the localization process. This paper address the problem of maximizing the accuracy of the map building process during exploration by adaptively selecting control actions that maximize localisation accuracy. The map building and exploration task is modeled using an Occupancy Grid (OG) with concurrent localisation performed using a feature-based Simultaneous Localisation And Mapping (SLAM) algorithm. Adaptive sensing aims at maximizing the map information by simultaneously maximizing the expected Shannon information gain (Mutual Information) on the OG map and minimizing the uncertainty of the vehicle pose and map feature uncertainty in the SLAM process. The resulting map building system is demonstrated in an indoor environment using data from a laser scanner mounted on a mobile platform.},
  isbn = {978-0-7803-7398-3},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/C3L3R6FA/Bourgault et al. - 2002 - Information Based Adaptive Robotic Exploration.pdf}
}

@online{brohanRT2VisionLanguageActionModels2023,
  title = {{{RT-2}}: {{Vision-Language-Action Models Transfer Web Knowledge}} to {{Robotic Control}}},
  shorttitle = {{{RT-2}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and Florence, Pete and Fu, Chuyuan and Arenas, Montse Gonzalez and Gopalakrishnan, Keerthana and Han, Kehang and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ichter, Brian and Irpan, Alex and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Lisa and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Michalewski, Henryk and Mordatch, Igor and Pertsch, Karl and Rao, Kanishka and Reymann, Krista and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sermanet, Pierre and Singh, Jaspiar and Singh, Anikait and Soricut, Radu and Tran, Huong and Vanhoucke, Vincent and Vuong, Quan and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Wu, Jialin and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  date = {2023-07-28},
  eprint = {2307.15818},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.15818},
  url = {http://arxiv.org/abs/2307.15818},
  urldate = {2026-01-02},
  abstract = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Website: https://robotics-transformer.github.io/},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WF5428KW/Brohan et al. - 2023 - RT-2 Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/U3KCFQT5/2307.html}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2026-01-02},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 40+32 pages},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/VFZNX4TP/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/FJG4YT6J/2005.html}
}

@online{buschOneMapFind2025,
  title = {One {{Map}} to {{Find Them All}}: {{Real-time Open-Vocabulary Mapping}} for {{Zero-shot Multi-Object Navigation}}},
  shorttitle = {One {{Map}} to {{Find Them All}}},
  author = {Busch, Finn Lukas and Homberger, Timon and Ortega-Peimbert, Jesús and Yang, Quantao and Andersson, Olov},
  date = {2025-03-03},
  eprint = {2409.11764},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.11764},
  url = {http://arxiv.org/abs/2409.11764},
  urldate = {2025-12-11},
  abstract = {The capability to efficiently search for objects in complex environments is fundamental for many real-world robot applications. Recent advances in open-vocabulary vision models have resulted in semantically-informed object navigation methods that allow a robot to search for an arbitrary object without prior training. However, these zero-shot methods have so far treated the environment as unknown for each consecutive query. In this paper we introduce a new benchmark for zero-shot multi-object navigation, allowing the robot to leverage information gathered from previous searches to more efficiently find new objects. To address this problem we build a reusable open-vocabulary feature map tailored for real-time object search. We further propose a probabilistic-semantic map update that mitigates common sources of errors in semantic feature extraction and leverage this semantic uncertainty for informed multi-object exploration. We evaluate our method on a set of object navigation tasks in both simulation as well as with a real robot, running in real-time on a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art approaches both on single and multi-object navigation tasks. Additional videos, code and the multi-object navigation benchmark will be available on https://finnbsch.github.io/OneMap.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5TN5PZLW/Busch et al. - 2025 - One Map to Find Them All Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/U2WXYRMQ/2409.html}
}

@online{changMatterport3DLearningRGBD2017,
  title = {{{Matterport3D}}: {{Learning}} from {{RGB-D Data}} in {{Indoor Environments}}},
  shorttitle = {{{Matterport3D}}},
  author = {Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Nießner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
  date = {2017-09-18},
  eprint = {1709.06158},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1709.06158},
  url = {http://arxiv.org/abs/1709.06158},
  urldate = {2026-01-06},
  abstract = {Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/73GM9BQP/Chang et al. - 2017 - Matterport3D Learning from RGB-D Data in Indoor Environments.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/JGMFFWZP/1709.html}
}

@online{chaplotObjectGoalNavigation2020,
  title = {Object {{Goal Navigation}} Using {{Goal-Oriented Semantic Exploration}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
  date = {2020-07-02},
  eprint = {2007.00643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.00643},
  url = {http://arxiv.org/abs/2007.00643},
  urldate = {2025-12-11},
  abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Winner of the CVPR 2020 AI-Habitat Object Goal Navigation Challenge. See the project webpage at https://devendrachaplot.github.io/projects/semantic-exploration.html},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/S9CD9YI9/Chaplot et al. - 2020 - Object Goal Navigation using Goal-Oriented Semantic Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DGZ6KSBI/2007.html}
}

@online{chaplotObjectGoalNavigation2020b,
  title = {Object {{Goal Navigation}} Using {{Goal-Oriented Semantic Exploration}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
  date = {2020-07-02},
  eprint = {2007.00643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.00643},
  url = {http://arxiv.org/abs/2007.00643},
  urldate = {2026-01-03},
  abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Winner of the CVPR 2020 AI-Habitat Object Goal Navigation Challenge. See the project webpage at https://devendrachaplot.github.io/projects/semantic-exploration.html},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/U5XWI5MD/Chaplot et al. - 2020 - Object Goal Navigation using Goal-Oriented Semantic Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WLZEGYWZ/2007.html}
}

@online{chengYOLOWorldRealTimeOpenVocabulary2024,
  title = {{{YOLO-World}}: {{Real-Time Open-Vocabulary Object Detection}}},
  shorttitle = {{{YOLO-World}}},
  author = {Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
  date = {2024-02-22},
  eprint = {2401.17270},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.17270},
  url = {http://arxiv.org/abs/2401.17270},
  urldate = {2026-01-08},
  abstract = {The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Work still in progress. Code \& models are available at: https://github.com/AILab-CVC/YOLO-World},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/KSH65BFR/Cheng et al. - 2024 - YOLO-World Real-Time Open-Vocabulary Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/PUH7ALVF/2401.html}
}

@online{chenHowNotTrain2023,
  title = {How {{To Not Train Your Dragon}}: {{Training-free Embodied Object Goal Navigation}} with {{Semantic Frontiers}}},
  shorttitle = {How {{To Not Train Your Dragon}}},
  author = {Chen, Junting and Li, Guohao and Kumar, Suryansh and Ghanem, Bernard and Yu, Fisher},
  date = {2023-05-26},
  eprint = {2305.16925},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.16925},
  url = {http://arxiv.org/abs/2305.16925},
  urldate = {2025-12-11},
  abstract = {Object goal navigation is an important problem in Embodied AI that involves guiding the agent to navigate to an instance of the object category in an unknown environment -- typically an indoor scene. Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches, \textbackslash eg, end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are typically costly to train and difficult to debug, leading to a lack of transferability and explainability. Inspired by recent successes in combining classical and learning methods, we present a modular and training-free solution, which embraces more classic approaches, to tackle the object goal navigation problem. Our method builds a structured scene representation based on the classic visual simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based frontier exploration to reason about promising areas to search for a goal object. Our structured scene representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph. Our method propagates semantics on the scene graphs based on language priors and scene statistics to introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can reason about the most promising frontier to explore. The proposed pipeline shows strong experimental performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in the object navigation task.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Accepted by/To be published in Robotics: Science and Systems (RSS) 2023; 11 pages, 5 figures},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/RGE5D2PZ/Chen et al. - 2023 - How To Not Train Your Dragon Training-free Embodied Object Goal Navigation with Semantic Frontiers.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/K42WJCBZ/2305.html}
}

@article{dorbalaCanEmbodiedAgent2024,
  title = {Can an {{Embodied Agent Find Your}} "{{Cat-shaped Mug}}"? {{LLM-Guided Exploration}} for {{Zero-Shot Object Navigation}}},
  shorttitle = {Can an {{Embodied Agent Find Your}} "{{Cat-shaped Mug}}"?},
  author = {Dorbala, Vishnu Sashank and Mullen, James F. and Manocha, Dinesh},
  date = {2024-05},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {9},
  number = {5},
  eprint = {2303.03480},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {4083--4090},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2023.3346800},
  url = {http://arxiv.org/abs/2303.03480},
  urldate = {2025-12-11},
  abstract = {We present LGX (Language-guided Exploration), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to a uniquely described target object in a previously unseen environment. Our approach makes use of Large Language Models (LLMs) for this task by leveraging the LLM's commonsense reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27\% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via \textbackslash textit\{real-world\} experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Robotics},
  note = {Comment: 10 pages},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8PEIEHVD/Dorbala et al. - 2024 - Can an Embodied Agent Find Your Cat-shaped Mug LLM-Guided Exploration for Zero-Shot Object Naviga.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/CR35QE3J/2303.html}
}

@online{FrontierbasedApproachAutonomous,
  title = {A Frontier-Based Approach for Autonomous Exploration},
  url = {https://ieeexplore-1ieee-1org-100033czj0337.han.technikum-wien.at/document/613851},
  urldate = {2025-12-12},
  abstract = {We introduce a new approach for exploration based on the concept of frontiers, regions on the boundary between open space and unexplored space. By moving to new frontiers, a mobile robot can extend its map into new territory until the entire environment has been explored. We describe a method for detecting frontiers in evidence grids and navigating to these frontiers. We also introduce a technique for minimizing specular reflections in evidence grids using laser-limited sonar. We have tested this approach with a real mobile robot, exploring real-world office environments cluttered with a variety of obstacles. An advantage of our approach is its ability to explore both large open spaces and narrow cluttered spaces, with walls and obstacles in arbitrary orientation.},
  langid = {american},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5FXT4Y7A/613851.html}
}

@online{gadreCoWsPastureBaselines2022,
  title = {{{CoWs}} on {{Pasture}}: {{Baselines}} and {{Benchmarks}} for {{Language-Driven Zero-Shot Object Navigation}}},
  shorttitle = {{{CoWs}} on {{Pasture}}},
  author = {Gadre, Samir Yitzhak and Wortsman, Mitchell and Ilharco, Gabriel and Schmidt, Ludwig and Song, Shuran},
  date = {2022-12-14},
  eprint = {2203.10421},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.10421},
  url = {http://arxiv.org/abs/2203.10421},
  urldate = {2025-12-11},
  abstract = {For robots to be generally useful, they must be able to find arbitrary objects described by people (i.e., be language-driven) even without expensive navigation training on in-domain data (i.e., perform zero-shot inference). We explore these capabilities in a unified setting: language-driven zero-shot object navigation (L-ZSON). Inspired by the recent success of open-vocabulary models for image classification, we investigate a straightforward framework, CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without fine-tuning. To better evaluate L-ZSON, we introduce the Pasture benchmark, which considers finding uncommon objects, objects described by spatial and appearance attributes, and hidden objects described relative to visible objects. We conduct an in-depth empirical study by directly deploying 21 CoW baselines across Habitat, RoboTHOR, and Pasture. In total, we evaluate over 90k navigation episodes and find that (1) CoW baselines often struggle to leverage language descriptions, but are proficient at finding uncommon objects. (2) A simple CoW, with CLIP-based object localization and classical exploration -- and no additional training -- matches the navigation efficiency of a state-of-the-art ZSON method trained for 500M steps on Habitat MP3D data. This same CoW provides a 15.6 percentage point improvement in success over a state-of-the-art RoboTHOR ZSON model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/68M88XPA/Gadre et al. - 2022 - CoWs on Pasture Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/HV8RG95V/2203.html}
}

@article{gargSemanticsRoboticMapping2020,
  title = {Semantics for {{Robotic Mapping}}, {{Perception}} and {{Interaction}}: {{A Survey}}},
  shorttitle = {Semantics for {{Robotic Mapping}}, {{Perception}} and {{Interaction}}},
  author = {Garg, Sourav and Sünderhauf, Niko and Dayoub, Feras and Morrison, Douglas and Cosgun, Akansel and Carneiro, Gustavo and Wu, Qi and Chin, Tat-Jun and Reid, Ian and Gould, Stephen and Corke, Peter and Milford, Michael},
  date = {2020},
  journaltitle = {Foundations and Trends® in Robotics},
  shortjournal = {FNT in Robotics},
  volume = {8},
  number = {1--2},
  eprint = {2101.00443},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--224},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000059},
  url = {http://arxiv.org/abs/2101.00443},
  urldate = {2025-12-11},
  abstract = {For robots to navigate and interact more richly with the world around them, they will likely require a deeper understanding of the world in which they operate. In robotics and related research fields, the study of understanding is often referred to as semantics, which dictates what does the world "mean" to a robot, and is strongly tied to the question of how to represent that meaning. With humans and robots increasingly operating in the same world, the prospects of human-robot interaction also bring semantics and ontology of natural language into the picture. Driven by need, as well as by enablers like increasing availability of training data and computational resources, semantics is a rapidly growing research area in robotics. The field has received significant attention in the research literature to date, but most reviews and surveys have focused on particular aspects of the topic: the technical research issues regarding its use in specific robotic topics like mapping or segmentation, or its relevance to one particular application domain like autonomous driving. A new treatment is therefore required, and is also timely because so much relevant research has occurred since many of the key surveys were published. This survey therefore provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity, in which semantics are extracted, used, or both. Within these broad categories we survey dozens of major topics including fundamentals from the computer vision field and key robotics research areas utilizing semantics, including mapping, navigation and interaction with the world. The survey also covers key practical considerations, including enablers like increased data availability and improved computational hardware, and major application areas where...},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 81 pages, 1 figure, published in Foundations and Trends in Robotics, 2020},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/C2YANZ7K/Garg et al. - 2020 - Semantics for Robotic Mapping, Perception and Interaction A Survey.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/NFYR4VZ2/2101.html}
}

@online{ghasemiComprehensiveSurveyReinforcement2025,
  title = {A {{Comprehensive Survey}} of {{Reinforcement Learning}}: {{From Algorithms}} to {{Practical Challenges}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Reinforcement Learning}}},
  author = {Ghasemi, Majid and Moosavi, Amir Hossein and Ebrahimi, Dariush},
  date = {2025-02-01},
  eprint = {2411.18892},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.18892},
  url = {http://arxiv.org/abs/2411.18892},
  urldate = {2026-01-05},
  abstract = {Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: 79 pages},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/9RBBBNKJ/Ghasemi et al. - 2025 - A Comprehensive Survey of Reinforcement Learning From Algorithms to Practical Challenges.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/F6GZQZYS/2411.html}
}

@online{guConceptGraphsOpenVocabulary3D2023,
  title = {{{ConceptGraphs}}: {{Open-Vocabulary 3D Scene Graphs}} for {{Perception}} and {{Planning}}},
  shorttitle = {{{ConceptGraphs}}},
  author = {Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and family=Melo, given=Celso Miguel, prefix=de, useprefix=false and Tenenbaum, Joshua B. and Torralba, Antonio and Shkurti, Florian and Paull, Liam},
  date = {2023-09-28},
  eprint = {2309.16650},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16650},
  url = {http://arxiv.org/abs/2309.16650},
  urldate = {2025-12-11},
  abstract = {For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc )},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/2ZBH5Z2M/Gu et al. - 2023 - ConceptGraphs Open-Vocabulary 3D Scene Graphs for Perception and Planning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/XFV25V23/2309.html}
}

@online{heMaskRCNN2018,
  title = {Mask {{R-CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  date = {2018-01-24},
  eprint = {1703.06870},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1703.06870},
  url = {http://arxiv.org/abs/1703.06870},
  urldate = {2026-01-06},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: open source; appendix on more results},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/6SXVS3Q2/He et al. - 2018 - Mask R-CNN.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/Q6GCME8B/1703.html}
}

@online{huangVisualLanguageMaps2023,
  title = {Visual {{Language Maps}} for {{Robot Navigation}}},
  author = {Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  date = {2023-03-08},
  eprint = {2210.05714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.05714},
  url = {http://arxiv.org/abs/2210.05714},
  urldate = {2025-12-11},
  abstract = {Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., "in between the sofa and TV" or "three meters to the right of the chair") directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Accepted at the 2023 IEEE International Conference on Robotics and Automation (ICRA). Project page: https://vlmaps.github.io},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/G4DABBR7/Huang et al. - 2023 - Visual Language Maps for Robot Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DVA7KMXT/2210.html}
}

@online{jatavallabhulaConceptFusionOpensetMultimodal2023,
  title = {{{ConceptFusion}}: {{Open-set Multimodal 3D Mapping}}},
  shorttitle = {{{ConceptFusion}}},
  author = {Jatavallabhula, Krishna Murthy and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Maalouf, Alaa and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and Tewari, Ayush and Tenenbaum, Joshua B. and family=Melo, given=Celso Miguel, prefix=de, useprefix=false and Krishna, Madhava and Paull, Liam and Shkurti, Florian and Torralba, Antonio},
  date = {2023-10-23},
  eprint = {2302.07241},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.07241},
  url = {http://arxiv.org/abs/2302.07241},
  urldate = {2026-01-03},
  abstract = {Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts. We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today's foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40\% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping. For more information, visit our project page https://concept-fusion.github.io or watch our 5-minute explainer video https://www.youtube.com/watch?v=rkXgws8fiDs},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: RSS 2023. Project page: https://concept-fusion.github.io Explainer video: https://www.youtube.com/watch?v=rkXgws8fiDs Code: https://github.com/concept-fusion/concept-fusion},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5FVLK83F/Jatavallabhula et al. - 2023 - ConceptFusion Open-set Multimodal 3D Mapping.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/9222GTMG/2302.html}
}

@online{jiangCLIPDINOVisual2024,
  title = {From {{CLIP}} to {{DINO}}: {{Visual Encoders Shout}} in {{Multi-modal Large Language Models}}},
  shorttitle = {From {{CLIP}} to {{DINO}}},
  author = {Jiang, Dongsheng and Liu, Yuchen and Liu, Songlin and Zhao, Jin'e and Zhang, Hao and Gao, Zhen and Zhang, Xiaopeng and Li, Jin and Xiong, Hongkai},
  date = {2024-03-08},
  eprint = {2310.08825},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.08825},
  url = {http://arxiv.org/abs/2310.08825},
  urldate = {2025-12-14},
  abstract = {Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named COMM, that integrates CLIP and DINO with Multi-level features Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of COMM compared to existing methods, showcasing its enhanced visual capabilities within MLLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5GPWXAI2/Jiang et al. - 2024 - From CLIP to DINO Visual Encoders Shout in Multi-modal Large Language Models.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/5BJ9BFIZ/2310.html}
}

@article{jiangDualMapOnlineOpenVocabulary2025,
  title = {{{DualMap}}: {{Online Open-Vocabulary Semantic Mapping}} for {{Natural Language Navigation}} in {{Dynamic Changing Scenes}}},
  shorttitle = {{{DualMap}}},
  author = {Jiang, Jiajun and Zhu, Yiming and Wu, Zirui and Song, Jie},
  date = {2025-12},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {10},
  number = {12},
  eprint = {2506.01950},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {12612--12619},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2025.3621942},
  url = {http://arxiv.org/abs/2506.01950},
  urldate = {2026-01-07},
  abstract = {We introduce DualMap, an online open-vocabulary mapping system that enables robots to understand and navigate dynamically changing environments through natural language queries. Designed for efficient semantic mapping and adaptability to changing environments, DualMap meets the essential requirements for real-world robot navigation applications. Our proposed hybrid segmentation frontend and object-level status check eliminate the costly 3D object merging required by prior methods, enabling efficient online scene mapping. The dual-map representation combines a global abstract map for high-level candidate selection with a local concrete map for precise goal-reaching, effectively managing and updating dynamic changes in the environment. Through extensive experiments in both simulation and real-world scenarios, we demonstrate state-of-the-art performance in 3D open-vocabulary segmentation, efficient scene mapping, and online language-guided navigation. Project page: https://eku127.github.io/DualMap/},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: 14 pages, 14 figures. Published in IEEE Robotics and Automation Letters (RA-L), 2025. Code: https://github.com/Eku127/DualMap Project page: https://eku127.github.io/DualMap/},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5LUYVIA7/Jiang et al. - 2025 - DualMap Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/8DQ4K2RH/2506.html}
}

@article{kansoSemanticSLAMComprehensive2025,
  title = {Semantic {{SLAM}}: {{A}} Comprehensive Survey of Methods and Applications},
  shorttitle = {Semantic {{SLAM}}},
  author = {Kanso, Houssein and Singh, Abhilasha and Zarif, Etaf El and Almohammed, Nooruldeen and Mounsef, Jinane and Maalouf, Noel and Arain, Bilal},
  date = {2025-12-01},
  journaltitle = {Intelligent Systems with Applications},
  shortjournal = {Intelligent Systems with Applications},
  volume = {28},
  pages = {200591},
  issn = {2667-3053},
  doi = {10.1016/j.iswa.2025.200591},
  url = {https://www.sciencedirect.com/science/article/pii/S2667305325001176},
  urldate = {2025-12-11},
  abstract = {This paper surveys the different approaches in semantic Simultaneous Localization and Mapping (SLAM), exploring how the incorporation of semantic information has enhanced performance in both indoor and outdoor settings, while highlighting key advancements in the field. It also identifies existing gaps and proposes potential directions for future improvements to address these issues. We provide a detailed review of the fundamentals of semantic SLAM, illustrating how incorporating semantic data enhances scene understanding and mapping accuracy. The paper presents semantic SLAM methods and core techniques that contribute to improved robustness and precision in mapping. A comprehensive overview of commonly used datasets for evaluating semantic SLAM systems is provided, along with a discussion of performance metrics used to assess their efficiency and accuracy. To demonstrate the reliability of semantic SLAM methodologies, we reproduce selected results from existing studies offering insights into the reproducibility of these approaches. The paper also addresses key challenges such as real-time processing, dynamic scene adaptation, and scalability while highlighting future research directions. Unlike prior surveys, this paper uniquely combines (i) a systematic taxonomy of semantic SLAM approaches across different sensing modalities and environments, (ii) a comparative review of datasets and evaluation metrics, and (iii) a reproducibility study of selected methods. To our knowledge, this is the first survey that integrates methods, datasets, evaluation practices, and application insights into a single comprehensive review, thereby offering a unified reference for researchers and practitioners. In conclusion, this review underscores the vital role of semantic SLAM in driving advancements in autonomous systems and intelligent navigation by analyzing recent developments, validating findings, and highlighting future research directions.},
  keywords = {Autonomous systems,Dynamic environments,Object-level SLAM,Semantic mapping,Semantic SLAM,Visual SLAM},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JYLEZSMP/Kanso et al. - 2025 - Semantic SLAM A comprehensive survey of methods and applications.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/2C2DQTME/S2667305325001176.html}
}

@online{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
  date = {2023-04-05},
  eprint = {2304.02643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.02643},
  url = {http://arxiv.org/abs/2304.02643},
  urldate = {2025-12-15},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Project web-page: https://segment-anything.com},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/TWTA6NG6/Kirillov et al. - 2023 - Segment Anything.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/ZTAF3529/2304.html}
}

@online{liBLIP2BootstrappingLanguageImage2023,
  title = {{{BLIP-2}}: {{Bootstrapping Language-Image Pre-training}} with {{Frozen Image Encoders}} and {{Large Language Models}}},
  shorttitle = {{{BLIP-2}}},
  author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  date = {2023-06-15},
  eprint = {2301.12597},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.12597},
  url = {http://arxiv.org/abs/2301.12597},
  urldate = {2025-12-15},
  abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/CK7EWWUD/Li et al. - 2023 - BLIP-2 Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Mode.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/EA8SKWTP/2301.html}
}

@online{liGroundedLanguageImagePretraining2022,
  title = {Grounded {{Language-Image Pre-training}}},
  author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
  date = {2022-06-17},
  eprint = {2112.03857},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.03857},
  url = {http://arxiv.org/abs/2112.03857},
  urldate = {2025-12-15},
  abstract = {This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  note = {Comment: CVPR 2022; updated visualizations; fixed hyper-parameters in Appendix C.1},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/EGSYAAGC/Li et al. - 2022 - Grounded Language-Image Pre-training.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/8V9KT4RJ/2112.html}
}

@online{liGroundedLanguageImagePretraining2022a,
  title = {Grounded {{Language-Image Pre-training}}},
  author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
  date = {2022-06-17},
  eprint = {2112.03857},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.03857},
  url = {http://arxiv.org/abs/2112.03857},
  urldate = {2026-01-05},
  abstract = {This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  note = {Comment: CVPR 2022; updated visualizations; fixed hyper-parameters in Appendix C.1},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/PLKLSXTB/Li et al. - 2022 - Grounded Language-Image Pre-training.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/QF4RG72G/2112.html}
}

@online{liLanguagedrivenSemanticSegmentation2022,
  title = {Language-Driven {{Semantic Segmentation}}},
  author = {Li, Boyi and Weinberger, Kilian Q. and Belongie, Serge and Koltun, Vladlen and Ranftl, René},
  date = {2022-04-03},
  eprint = {2201.03546},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.03546},
  url = {http://arxiv.org/abs/2201.03546},
  urldate = {2026-01-07},
  abstract = {We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., "grass" or "building") together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class. The text embeddings provide a flexible label representation in which semantically similar labels map to similar regions in the embedding space (e.g., "cat" and "furry"). This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample. We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided. Code and demo are available at https://github.com/isl-org/lang-seg.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: ICLR 2022},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/IGJ4LZK9/Li et al. - 2022 - Language-driven Semantic Segmentation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/EPE7TZNN/2201.html}
}

@online{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2015-02-21},
  eprint = {1405.0312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1405.0312},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2025-12-11},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/I5ATQM2D/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/RZ8JVYSE/1405.html}
}

@book{liu3DPointCloud2021,
  title = {{{3D Point Cloud Analysis}}: {{Traditional}}, {{Deep Learning}}, and {{Explainable Machine Learning Methods}}},
  shorttitle = {{{3D Point Cloud Analysis}}},
  author = {Liu, Shan and Zhang, Min and Kadam, Pranav and Kuo, C.-C. Jay},
  date = {2021},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-89180-0},
  url = {https://link.springer.com/10.1007/978-3-030-89180-0},
  urldate = {2026-01-03},
  isbn = {978-3-030-89179-4 978-3-030-89180-0},
  langid = {english},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/G3D7SSDB/Liu et al. - 2021 - 3D Point Cloud Analysis Traditional, Deep Learning, and Explainable Machine Learning Methods.pdf}
}

@online{liuGroundingDINOMarrying2024,
  title = {Grounding {{DINO}}: {{Marrying DINO}} with {{Grounded Pre-Training}} for {{Open-Set Object Detection}}},
  shorttitle = {Grounding {{DINO}}},
  author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
  date = {2024-07-19},
  eprint = {2303.05499},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.05499},
  url = {http://arxiv.org/abs/2303.05499},
  urldate = {2025-12-15},
  abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at \textbackslash url\{https://github.com/IDEA-Research/GroundingDINO\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Code will be available at https://github.com/IDEA-Research/GroundingDINO},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/TB8JHP9D/Liu et al. - 2024 - Grounding DINO Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/JM9R998B/2303.html}
}

@online{liuGroundingDINOMarrying2024a,
  title = {Grounding {{DINO}}: {{Marrying DINO}} with {{Grounded Pre-Training}} for {{Open-Set Object Detection}}},
  shorttitle = {Grounding {{DINO}}},
  author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
  date = {2024-07-19},
  eprint = {2303.05499},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.05499},
  url = {http://arxiv.org/abs/2303.05499},
  urldate = {2026-01-03},
  abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at \textbackslash url\{https://github.com/IDEA-Research/GroundingDINO\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Code will be available at https://github.com/IDEA-Research/GroundingDINO},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/DW4ZL8U7/Liu et al. - 2024 - Grounding DINO Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WDYGMMKN/2303.html}
}

@online{liuGroundingDINOMarrying2024b,
  title = {Grounding {{DINO}}: {{Marrying DINO}} with {{Grounded Pre-Training}} for {{Open-Set Object Detection}}},
  shorttitle = {Grounding {{DINO}}},
  author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
  date = {2024-07-19},
  eprint = {2303.05499},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.05499},
  url = {http://arxiv.org/abs/2303.05499},
  urldate = {2026-01-05},
  abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at \textbackslash url\{https://github.com/IDEA-Research/GroundingDINO\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Code will be available at https://github.com/IDEA-Research/GroundingDINO},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/24KD9Y9N/Liu et al. - 2024 - Grounding DINO Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/FWPTPB62/2303.html}
}

@article{lluviaActiveMappingRobot2021,
  title = {Active {{Mapping}} and {{Robot Exploration}}: {{A Survey}}},
  shorttitle = {Active {{Mapping}} and {{Robot Exploration}}},
  author = {Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander and Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander},
  date = {2021-04-02},
  journaltitle = {Sensors},
  volume = {21},
  number = {7},
  publisher = {publisher},
  issn = {1424-8220},
  doi = {10.3390/s21072445},
  url = {https://www.mdpi.com/1424-8220/21/7/2445},
  urldate = {2025-12-12},
  abstract = {Simultaneous localization and mapping responds to the problem of building a map of the environment without any prior information and based on the data...},
  langid = {english},
  keywords = {exploration,frontiers,mapping,mobile robots,next best view,path planning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WJMBQ9GT/Lluvia et al. - 2021 - Active Mapping and Robot Exploration A Survey.pdf}
}

@online{majumdarZSONZeroShotObjectGoal2023,
  title = {{{ZSON}}: {{Zero-Shot Object-Goal Navigation}} Using {{Multimodal Goal Embeddings}}},
  shorttitle = {{{ZSON}}},
  author = {Majumdar, Arjun and Aggarwal, Gunjan and Devnani, Bhavika and Hoffman, Judy and Batra, Dhruv},
  date = {2023-10-13},
  eprint = {2206.12403},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.12403},
  url = {http://arxiv.org/abs/2206.12403},
  urldate = {2025-12-11},
  abstract = {We present a scalable approach for learning open-world object-goal navigation (ObjectNav) -- the task of asking a virtual robot (agent) to find any instance of an object in an unexplored environment (e.g., "find a sink"). Our approach is entirely zero-shot -- i.e., it does not require ObjectNav rewards or demonstrations of any kind. Instead, we train on the image-goal navigation (ImageNav) task, in which agents find the location where a picture (i.e., goal image) was captured. Specifically, we encode goal images into a multimodal, semantic embedding space to enable training semantic-goal navigation (SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D). After training, SemanticNav agents can be instructed to find objects described in free-form natural language (e.g., "sink", "bathroom sink", etc.) by projecting language goals into the same multimodal, semantic embedding space. As a result, our approach enables open-world ObjectNav. We extensively evaluate our agents on three ObjectNav datasets (Gibson, HM3D, and MP3D) and observe absolute improvements in success of 4.2\% - 20.0\% over existing zero-shot methods. For reference, these gains are similar or better than the 5\% improvement in success between the Habitat 2020 and 2021 ObjectNav challenge winners. In an open-world setting, we discover that our agents can generalize to compound instructions with a room explicitly mentioned (e.g., "Find a kitchen sink") and when the target room can be inferred (e.g., "Find a sink and a stove").},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: code: https://github.com/gunagg/zson},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/Q9ILVZFE/Majumdar et al. - 2023 - ZSON Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/QJ3M4582/2206.html}
}

@article{miaoFrontierReviewSemantic2025,
  title = {A {{Frontier Review}} of {{Semantic SLAM Technologies Applied}} to the {{Open World}}},
  author = {Miao, Le and Liu, Wen and Deng, Zhongliang and Miao, Le and Liu, Wen and Deng, Zhongliang},
  date = {2025-08-12},
  journaltitle = {Sensors},
  volume = {25},
  number = {16},
  publisher = {publisher},
  issn = {1424-8220},
  doi = {10.3390/s25164994},
  url = {https://www.mdpi.com/1424-8220/25/16/4994},
  urldate = {2025-12-11},
  abstract = {With the growing demand for autonomous robotic operations in complex and unstructured environments, traditional semantic SLAM systems—which rely on cl...},
  langid = {english},
  keywords = {multimodal sensor fusion,open-world perception,robotic perception,semantic SLAM,zero-shot learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/NBHH6UZS/Miao et al. - 2025 - A Frontier Review of Semantic SLAM Technologies Applied to the Open World.pdf}
}

@online{PIGEONVLMDrivenObject,
  title = {{{PIGEON}}: {{VLM-Driven Object Navigation}} via {{Points}} of {{Interest SelectionPreprint}}. {{Work}} in {{Progress}}.},
  url = {https://arxiv.org/html/2511.13207v1},
  urldate = {2025-12-11},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/P4ZNIYZ5/2511.html}
}

@online{qiuLearningGeneralizableFeature2024,
  title = {Learning {{Generalizable Feature Fields}} for {{Mobile Manipulation}}},
  author = {Qiu, Ri-Zhao and Hu, Yafei and Song, Yuchen and Yang, Ge and Fu, Yang and Ye, Jianglong and Mu, Jiteng and Yang, Ruihan and Atanasov, Nikolay and Scherer, Sebastian and Wang, Xiaolong},
  date = {2024-11-26},
  eprint = {2403.07563},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.07563},
  url = {http://arxiv.org/abs/2403.07563},
  urldate = {2025-12-11},
  abstract = {An open problem in mobile manipulation is how to represent objects and scenes in a unified manner so that robots can use both for navigation and manipulation. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherent at an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We quantitatively evaluate GeFF's ability for open-vocabulary object-/part-level manipulation and show that GeFF outperforms point-based baselines in runtime and storage-accuracy trade-offs, with qualitative examples of semantics-aware navigation and articulated object manipulation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Preprint. Project website is at: https://geff-b1.github.io/},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ZQ7PJHJN/Qiu et al. - 2024 - Learning Generalizable Feature Fields for Mobile Manipulation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/BFJCQT64/2403.html}
}

@article{quinApproachesEfficientlyDetecting2021,
  title = {Approaches for {{Efficiently Detecting Frontier Cells}} in {{Robotics Exploration}}},
  author = {Quin, Phillip and Nguyen, Dac and Vu, Thanh and Alempijevic, Alen and Paul, Gavin},
  date = {2021-02-25},
  journaltitle = {Frontiers in Robotics and AI},
  shortjournal = {Frontiers in Robotics and AI},
  volume = {8},
  pages = {616470},
  doi = {10.3389/frobt.2021.616470},
  abstract = {Many robot exploration algorithms that are used to explore office, home, or outdoor environments, rely on the concept of frontier cells. Frontier cells define the border between known and unknown space. Frontier-based exploration is the process of repeatedly detecting frontiers and moving towards them, until there are no more frontiers and therefore no more unknown regions. The faster frontier cells can be detected, the more efficient exploration becomes. This paper proposes several algorithms for detecting frontiers. The first is called Naïve Active Area (NaïveAA) frontier detection and achieves frontier detection in constant time by only evaluating the cells in the active area defined by scans taken. The second algorithm is called Expanding-Wavefront Frontier Detection (EWFD) and uses frontiers from the previous timestep as a starting point for searching for frontiers in newly discovered space. The third approach is called Frontier-Tracing Frontier Detection (FTFD) and also uses the frontiers from the previous timestep as well as the endpoints of the scan, to determine the frontiers at the current timestep. Algorithms are compared to state-of-the-art algorithms such as Naïve, WFD, and WFD-INC. NaïveAA is shown to operate in constant time and therefore is suitable as a basic benchmark for frontier detection algorithms. EWFD and FTFD are found to be significantly faster than other algorithms.},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/A6KDFCUD/Quin et al. - 2021 - Approaches for Efficiently Detecting Frontier Cells in Robotics Exploration.pdf}
}

@online{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.00020},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2025-12-13},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WBDSAGZM/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/QHYY9RBJ/2103.html}
}

@online{radfordLearningTransferableVisual2021a,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.00020},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2026-01-02},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/XUIE3LLL/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/2A2BP49U/2103.html}
}

@online{ramakrishnanPONIPotentialFunctions2022,
  title = {{{PONI}}: {{Potential Functions}} for {{ObjectGoal Navigation}} with {{Interaction-free Learning}}},
  shorttitle = {{{PONI}}},
  author = {Ramakrishnan, Santhosh Kumar and Chaplot, Devendra Singh and Al-Halah, Ziad and Malik, Jitendra and Grauman, Kristen},
  date = {2022-06-17},
  eprint = {2201.10029},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.10029},
  url = {http://arxiv.org/abs/2201.10029},
  urldate = {2025-12-11},
  abstract = {State-of-the-art approaches to ObjectGoal navigation rely on reinforcement learning and typically require significant computational resources and time for learning. We propose Potential functions for ObjectGoal Navigation with Interaction-free learning (PONI), a modular approach that disentangles the skills of `where to look?' for an object and `how to navigate to (x, y)?'. Our key insight is that `where to look?' can be treated purely as a perception problem, and learned without environment interactions. To address this, we propose a network that predicts two complementary potential functions conditioned on a semantic map and uses them to decide where to look for an unseen object. We train the potential function network using supervised learning on a passive dataset of top-down semantic maps, and integrate it into a modular framework to perform ObjectGoal navigation. Experiments on Gibson and Matterport3D demonstrate that our method achieves the state-of-the-art for ObjectGoal navigation while incurring up to 1,600x less computational cost for training. Code and pre-trained models are available: https://vision.cs.utexas.edu/projects/poni/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 8 pages + supplementary. Accepted in CVPR 2022},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/SCUSH2CB/Ramakrishnan et al. - 2022 - PONI Potential Functions for ObjectGoal Navigation with Interaction-free Learning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/EY8AD5U3/2201.html}
}

@online{ramrakhyaPIRLNavPretrainingImitation2023,
  title = {{{PIRLNav}}: {{Pretraining}} with {{Imitation}} and {{RL Finetuning}} for {{ObjectNav}}},
  shorttitle = {{{PIRLNav}}},
  author = {Ramrakhya, Ram and Batra, Dhruv and Wijmans, Erik and Das, Abhishek},
  date = {2023-03-26},
  eprint = {2301.07302},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.07302},
  url = {http://arxiv.org/abs/2301.07302},
  urldate = {2025-12-11},
  abstract = {We study ObjectGoal Navigation -- where a virtual robot situated in a new environment is asked to navigate to an object. Prior work has shown that imitation learning (IL) using behavior cloning (BC) on a dataset of human demonstrations achieves promising results. However, this has limitations -- 1) BC policies generalize poorly to new states, since the training mimics actions not their consequences, and 2) collecting demonstrations is expensive. On the other hand, reinforcement learning (RL) is trivially scalable, but requires careful reward engineering to achieve desirable behavior. We present PIRLNav, a two-stage learning scheme for BC pretraining on human demonstrations followed by RL-finetuning. This leads to a policy that achieves a success rate of \$65.0\textbackslash\%\$ on ObjectNav (\$+5.0\textbackslash\%\$ absolute over previous state-of-the-art). Using this BC\$\textbackslash rightarrow\$RL training recipe, we present a rigorous empirical analysis of design choices. First, we investigate whether human demonstrations can be replaced with `free' (automatically generated) sources of demonstrations, e.g. shortest paths (SP) or task-agnostic frontier exploration (FE) trajectories. We find that BC\$\textbackslash rightarrow\$RL on human demonstrations outperforms BC\$\textbackslash rightarrow\$RL on SP and FE trajectories, even when controlled for same BC-pretraining success on train, and even on a subset of val episodes where BC-pretraining success favors the SP or FE policies. Next, we study how RL-finetuning performance scales with the size of the BC pretraining dataset. We find that as we increase the size of BC-pretraining dataset and get to high BC accuracies, improvements from RL-finetuning are smaller, and that \$90\textbackslash\%\$ of the performance of our best BC\$\textbackslash rightarrow\$RL policy can be achieved with less than half the number of BC demonstrations. Finally, we analyze failure modes of our ObjectNav policies, and present guidelines for further improving them.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 8 pages + supplement},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/44TK5T7E/Ramrakhya et al. - 2023 - PIRLNav Pretraining with Imitation and RL Finetuning for ObjectNav.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/KVXYYXVM/2301.html}
}

@online{RoboticExplorationUsing,
  title = {Robotic {{Exploration Using Generalized Behavioral Entropy}}},
  url = {https://ieeexplore-1ieee-1org-100033czj02b7.han.technikum-wien.at/document/10608408},
  urldate = {2025-12-11},
  abstract = {This letter presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception. To do this, we introduce a measure of uncertainty that we term “Behavioral entropy”, which builds on Prelec's probability weighting from Behavioral Economics. We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon's and Renyi's. In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here. Then we use Behavioral entropy to define a new type of utility function that can guide a frontier-based environment exploration process. The approach's benefits are illustrated and compared in a Proof-of-Concept and ROS-Unity simulation environment with a Clearpath Warthog robot. We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies.},
  langid = {american},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/MTWWXC4V/10608408.html}
}

@inproceedings{ruanTaxonomySemanticInformation2022,
  title = {A {{Taxonomy}} of {{Semantic Information}} in {{Robot-Assisted Disaster Response}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Safety}}, {{Security}}, and {{Rescue Robotics}} ({{SSRR}})},
  author = {Ruan, Tianshu and Wang, Hao and Stolkin, Rustam and Chiou, Manolis},
  date = {2022-11-08},
  eprint = {2210.00125},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {285--292},
  doi = {10.1109/SSRR56537.2022.10018727},
  url = {http://arxiv.org/abs/2210.00125},
  urldate = {2025-12-11},
  abstract = {This paper proposes a taxonomy of semantic information in robot-assisted disaster response. Robots are increasingly being used in hazardous environment industries and emergency response teams to perform various tasks. Operational decision-making in such applications requires a complex semantic understanding of environments that are remote from the human operator. Low-level sensory data from the robot is transformed into perception and informative cognition. Currently, such cognition is predominantly performed by a human expert, who monitors remote sensor data such as robot video feeds. This engenders a need for AI-generated semantic understanding capabilities on the robot itself. Current work on semantics and AI lies towards the relatively academic end of the research spectrum, hence relatively removed from the practical realities of first responder teams. We aim for this paper to be a step towards bridging this divide. We first review common robot tasks in disaster response and the types of information such robots must collect. We then organize the types of semantic features and understanding that may be useful in disaster operations into a taxonomy of semantic information. We also briefly review the current state-of-the-art semantic understanding techniques. We highlight potential synergies, but we also identify gaps that need to be bridged to apply these ideas. We aim to stimulate the research that is needed to adapt, robustify, and implement state-of-the-art AI semantics methods in the challenging conditions of disasters and first responder scenarios.},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/BF73MJ3E/Ruan et al. - 2022 - A Taxonomy of Semantic Information in Robot-Assisted Disaster Response.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/JBWAHDQ4/2210.html}
}

@online{savvaHabitatPlatformEmbodied2019,
  title = {Habitat: {{A Platform}} for {{Embodied AI Research}}},
  shorttitle = {Habitat},
  author = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and Parikh, Devi and Batra, Dhruv},
  date = {2019-11-25},
  eprint = {1904.01201},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.01201},
  url = {http://arxiv.org/abs/1904.01201},
  urldate = {2026-01-05},
  abstract = {We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments \{train, test\} x \{Matterport3D, Gibson\} for multiple sensors \{blind, RGB, RGBD, D\} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: ICCV 2019},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/6QJVE7UY/Savva et al. - 2019 - Habitat A Platform for Embodied AI Research.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/T7RRTKK3/1904.html}
}

@online{schwaigerUGVCBRNUnmannedGround2024,
  title = {{{UGV-CBRN}}: {{An Unmanned Ground Vehicle}} for {{Chemical}}, {{Biological}}, {{Radiological}}, and {{Nuclear Disaster Response}}},
  shorttitle = {{{UGV-CBRN}}},
  author = {Schwaiger, Simon and Muster, Lucas and Novotny, Georg and Schebek, Michael and Wöber, Wilfried and Thalhammer, Stefan and Böhm, Christoph},
  date = {2024-09-20},
  eprint = {2406.14385},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.14385},
  url = {http://arxiv.org/abs/2406.14385},
  urldate = {2026-01-02},
  abstract = {Robotic search and rescue (SAR) supports response teams by accelerating disaster assessment and by keeping operators away from hazardous environments. In the event of a chemical, biological, radiological, and nuclear (CBRN) disaster, robots are deployed to identify and locate radiation sources. Human responders then assess the situation and neutralize the danger. The presented system takes a step toward enhanced integration of robots into SAR teams. Integrating autonomous radiation mapping with semi-autonomous substance sampling and online analysis of the CBRN threat lets the human operator localize and assess the threat from a safe distance. Two LiDARs, an IMU, and a Geiger counter are used for mapping the surrounding area and localizing potential radiation sources. A mobile manipulator with six Degrees of Freedom manipulates valves and samples substances that are analyzed by an onboard Raman spectrometer. The human operator monitors the mission's progression from a remote location defining target locations and directing the semi-autonomous manipulation processes. Diverse recovery behaviours aid robot deployment, system state monitoring, as well as recovery of hard- and software. Field tests showcase the capabilities of the presented system during trials at the CBRN disaster response challenge European Robotics Hackathon (EnRicH). We provide recorded sensor data and implemented software through a GitHub repository: https://github.com/TW-Robotics/search-and-rescue-robot-2024.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/SYKUFV7W/Schwaiger et al. - 2024 - UGV-CBRN An Unmanned Ground Vehicle for Chemical, Biological, Radiological, and Nuclear Disaster Re.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/IMLX46I7/2406.html}
}

@online{straubReplicaDatasetDigital2019,
  title = {The {{Replica Dataset}}: {{A Digital Replica}} of {{Indoor Spaces}}},
  shorttitle = {The {{Replica Dataset}}},
  author = {Straub, Julian and Whelan, Thomas and Ma, Lingni and Chen, Yufan and Wijmans, Erik and Green, Simon and Engel, Jakob J. and Mur-Artal, Raul and Ren, Carl and Verma, Shobhit and Clarkson, Anton and Yan, Mingfei and Budge, Brian and Yan, Yajie and Pan, Xiaqing and Yon, June and Zou, Yuyang and Leon, Kimberly and Carter, Nigel and Briales, Jesus and Gillingham, Tyler and Mueggler, Elias and Pesqueira, Luis and Savva, Manolis and Batra, Dhruv and Strasdat, Hauke M. and Nardi, Renzo De and Goesele, Michael and Lovegrove, Steven and Newcombe, Richard},
  date = {2019-06-13},
  eprint = {1906.05797},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.05797},
  url = {http://arxiv.org/abs/1906.05797},
  urldate = {2026-01-05},
  abstract = {We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/KJFXQ9KY/Straub et al. - 2019 - The Replica Dataset A Digital Replica of Indoor Spaces.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/62D6ZF7Q/1906.html}
}

@online{tellaroliFrontierBasedExplorationMultiRobot2024,
  title = {Frontier-{{Based Exploration}} for {{Multi-Robot Rendezvous}} in {{Communication-Restricted Unknown Environments}}},
  author = {Tellaroli, Mauro and Luperto, Matteo and Antonazzi, Michele and Basilico, Nicola},
  date = {2024-07-19},
  eprint = {2403.11617},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.11617},
  url = {http://arxiv.org/abs/2403.11617},
  urldate = {2026-01-03},
  abstract = {Multi-robot rendezvous and exploration are fundamental challenges in the domain of mobile robotic systems. This paper addresses multi-robot rendezvous within an initially unknown environment where communication is only possible after the rendezvous. Traditionally, exploration has been focused on rapidly mapping the environment, often leading to suboptimal rendezvous performance in later stages. We adapt a standard frontier-based exploration technique to integrate exploration and rendezvous into a unified strategy, with a mechanism that allows robots to re-visit previously explored regions thus enhancing rendezvous opportunities. We validate our approach in 3D realistic simulations using ROS, showcasing its effectiveness in achieving faster rendezvous times compared to exploration strategies.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/CXSQP73T/Tellaroli et al. - 2024 - Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environmen.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/XB62J83R/2403.html}
}

@online{thomasPolicyGradientMethods2017,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}} and {{Action-Dependent Baselines}}},
  author = {Thomas, Philip S. and Brunskill, Emma},
  date = {2017-06-20},
  eprint = {1706.06643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.06643},
  url = {http://arxiv.org/abs/1706.06643},
  urldate = {2026-01-06},
  abstract = {We show how an action-dependent baseline can be used by the policy gradient theorem using function approximation, originally presented with action-independent baselines by (Sutton et al. 2000).},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/B7SCR2PA/Thomas and Brunskill - 2017 - Policy Gradient Methods for Reinforcement Learning with Function Approximation and Action-Dependent.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/3MTWXN3R/1706.html}
}

@book{thrunProbabilisticRobotics2006,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  date = {2006},
  series = {Intelligent Robotics and Autonomous Agents},
  publisher = {MIT Press},
  location = {Cambridge, Massachusetts London},
  abstract = {An introduction to the techniques and algorithms of the newest field in robotics. Probabilistic robotics is a new and growing area in robotics, concerned with perception and control in the face of uncertainty. Building on the field of mathematical statistics, probabilistic robotics endows robots with a new level of robustness in real-world situations. This book introduces the reader to a wealth of techniques and algorithms in the field. All algorithms are based on a single overarching mathematical foundation. Each chapter provides example implementations in pseudo code, detailed mathematical derivations, discussions from a practitioner's perspective, and extensive lists of exercises and class projects. The book's Web site, www.probabilistic-robotics.org, has additional material. The book is relevant for anyone involved in robotic software development and scientific research. It will also be of interest to applied statisticians and engineers dealing with real-world sensor data},
  isbn = {978-0-262-20162-9 978-0-262-30380-4},
  langid = {english},
  pagetotal = {1},
  note = {Description based on publisher supplied metadata and other sources}
}

@online{topiwalaFrontierBasedExploration2018,
  title = {Frontier {{Based Exploration}} for {{Autonomous Robot}}},
  author = {Topiwala, Anirudh and Inani, Pranav and Kathpal, Abhishek},
  date = {2018-06-10},
  eprint = {1806.03581},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.03581},
  url = {http://arxiv.org/abs/1806.03581},
  urldate = {2025-12-11},
  abstract = {Exploration is process of selecting target points that yield the biggest contribution to a specific gain function at an initially unknown environment. Frontier-based exploration is the most common approach to exploration, wherein frontiers are regions on the boundary between open space and unexplored space. By moving to a new frontier, we can keep building the map of the environment, until there are no new frontiers left to detect. In this paper, an autonomous frontier-based exploration strategy, namely Wavefront Frontier Detector (WFD) is described and implemented on Gazebo Simulation Environment as well as on hardware platform, i.e. Kobuki TurtleBot using Robot Operating System (ROS). The advantage of this algorithm is that the robot can explore large open spaces as well as small cluttered spaces. Further, the map generated from this technique is compared and validated with the map generated using turtlebot\_teleop ROS Package.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8AUGCNII/Topiwala et al. - 2018 - Frontier Based Exploration for Autonomous Robot.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WIHJKPVN/1806.html}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-12-11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 15 pages, 5 figures},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8LCESDHM/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WHZ65CNY/1706.html}
}

@online{wangYOLOERealTimeSeeing2025,
  title = {{{YOLOE}}: {{Real-Time Seeing Anything}}},
  shorttitle = {{{YOLOE}}},
  author = {Wang, Ao and Liu, Lihao and Chen, Hui and Lin, Zijia and Han, Jungong and Ding, Guiguang},
  date = {2025-10-17},
  eprint = {2503.07465},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.07465},
  url = {http://arxiv.org/abs/2503.07465},
  urldate = {2026-01-05},
  abstract = {Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3\$\textbackslash times\$ less training cost and 1.4\$\textbackslash times\$ inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP\$\textasciicircum b\$ and 0.4 AP\$\textasciicircum m\$ gains over closed-set YOLOv8-L with nearly 4\$\textbackslash times\$ less training time. Code and models are available at https://github.com/THU-MIG/yoloe.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ICCV 2025 Camera-ready Version},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/XTZ6WDR7/Wang et al. - 2025 - YOLOE Real-Time Seeing Anything.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/CDRRU6C4/2503.html}
}

@online{wangYOLOv7TrainableBagoffreebies2022,
  title = {{{YOLOv7}}: {{Trainable}} Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors},
  shorttitle = {{{YOLOv7}}},
  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  date = {2022-07-06},
  eprint = {2207.02696},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.02696},
  url = {http://arxiv.org/abs/2207.02696},
  urldate = {2026-01-06},
  abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/K9WRM94X/Wang et al. - 2022 - YOLOv7 Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/K3HN4JHE/2207.html}
}

@online{xiaGibsonEnvRealWorld2018,
  title = {Gibson {{Env}}: {{Real-World Perception}} for {{Embodied Agents}}},
  shorttitle = {Gibson {{Env}}},
  author = {Xia, Fei and Zamir, Amir and He, Zhi-Yang and Sax, Alexander and Malik, Jitendra and Savarese, Silvio},
  date = {2018-08-31},
  eprint = {1808.10654},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1808.10654},
  url = {http://arxiv.org/abs/1808.10654},
  urldate = {2026-01-06},
  abstract = {Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, "Goggles", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Access the code, dataset, and project website at http://gibsonenv.vision/ . CVPR 2018},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ZNY4ZRRZ/Xia et al. - 2018 - Gibson Env Real-World Perception for Embodied Agents.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/8XISCSLS/1808.html}
}

@online{yamazakiOpenFusionRealtimeOpenVocabulary2023,
  title = {Open-{{Fusion}}: {{Real-time Open-Vocabulary 3D Mapping}} and {{Queryable Scene Representation}}},
  shorttitle = {Open-{{Fusion}}},
  author = {Yamazaki, Kashu and Hanyu, Taisei and Vo, Khoa and Pham, Thang and Tran, Minh and Doretto, Gianfranco and Nguyen, Anh and Le, Ngan},
  date = {2023-10-05},
  eprint = {2310.03923},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.03923},
  url = {http://arxiv.org/abs/2310.03923},
  urldate = {2025-12-11},
  abstract = {Precise 3D environmental mapping is pivotal in robotics. Existing methods often rely on predefined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, a groundbreaking approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pre-trained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D segmentation for open-vocabulary without necessitating additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: https://uark-aicv.github.io/OpenFusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/TXJ928CY/Yamazaki et al. - 2023 - Open-Fusion Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/5UNB552N/2310.html}
}

@online{yokoyamaVLFMVisionLanguageFrontier2023,
  title = {{{VLFM}}: {{Vision-Language Frontier Maps}} for {{Zero-Shot Semantic Navigation}}},
  shorttitle = {{{VLFM}}},
  author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv and Wang, Jiuguang and Bucher, Bernadette},
  date = {2023},
  doi = {10.48550/ARXIV.2312.03275},
  url = {https://arxiv.org/abs/2312.03275},
  urldate = {2025-12-11},
  abstract = {Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Robotics (cs.RO)}
}

@online{zareianOpenVocabularyObjectDetection2021,
  title = {Open-{{Vocabulary Object Detection Using Captions}}},
  author = {Zareian, Alireza and Rosa, Kevin Dela and Hu, Derek Hao and Chang, Shih-Fu},
  date = {2021-03-14},
  eprint = {2011.10678},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.10678},
  url = {http://arxiv.org/abs/2011.10678},
  urldate = {2026-01-03},
  abstract = {Despite the remarkable accuracy of deep neural networks in object detection, they are costly to train and scale due to supervision requirements. Particularly, learning more object categories typically requires proportionally more bounding box annotations. Weakly supervised and zero-shot learning techniques have been explored to scale object detectors to more categories with less supervision, but they have not been as successful and widely adopted as supervised models. In this paper, we put forth a novel formulation of the object detection problem, namely open-vocabulary object detection, which is more general, more practical, and more effective than weakly supervised and zero-shot approaches. We propose a new method to train object detectors using bounding box annotations for a limited set of object categories, as well as image-caption pairs that cover a larger variety of objects at a significantly lower cost. We show that the proposed method can detect and localize objects for which no bounding box annotation is provided during training, at a significantly higher accuracy than zero-shot approaches. Meanwhile, objects with bounding box annotation can be detected almost as accurately as supervised methods, which is significantly better than weakly supervised baselines. Accordingly, we establish a new state of the art for scalable object detection.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: To be presented at CVPR 2021 (oral paper)},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/2Z46A547/Zareian et al. - 2021 - Open-Vocabulary Object Detection Using Captions.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/GNF5RLZW/2011.html}
}

@article{zhanSemanticExplorationDense2025,
  title = {Semantic {{Exploration}} and {{Dense Mapping}} of {{Complex Environments}} Using {{Ground Robot}} with {{Panoramic LiDAR-Camera Fusion}}},
  author = {Zhan, Xiaoyang and Zhou, Shixin and Yang, Qianqian and Zhao, Yixuan and Liu, Hao and Ramineni, Srinivas Chowdary and Shimada, Kenji},
  date = {2025-11},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {10},
  number = {11},
  eprint = {2505.22880},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {11196--11203},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2025.3609216},
  url = {http://arxiv.org/abs/2505.22880},
  urldate = {2025-12-11},
  abstract = {This paper presents a system for autonomous semantic exploration and dense semantic target mapping of a complex unknown environment using a ground robot equipped with a LiDAR-panoramic camera suite. Existing approaches often struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal. To fill this gap, we propose a complete system combining mapping and planning. We first redefine the task as completing both geometric coverage and semantic viewpoint observation. We then manage semantic and geometric viewpoints separately and propose a novel Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This enables explicit multi-view semantic inspection and voxel coverage without unnecessary repetition. Building on this, we develop a hierarchical planner to ensure efficient global coverage. In addition, we propose a Safe Aggressive Exploration State Machine, which allows aggressive exploration behavior while ensuring the robot's safety. Our system includes a plug-and-play semantic target mapping module that integrates seamlessly with state-of-the-art SLAM algorithms for pointcloud-level dense semantic target mapping. We validate our approach through extensive experiments in both realistic simulations and complex real-world environments. Simulation results show that our planner achieves faster exploration and shorter travel distances while guaranteeing a specified number of multi-view inspections. Real-world experiments further confirm the system's effectiveness in achieving accurate dense semantic object mapping of unstructured environments.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  note = {Comment: Accepted by IEEE Robotics and Automation Letters},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/869W3E5V/Zhan et al. - 2025 - Semantic Exploration and Dense Mapping of Complex Environments using Ground Robot with Panoramic LiD.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/V5CCNKQI/2505.html}
}

@online{zhouESCExplorationSoft2023,
  title = {{{ESC}}: {{Exploration}} with {{Soft Commonsense Constraints}} for {{Zero-shot Object Navigation}}},
  shorttitle = {{{ESC}}},
  author = {Zhou, Kaiwen and Zheng, Kaizhi and Pryor, Connor and Shen, Yilin and Jin, Hongxia and Getoor, Lise and Wang, Xin Eric},
  date = {2023-07-06},
  eprint = {2301.13166},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.13166},
  url = {http://arxiv.org/abs/2301.13166},
  urldate = {2025-12-11},
  abstract = {The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method improves significantly over baselines, and achieves new state-of-the-art results for zero-shot object navigation (e.g., 288\% relative Success Rate improvement than CoW on MP3D).},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/LS4TJXLV/Zhou et al. - 2023 - ESC Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/AKX9RGIY/2301.html}
}

@online{zouSegmentEverythingEverywhere2023,
  title = {Segment {{Everything Everywhere All}} at {{Once}}},
  author = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  date = {2023-07-11},
  eprint = {2304.06718},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.06718},
  url = {http://arxiv.org/abs/2304.06718},
  urldate = {2025-12-11},
  abstract = {In this work, we present SEEM, a promptable and interactive model for segmenting everything everywhere all at once in an image, as shown in Fig.1. In SEEM, we propose a novel decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal segmentation interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata: i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks; iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from decoder to image features; and iv) Semantic-awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. Notably, our single SEEM model achieves competitive performance across interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity for generalization to novel prompts or their combinations, rendering it a readily universal image segmentation interface.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JK3PPLKG/Zou et al. - 2023 - Segment Everything Everywhere All at Once.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/ABMEYVHQ/2304.html}
}
