@online{buschOneMapFind2025,
  title = {One {{Map}} to {{Find Them All}}: {{Real-time Open-Vocabulary Mapping}} for {{Zero-shot Multi-Object Navigation}}},
  shorttitle = {One {{Map}} to {{Find Them All}}},
  author = {Busch, Finn Lukas and Homberger, Timon and Ortega-Peimbert, Jesús and Yang, Quantao and Andersson, Olov},
  date = {2025-03-03},
  eprint = {2409.11764},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.11764},
  url = {http://arxiv.org/abs/2409.11764},
  urldate = {2025-12-11},
  abstract = {The capability to efficiently search for objects in complex environments is fundamental for many real-world robot applications. Recent advances in open-vocabulary vision models have resulted in semantically-informed object navigation methods that allow a robot to search for an arbitrary object without prior training. However, these zero-shot methods have so far treated the environment as unknown for each consecutive query. In this paper we introduce a new benchmark for zero-shot multi-object navigation, allowing the robot to leverage information gathered from previous searches to more efficiently find new objects. To address this problem we build a reusable open-vocabulary feature map tailored for real-time object search. We further propose a probabilistic-semantic map update that mitigates common sources of errors in semantic feature extraction and leverage this semantic uncertainty for informed multi-object exploration. We evaluate our method on a set of object navigation tasks in both simulation as well as with a real robot, running in real-time on a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art approaches both on single and multi-object navigation tasks. Additional videos, code and the multi-object navigation benchmark will be available on https://finnbsch.github.io/OneMap.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5TN5PZLW/Busch et al. - 2025 - One Map to Find Them All Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/U2WXYRMQ/2409.html}
}

@online{chaplotObjectGoalNavigation2020,
  title = {Object {{Goal Navigation}} Using {{Goal-Oriented Semantic Exploration}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
  date = {2020-07-02},
  eprint = {2007.00643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.00643},
  url = {http://arxiv.org/abs/2007.00643},
  urldate = {2025-12-11},
  abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Winner of the CVPR 2020 AI-Habitat Object Goal Navigation Challenge. See the project webpage at https://devendrachaplot.github.io/projects/semantic-exploration.html},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/S9CD9YI9/Chaplot et al. - 2020 - Object Goal Navigation using Goal-Oriented Semantic Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DGZ6KSBI/2007.html}
}

@online{guConceptGraphsOpenVocabulary3D2023,
  title = {{{ConceptGraphs}}: {{Open-Vocabulary 3D Scene Graphs}} for {{Perception}} and {{Planning}}},
  shorttitle = {{{ConceptGraphs}}},
  author = {Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and family=Melo, given=Celso Miguel, prefix=de, useprefix=false and Tenenbaum, Joshua B. and Torralba, Antonio and Shkurti, Florian and Paull, Liam},
  date = {2023-09-28},
  eprint = {2309.16650},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16650},
  url = {http://arxiv.org/abs/2309.16650},
  urldate = {2025-12-11},
  abstract = {For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc )},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/2ZBH5Z2M/Gu et al. - 2023 - ConceptGraphs Open-Vocabulary 3D Scene Graphs for Perception and Planning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/XFV25V23/2309.html}
}

@online{huangVisualLanguageMaps2023,
  title = {Visual {{Language Maps}} for {{Robot Navigation}}},
  author = {Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  date = {2023-03-08},
  eprint = {2210.05714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.05714},
  url = {http://arxiv.org/abs/2210.05714},
  urldate = {2025-12-11},
  abstract = {Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., "in between the sofa and TV" or "three meters to the right of the chair") directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Accepted at the 2023 IEEE International Conference on Robotics and Automation (ICRA). Project page: https://vlmaps.github.io},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/G4DABBR7/Huang et al. - 2023 - Visual Language Maps for Robot Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DVA7KMXT/2210.html}
}

@article{kansoSemanticSLAMComprehensive2025,
  title = {Semantic {{SLAM}}: {{A}} Comprehensive Survey of Methods and Applications},
  shorttitle = {Semantic {{SLAM}}},
  author = {Kanso, Houssein and Singh, Abhilasha and Zarif, Etaf El and Almohammed, Nooruldeen and Mounsef, Jinane and Maalouf, Noel and Arain, Bilal},
  date = {2025-12-01},
  journaltitle = {Intelligent Systems with Applications},
  shortjournal = {Intelligent Systems with Applications},
  volume = {28},
  pages = {200591},
  issn = {2667-3053},
  doi = {10.1016/j.iswa.2025.200591},
  url = {https://www.sciencedirect.com/science/article/pii/S2667305325001176},
  urldate = {2025-12-11},
  abstract = {This paper surveys the different approaches in semantic Simultaneous Localization and Mapping (SLAM), exploring how the incorporation of semantic information has enhanced performance in both indoor and outdoor settings, while highlighting key advancements in the field. It also identifies existing gaps and proposes potential directions for future improvements to address these issues. We provide a detailed review of the fundamentals of semantic SLAM, illustrating how incorporating semantic data enhances scene understanding and mapping accuracy. The paper presents semantic SLAM methods and core techniques that contribute to improved robustness and precision in mapping. A comprehensive overview of commonly used datasets for evaluating semantic SLAM systems is provided, along with a discussion of performance metrics used to assess their efficiency and accuracy. To demonstrate the reliability of semantic SLAM methodologies, we reproduce selected results from existing studies offering insights into the reproducibility of these approaches. The paper also addresses key challenges such as real-time processing, dynamic scene adaptation, and scalability while highlighting future research directions. Unlike prior surveys, this paper uniquely combines (i) a systematic taxonomy of semantic SLAM approaches across different sensing modalities and environments, (ii) a comparative review of datasets and evaluation metrics, and (iii) a reproducibility study of selected methods. To our knowledge, this is the first survey that integrates methods, datasets, evaluation practices, and application insights into a single comprehensive review, thereby offering a unified reference for researchers and practitioners. In conclusion, this review underscores the vital role of semantic SLAM in driving advancements in autonomous systems and intelligent navigation by analyzing recent developments, validating findings, and highlighting future research directions.},
  keywords = {Autonomous systems,Dynamic environments,Object-level SLAM,Semantic mapping,Semantic SLAM,Visual SLAM},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JYLEZSMP/Kanso et al. - 2025 - Semantic SLAM A comprehensive survey of methods and applications.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/2C2DQTME/S2667305325001176.html}
}

@article{miaoFrontierReviewSemantic2025,
  title = {A {{Frontier Review}} of {{Semantic SLAM Technologies Applied}} to the {{Open World}}},
  author = {Miao, Le and Liu, Wen and Deng, Zhongliang and Miao, Le and Liu, Wen and Deng, Zhongliang},
  date = {2025-08-12},
  journaltitle = {Sensors},
  volume = {25},
  number = {16},
  publisher = {publisher},
  issn = {1424-8220},
  doi = {10.3390/s25164994},
  url = {https://www.mdpi.com/1424-8220/25/16/4994},
  urldate = {2025-12-11},
  abstract = {With the growing demand for autonomous robotic operations in complex and unstructured environments, traditional semantic SLAM systems—which rely on cl...},
  langid = {english},
  keywords = {multimodal sensor fusion,open-world perception,robotic perception,semantic SLAM,zero-shot learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/NBHH6UZS/Miao et al. - 2025 - A Frontier Review of Semantic SLAM Technologies Applied to the Open World.pdf}
}

@online{qiuLearningGeneralizableFeature2024,
  title = {Learning {{Generalizable Feature Fields}} for {{Mobile Manipulation}}},
  author = {Qiu, Ri-Zhao and Hu, Yafei and Song, Yuchen and Yang, Ge and Fu, Yang and Ye, Jianglong and Mu, Jiteng and Yang, Ruihan and Atanasov, Nikolay and Scherer, Sebastian and Wang, Xiaolong},
  date = {2024-11-26},
  eprint = {2403.07563},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.07563},
  url = {http://arxiv.org/abs/2403.07563},
  urldate = {2025-12-11},
  abstract = {An open problem in mobile manipulation is how to represent objects and scenes in a unified manner so that robots can use both for navigation and manipulation. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherent at an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We quantitatively evaluate GeFF's ability for open-vocabulary object-/part-level manipulation and show that GeFF outperforms point-based baselines in runtime and storage-accuracy trade-offs, with qualitative examples of semantics-aware navigation and articulated object manipulation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Preprint. Project website is at: https://geff-b1.github.io/},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ZQ7PJHJN/Qiu et al. - 2024 - Learning Generalizable Feature Fields for Mobile Manipulation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/BFJCQT64/2403.html}
}

@online{RoboticExplorationUsing,
  title = {Robotic {{Exploration Using Generalized Behavioral Entropy}}},
  url = {https://ieeexplore-1ieee-1org-100033czj02b7.han.technikum-wien.at/document/10608408},
  urldate = {2025-12-11},
  abstract = {This letter presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception. To do this, we introduce a measure of uncertainty that we term “Behavioral entropy”, which builds on Prelec's probability weighting from Behavioral Economics. We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon's and Renyi's. In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here. Then we use Behavioral entropy to define a new type of utility function that can guide a frontier-based environment exploration process. The approach's benefits are illustrated and compared in a Proof-of-Concept and ROS-Unity simulation environment with a Clearpath Warthog robot. We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies.},
  langid = {american},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/MTWWXC4V/10608408.html}
}

@online{topiwalaFrontierBasedExploration2018,
  title = {Frontier {{Based Exploration}} for {{Autonomous Robot}}},
  author = {Topiwala, Anirudh and Inani, Pranav and Kathpal, Abhishek},
  date = {2018-06-10},
  eprint = {1806.03581},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.03581},
  url = {http://arxiv.org/abs/1806.03581},
  urldate = {2025-12-11},
  abstract = {Exploration is process of selecting target points that yield the biggest contribution to a specific gain function at an initially unknown environment. Frontier-based exploration is the most common approach to exploration, wherein frontiers are regions on the boundary between open space and unexplored space. By moving to a new frontier, we can keep building the map of the environment, until there are no new frontiers left to detect. In this paper, an autonomous frontier-based exploration strategy, namely Wavefront Frontier Detector (WFD) is described and implemented on Gazebo Simulation Environment as well as on hardware platform, i.e. Kobuki TurtleBot using Robot Operating System (ROS). The advantage of this algorithm is that the robot can explore large open spaces as well as small cluttered spaces. Further, the map generated from this technique is compared and validated with the map generated using turtlebot\_teleop ROS Package.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8AUGCNII/Topiwala et al. - 2018 - Frontier Based Exploration for Autonomous Robot.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WIHJKPVN/1806.html}
}

@online{yamazakiOpenFusionRealtimeOpenVocabulary2023,
  title = {Open-{{Fusion}}: {{Real-time Open-Vocabulary 3D Mapping}} and {{Queryable Scene Representation}}},
  shorttitle = {Open-{{Fusion}}},
  author = {Yamazaki, Kashu and Hanyu, Taisei and Vo, Khoa and Pham, Thang and Tran, Minh and Doretto, Gianfranco and Nguyen, Anh and Le, Ngan},
  date = {2023-10-05},
  eprint = {2310.03923},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.03923},
  url = {http://arxiv.org/abs/2310.03923},
  urldate = {2025-12-11},
  abstract = {Precise 3D environmental mapping is pivotal in robotics. Existing methods often rely on predefined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, a groundbreaking approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pre-trained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D segmentation for open-vocabulary without necessitating additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: https://uark-aicv.github.io/OpenFusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/TXJ928CY/Yamazaki et al. - 2023 - Open-Fusion Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/5UNB552N/2310.html}
}

@online{yokoyamaVLFMVisionLanguageFrontier2023,
  title = {{{VLFM}}: {{Vision-Language Frontier Maps}} for {{Zero-Shot Semantic Navigation}}},
  shorttitle = {{{VLFM}}},
  author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv and Wang, Jiuguang and Bucher, Bernadette},
  date = {2023},
  doi = {10.48550/ARXIV.2312.03275},
  url = {https://arxiv.org/abs/2312.03275},
  urldate = {2025-12-11},
  abstract = {Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Robotics (cs.RO)}
}

@online{zouSegmentEverythingEverywhere2023,
  title = {Segment {{Everything Everywhere All}} at {{Once}}},
  author = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  date = {2023-07-11},
  eprint = {2304.06718},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.06718},
  url = {http://arxiv.org/abs/2304.06718},
  urldate = {2025-12-11},
  abstract = {In this work, we present SEEM, a promptable and interactive model for segmenting everything everywhere all at once in an image, as shown in Fig.1. In SEEM, we propose a novel decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal segmentation interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata: i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks; iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from decoder to image features; and iv) Semantic-awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. Notably, our single SEEM model achieves competitive performance across interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity for generalization to novel prompts or their combinations, rendering it a readily universal image segmentation interface.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JK3PPLKG/Zou et al. - 2023 - Segment Everything Everywhere All at Once.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/ABMEYVHQ/2304.html}
}
