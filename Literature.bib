@online{alamaRayFrontsOpenSetSemantic2025,
  title = {{{RayFronts}}: {{Open-Set Semantic Ray Frontiers}} for {{Online Scene Understanding}} and {{Exploration}}},
  shorttitle = {{{RayFronts}}},
  author = {Alama, Omar and Bhattacharya, Avigyan and He, Haoyang and Kim, Seungchan and Qiu, Yuheng and Wang, Wenshan and Ho, Cherie and Keetha, Nikhil and Scherer, Sebastian},
  date = {2025-04-09},
  eprint = {2504.06994},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.06994},
  url = {http://arxiv.org/abs/2504.06994},
  urldate = {2025-12-11},
  abstract = {Open-set semantic mapping is crucial for open-world robots. Current mapping approaches either are limited by the depth range or only map beyond-range entities in constrained settings, where overall they fail to combine within-range and beyond-range observations. Furthermore, these methods make a trade-off between fine-grained semantics and efficiency. We introduce RayFronts, a unified representation that enables both dense and beyond-range efficient semantic mapping. RayFronts encodes task-agnostic open-set semantics to both in-range voxels and beyond-range rays encoded at map boundaries, empowering the robot to reduce search volumes significantly and make informed decisions both within \& beyond sensory range, while running at 8.84 Hz on an Orin AGX. Benchmarking the within-range semantics shows that RayFronts's fine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation performance while improving throughput by 16.5x. Traditionally, online mapping performance is entangled with other system components, complicating evaluation. We propose a planner-agnostic evaluation framework that captures the utility for online beyond-range search and exploration, and show RayFronts reduces search volume 2.2x more efficiently than the closest online baselines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/9YK7FELL/Alama et al. - 2025 - RayFronts Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/IUS9YCRU/2504.html}
}

@article{almadhounSurveyInspectingStructures2016,
  title = {A Survey on Inspecting Structures Using Robotic Systems},
  author = {Almadhoun, Randa and Taha, Tarek and Seneviratne, Lakmal and Dias, Jorge and Cai, Guowei},
  date = {2016-11-28},
  journaltitle = {International Journal of Advanced Robotic Systems},
  shortjournal = {International Journal of Advanced Robotic Systems},
  volume = {13},
  doi = {10.1177/1729881416663664},
  abstract = {Advancements in robotics and autonomous systems are being deployed nowadays in many application domains such as search and rescue, industrial automation, domestic services and healthcare. These systems are developed to tackle tasks in some of the most challenging, labour intensive and dangerous environments. Inspecting structures (e.g. bridges, buildings, ships, wind turbines and aircrafts) is considered a hard task for humans to perform and of critical importance since missing any details could affect the structure’s performance and integrity. Additionally, structure inspection is time and resource intensive and should be performed as efficiently and accurately as possible. Inspecting various structures has been reported in the literature using different robotic platforms to: inspect difficult to reach areas and detect various types of faults and anomalies. Typically, inspection missions involve performing three main tasks: coverage path planning, shape, model or surface reconstruction and the actual inspection of the structure. Coverage path planning ensures the generation of an optimized path that guarantees the complete coverage of the structure of interest in order to gather highly accurate information to be used for shape/model reconstruction. This article aims to provide an overview of the recent work and breakthroughs in the field of coverage path planning and model reconstruction, with focus on 3D reconstruction, for the purpose of robotic inspection.},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/UCI3I6EJ/Almadhoun et al. - 2016 - A survey on inspecting structures using robotic systems.pdf}
}

@inproceedings{bourgaultInformationBasedAdaptive2002,
  title = {Information {{Based Adaptive Robotic Exploration}}},
  author = {Bourgault, Frédéric and Makarenko, Alexei and Williams, Stefan and Grocholsky, Ben and Durrant-Whyte, Hugh},
  date = {2002-02-01},
  volume = {1},
  pages = {540-545 vol.1},
  doi = {10.1109/IRDS.2002.1041446},
  abstract = {Exploration involving mapping and concurrent localization in an unknown environment is a pervasive task in mobile robotics. In general, the accuracy of the mapping process depends directly on the accuracy of the localization process. This paper address the problem of maximizing the accuracy of the map building process during exploration by adaptively selecting control actions that maximize localisation accuracy. The map building and exploration task is modeled using an Occupancy Grid (OG) with concurrent localisation performed using a feature-based Simultaneous Localisation And Mapping (SLAM) algorithm. Adaptive sensing aims at maximizing the map information by simultaneously maximizing the expected Shannon information gain (Mutual Information) on the OG map and minimizing the uncertainty of the vehicle pose and map feature uncertainty in the SLAM process. The resulting map building system is demonstrated in an indoor environment using data from a laser scanner mounted on a mobile platform.},
  isbn = {978-0-7803-7398-3},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/C3L3R6FA/Bourgault et al. - 2002 - Information Based Adaptive Robotic Exploration.pdf}
}

@inproceedings{bourgaultInformationBasedAdaptive2002a,
  title = {Information {{Based Adaptive Robotic Exploration}}},
  author = {Bourgault, Frédéric and Makarenko, Alexei and Williams, Stefan and Grocholsky, Ben and Durrant-Whyte, Hugh},
  date = {2002-02-01},
  volume = {1},
  pages = {540-545 vol.1},
  doi = {10.1109/IRDS.2002.1041446},
  abstract = {Exploration involving mapping and concurrent localization in an unknown environment is a pervasive task in mobile robotics. In general, the accuracy of the mapping process depends directly on the accuracy of the localization process. This paper address the problem of maximizing the accuracy of the map building process during exploration by adaptively selecting control actions that maximize localisation accuracy. The map building and exploration task is modeled using an Occupancy Grid (OG) with concurrent localisation performed using a feature-based Simultaneous Localisation And Mapping (SLAM) algorithm. Adaptive sensing aims at maximizing the map information by simultaneously maximizing the expected Shannon information gain (Mutual Information) on the OG map and minimizing the uncertainty of the vehicle pose and map feature uncertainty in the SLAM process. The resulting map building system is demonstrated in an indoor environment using data from a laser scanner mounted on a mobile platform.},
  isbn = {978-0-7803-7398-3},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/SZWDGPE5/Bourgault et al. - 2002 - Information Based Adaptive Robotic Exploration.pdf}
}

@online{buschOneMapFind2025,
  title = {One {{Map}} to {{Find Them All}}: {{Real-time Open-Vocabulary Mapping}} for {{Zero-shot Multi-Object Navigation}}},
  shorttitle = {One {{Map}} to {{Find Them All}}},
  author = {Busch, Finn Lukas and Homberger, Timon and Ortega-Peimbert, Jesús and Yang, Quantao and Andersson, Olov},
  date = {2025-03-03},
  eprint = {2409.11764},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.11764},
  url = {http://arxiv.org/abs/2409.11764},
  urldate = {2025-12-11},
  abstract = {The capability to efficiently search for objects in complex environments is fundamental for many real-world robot applications. Recent advances in open-vocabulary vision models have resulted in semantically-informed object navigation methods that allow a robot to search for an arbitrary object without prior training. However, these zero-shot methods have so far treated the environment as unknown for each consecutive query. In this paper we introduce a new benchmark for zero-shot multi-object navigation, allowing the robot to leverage information gathered from previous searches to more efficiently find new objects. To address this problem we build a reusable open-vocabulary feature map tailored for real-time object search. We further propose a probabilistic-semantic map update that mitigates common sources of errors in semantic feature extraction and leverage this semantic uncertainty for informed multi-object exploration. We evaluate our method on a set of object navigation tasks in both simulation as well as with a real robot, running in real-time on a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art approaches both on single and multi-object navigation tasks. Additional videos, code and the multi-object navigation benchmark will be available on https://finnbsch.github.io/OneMap.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5TN5PZLW/Busch et al. - 2025 - One Map to Find Them All Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/U2WXYRMQ/2409.html}
}

@online{chaplotObjectGoalNavigation2020,
  title = {Object {{Goal Navigation}} Using {{Goal-Oriented Semantic Exploration}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
  date = {2020-07-02},
  eprint = {2007.00643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.00643},
  url = {http://arxiv.org/abs/2007.00643},
  urldate = {2025-12-11},
  abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Winner of the CVPR 2020 AI-Habitat Object Goal Navigation Challenge. See the project webpage at https://devendrachaplot.github.io/projects/semantic-exploration.html},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/S9CD9YI9/Chaplot et al. - 2020 - Object Goal Navigation using Goal-Oriented Semantic Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DGZ6KSBI/2007.html}
}

@online{chaplotObjectGoalNavigation2020a,
  title = {Object {{Goal Navigation}} Using {{Goal-Oriented Semantic Exploration}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
  date = {2020-07-02},
  eprint = {2007.00643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.00643},
  url = {http://arxiv.org/abs/2007.00643},
  urldate = {2025-12-11},
  abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Winner of the CVPR 2020 AI-Habitat Object Goal Navigation Challenge. See the project webpage at https://devendrachaplot.github.io/projects/semantic-exploration.html},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JYXR23T8/Chaplot et al. - 2020 - Object Goal Navigation using Goal-Oriented Semantic Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/PU8J83TN/2007.html}
}

@online{chenHowNotTrain2023,
  title = {How {{To Not Train Your Dragon}}: {{Training-free Embodied Object Goal Navigation}} with {{Semantic Frontiers}}},
  shorttitle = {How {{To Not Train Your Dragon}}},
  author = {Chen, Junting and Li, Guohao and Kumar, Suryansh and Ghanem, Bernard and Yu, Fisher},
  date = {2023-05-26},
  eprint = {2305.16925},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.16925},
  url = {http://arxiv.org/abs/2305.16925},
  urldate = {2025-12-11},
  abstract = {Object goal navigation is an important problem in Embodied AI that involves guiding the agent to navigate to an instance of the object category in an unknown environment -- typically an indoor scene. Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches, \textbackslash eg, end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are typically costly to train and difficult to debug, leading to a lack of transferability and explainability. Inspired by recent successes in combining classical and learning methods, we present a modular and training-free solution, which embraces more classic approaches, to tackle the object goal navigation problem. Our method builds a structured scene representation based on the classic visual simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based frontier exploration to reason about promising areas to search for a goal object. Our structured scene representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph. Our method propagates semantics on the scene graphs based on language priors and scene statistics to introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can reason about the most promising frontier to explore. The proposed pipeline shows strong experimental performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in the object navigation task.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Accepted by/To be published in Robotics: Science and Systems (RSS) 2023; 11 pages, 5 figures},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/RGE5D2PZ/Chen et al. - 2023 - How To Not Train Your Dragon Training-free Embodied Object Goal Navigation with Semantic Frontiers.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/K42WJCBZ/2305.html}
}

@article{dorbalaCanEmbodiedAgent2024,
  title = {Can an {{Embodied Agent Find Your}} "{{Cat-shaped Mug}}"? {{LLM-Guided Exploration}} for {{Zero-Shot Object Navigation}}},
  shorttitle = {Can an {{Embodied Agent Find Your}} "{{Cat-shaped Mug}}"?},
  author = {Dorbala, Vishnu Sashank and Mullen, James F. and Manocha, Dinesh},
  date = {2024-05},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {9},
  number = {5},
  eprint = {2303.03480},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {4083--4090},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2023.3346800},
  url = {http://arxiv.org/abs/2303.03480},
  urldate = {2025-12-11},
  abstract = {We present LGX (Language-guided Exploration), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to a uniquely described target object in a previously unseen environment. Our approach makes use of Large Language Models (LLMs) for this task by leveraging the LLM's commonsense reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27\% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via \textbackslash textit\{real-world\} experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Robotics},
  note = {Comment: 10 pages},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8PEIEHVD/Dorbala et al. - 2024 - Can an Embodied Agent Find Your Cat-shaped Mug LLM-Guided Exploration for Zero-Shot Object Naviga.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/CR35QE3J/2303.html}
}

@online{FrontierbasedApproachAutonomous,
  title = {A Frontier-Based Approach for Autonomous Exploration},
  url = {https://ieeexplore-1ieee-1org-100033czj0337.han.technikum-wien.at/document/613851},
  urldate = {2025-12-12},
  abstract = {We introduce a new approach for exploration based on the concept of frontiers, regions on the boundary between open space and unexplored space. By moving to new frontiers, a mobile robot can extend its map into new territory until the entire environment has been explored. We describe a method for detecting frontiers in evidence grids and navigating to these frontiers. We also introduce a technique for minimizing specular reflections in evidence grids using laser-limited sonar. We have tested this approach with a real mobile robot, exploring real-world office environments cluttered with a variety of obstacles. An advantage of our approach is its ability to explore both large open spaces and narrow cluttered spaces, with walls and obstacles in arbitrary orientation.},
  langid = {american},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5FXT4Y7A/613851.html}
}

@online{gadreCoWsPastureBaselines2022,
  title = {{{CoWs}} on {{Pasture}}: {{Baselines}} and {{Benchmarks}} for {{Language-Driven Zero-Shot Object Navigation}}},
  shorttitle = {{{CoWs}} on {{Pasture}}},
  author = {Gadre, Samir Yitzhak and Wortsman, Mitchell and Ilharco, Gabriel and Schmidt, Ludwig and Song, Shuran},
  date = {2022-12-14},
  eprint = {2203.10421},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.10421},
  url = {http://arxiv.org/abs/2203.10421},
  urldate = {2025-12-11},
  abstract = {For robots to be generally useful, they must be able to find arbitrary objects described by people (i.e., be language-driven) even without expensive navigation training on in-domain data (i.e., perform zero-shot inference). We explore these capabilities in a unified setting: language-driven zero-shot object navigation (L-ZSON). Inspired by the recent success of open-vocabulary models for image classification, we investigate a straightforward framework, CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without fine-tuning. To better evaluate L-ZSON, we introduce the Pasture benchmark, which considers finding uncommon objects, objects described by spatial and appearance attributes, and hidden objects described relative to visible objects. We conduct an in-depth empirical study by directly deploying 21 CoW baselines across Habitat, RoboTHOR, and Pasture. In total, we evaluate over 90k navigation episodes and find that (1) CoW baselines often struggle to leverage language descriptions, but are proficient at finding uncommon objects. (2) A simple CoW, with CLIP-based object localization and classical exploration -- and no additional training -- matches the navigation efficiency of a state-of-the-art ZSON method trained for 500M steps on Habitat MP3D data. This same CoW provides a 15.6 percentage point improvement in success over a state-of-the-art RoboTHOR ZSON model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/68M88XPA/Gadre et al. - 2022 - CoWs on Pasture Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/HV8RG95V/2203.html}
}

@article{gargSemanticsRoboticMapping2020,
  title = {Semantics for {{Robotic Mapping}}, {{Perception}} and {{Interaction}}: {{A Survey}}},
  shorttitle = {Semantics for {{Robotic Mapping}}, {{Perception}} and {{Interaction}}},
  author = {Garg, Sourav and Sünderhauf, Niko and Dayoub, Feras and Morrison, Douglas and Cosgun, Akansel and Carneiro, Gustavo and Wu, Qi and Chin, Tat-Jun and Reid, Ian and Gould, Stephen and Corke, Peter and Milford, Michael},
  date = {2020},
  journaltitle = {Foundations and Trends® in Robotics},
  shortjournal = {FNT in Robotics},
  volume = {8},
  number = {1--2},
  eprint = {2101.00443},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--224},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000059},
  url = {http://arxiv.org/abs/2101.00443},
  urldate = {2025-12-11},
  abstract = {For robots to navigate and interact more richly with the world around them, they will likely require a deeper understanding of the world in which they operate. In robotics and related research fields, the study of understanding is often referred to as semantics, which dictates what does the world "mean" to a robot, and is strongly tied to the question of how to represent that meaning. With humans and robots increasingly operating in the same world, the prospects of human-robot interaction also bring semantics and ontology of natural language into the picture. Driven by need, as well as by enablers like increasing availability of training data and computational resources, semantics is a rapidly growing research area in robotics. The field has received significant attention in the research literature to date, but most reviews and surveys have focused on particular aspects of the topic: the technical research issues regarding its use in specific robotic topics like mapping or segmentation, or its relevance to one particular application domain like autonomous driving. A new treatment is therefore required, and is also timely because so much relevant research has occurred since many of the key surveys were published. This survey therefore provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity, in which semantics are extracted, used, or both. Within these broad categories we survey dozens of major topics including fundamentals from the computer vision field and key robotics research areas utilizing semantics, including mapping, navigation and interaction with the world. The survey also covers key practical considerations, including enablers like increased data availability and improved computational hardware, and major application areas where...},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 81 pages, 1 figure, published in Foundations and Trends in Robotics, 2020},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/C2YANZ7K/Garg et al. - 2020 - Semantics for Robotic Mapping, Perception and Interaction A Survey.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/NFYR4VZ2/2101.html}
}

@online{guConceptGraphsOpenVocabulary3D2023,
  title = {{{ConceptGraphs}}: {{Open-Vocabulary 3D Scene Graphs}} for {{Perception}} and {{Planning}}},
  shorttitle = {{{ConceptGraphs}}},
  author = {Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and family=Melo, given=Celso Miguel, prefix=de, useprefix=false and Tenenbaum, Joshua B. and Torralba, Antonio and Shkurti, Florian and Paull, Liam},
  date = {2023-09-28},
  eprint = {2309.16650},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16650},
  url = {http://arxiv.org/abs/2309.16650},
  urldate = {2025-12-11},
  abstract = {For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc )},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/2ZBH5Z2M/Gu et al. - 2023 - ConceptGraphs Open-Vocabulary 3D Scene Graphs for Perception and Planning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/XFV25V23/2309.html}
}

@online{huangVisualLanguageMaps2023,
  title = {Visual {{Language Maps}} for {{Robot Navigation}}},
  author = {Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  date = {2023-03-08},
  eprint = {2210.05714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.05714},
  url = {http://arxiv.org/abs/2210.05714},
  urldate = {2025-12-11},
  abstract = {Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., "in between the sofa and TV" or "three meters to the right of the chair") directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Accepted at the 2023 IEEE International Conference on Robotics and Automation (ICRA). Project page: https://vlmaps.github.io},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/G4DABBR7/Huang et al. - 2023 - Visual Language Maps for Robot Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DVA7KMXT/2210.html}
}

@online{jiangCLIPDINOVisual2024,
  title = {From {{CLIP}} to {{DINO}}: {{Visual Encoders Shout}} in {{Multi-modal Large Language Models}}},
  shorttitle = {From {{CLIP}} to {{DINO}}},
  author = {Jiang, Dongsheng and Liu, Yuchen and Liu, Songlin and Zhao, Jin'e and Zhang, Hao and Gao, Zhen and Zhang, Xiaopeng and Li, Jin and Xiong, Hongkai},
  date = {2024-03-08},
  eprint = {2310.08825},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.08825},
  url = {http://arxiv.org/abs/2310.08825},
  urldate = {2025-12-14},
  abstract = {Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named COMM, that integrates CLIP and DINO with Multi-level features Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of COMM compared to existing methods, showcasing its enhanced visual capabilities within MLLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5GPWXAI2/Jiang et al. - 2024 - From CLIP to DINO Visual Encoders Shout in Multi-modal Large Language Models.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/5BJ9BFIZ/2310.html}
}

@article{kansoSemanticSLAMComprehensive2025,
  title = {Semantic {{SLAM}}: {{A}} Comprehensive Survey of Methods and Applications},
  shorttitle = {Semantic {{SLAM}}},
  author = {Kanso, Houssein and Singh, Abhilasha and Zarif, Etaf El and Almohammed, Nooruldeen and Mounsef, Jinane and Maalouf, Noel and Arain, Bilal},
  date = {2025-12-01},
  journaltitle = {Intelligent Systems with Applications},
  shortjournal = {Intelligent Systems with Applications},
  volume = {28},
  pages = {200591},
  issn = {2667-3053},
  doi = {10.1016/j.iswa.2025.200591},
  url = {https://www.sciencedirect.com/science/article/pii/S2667305325001176},
  urldate = {2025-12-11},
  abstract = {This paper surveys the different approaches in semantic Simultaneous Localization and Mapping (SLAM), exploring how the incorporation of semantic information has enhanced performance in both indoor and outdoor settings, while highlighting key advancements in the field. It also identifies existing gaps and proposes potential directions for future improvements to address these issues. We provide a detailed review of the fundamentals of semantic SLAM, illustrating how incorporating semantic data enhances scene understanding and mapping accuracy. The paper presents semantic SLAM methods and core techniques that contribute to improved robustness and precision in mapping. A comprehensive overview of commonly used datasets for evaluating semantic SLAM systems is provided, along with a discussion of performance metrics used to assess their efficiency and accuracy. To demonstrate the reliability of semantic SLAM methodologies, we reproduce selected results from existing studies offering insights into the reproducibility of these approaches. The paper also addresses key challenges such as real-time processing, dynamic scene adaptation, and scalability while highlighting future research directions. Unlike prior surveys, this paper uniquely combines (i) a systematic taxonomy of semantic SLAM approaches across different sensing modalities and environments, (ii) a comparative review of datasets and evaluation metrics, and (iii) a reproducibility study of selected methods. To our knowledge, this is the first survey that integrates methods, datasets, evaluation practices, and application insights into a single comprehensive review, thereby offering a unified reference for researchers and practitioners. In conclusion, this review underscores the vital role of semantic SLAM in driving advancements in autonomous systems and intelligent navigation by analyzing recent developments, validating findings, and highlighting future research directions.},
  keywords = {Autonomous systems,Dynamic environments,Object-level SLAM,Semantic mapping,Semantic SLAM,Visual SLAM},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JYLEZSMP/Kanso et al. - 2025 - Semantic SLAM A comprehensive survey of methods and applications.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/2C2DQTME/S2667305325001176.html}
}

@online{liGroundedLanguageImagePretraining2022,
  title = {Grounded {{Language-Image Pre-training}}},
  author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
  date = {2022-06-17},
  eprint = {2112.03857},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.03857},
  url = {http://arxiv.org/abs/2112.03857},
  urldate = {2025-12-15},
  abstract = {This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  note = {Comment: CVPR 2022; updated visualizations; fixed hyper-parameters in Appendix C.1},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/EGSYAAGC/Li et al. - 2022 - Grounded Language-Image Pre-training.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/8V9KT4RJ/2112.html}
}

@online{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2015-02-21},
  eprint = {1405.0312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1405.0312},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2025-12-11},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/I5ATQM2D/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/RZ8JVYSE/1405.html}
}

@article{lluviaActiveMappingRobot2021,
  title = {Active {{Mapping}} and {{Robot Exploration}}: {{A Survey}}},
  shorttitle = {Active {{Mapping}} and {{Robot Exploration}}},
  author = {Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander and Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander},
  date = {2021-04-02},
  journaltitle = {Sensors},
  volume = {21},
  number = {7},
  publisher = {publisher},
  issn = {1424-8220},
  doi = {10.3390/s21072445},
  url = {https://www.mdpi.com/1424-8220/21/7/2445},
  urldate = {2025-12-12},
  abstract = {Simultaneous localization and mapping responds to the problem of building a map of the environment without any prior information and based on the data...},
  langid = {english},
  keywords = {exploration,frontiers,mapping,mobile robots,next best view,path planning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WJMBQ9GT/Lluvia et al. - 2021 - Active Mapping and Robot Exploration A Survey.pdf}
}

@online{majumdarZSONZeroShotObjectGoal2023,
  title = {{{ZSON}}: {{Zero-Shot Object-Goal Navigation}} Using {{Multimodal Goal Embeddings}}},
  shorttitle = {{{ZSON}}},
  author = {Majumdar, Arjun and Aggarwal, Gunjan and Devnani, Bhavika and Hoffman, Judy and Batra, Dhruv},
  date = {2023-10-13},
  eprint = {2206.12403},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.12403},
  url = {http://arxiv.org/abs/2206.12403},
  urldate = {2025-12-11},
  abstract = {We present a scalable approach for learning open-world object-goal navigation (ObjectNav) -- the task of asking a virtual robot (agent) to find any instance of an object in an unexplored environment (e.g., "find a sink"). Our approach is entirely zero-shot -- i.e., it does not require ObjectNav rewards or demonstrations of any kind. Instead, we train on the image-goal navigation (ImageNav) task, in which agents find the location where a picture (i.e., goal image) was captured. Specifically, we encode goal images into a multimodal, semantic embedding space to enable training semantic-goal navigation (SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D). After training, SemanticNav agents can be instructed to find objects described in free-form natural language (e.g., "sink", "bathroom sink", etc.) by projecting language goals into the same multimodal, semantic embedding space. As a result, our approach enables open-world ObjectNav. We extensively evaluate our agents on three ObjectNav datasets (Gibson, HM3D, and MP3D) and observe absolute improvements in success of 4.2\% - 20.0\% over existing zero-shot methods. For reference, these gains are similar or better than the 5\% improvement in success between the Habitat 2020 and 2021 ObjectNav challenge winners. In an open-world setting, we discover that our agents can generalize to compound instructions with a room explicitly mentioned (e.g., "Find a kitchen sink") and when the target room can be inferred (e.g., "Find a sink and a stove").},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: code: https://github.com/gunagg/zson},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/Q9ILVZFE/Majumdar et al. - 2023 - ZSON Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/QJ3M4582/2206.html}
}

@article{miaoFrontierReviewSemantic2025,
  title = {A {{Frontier Review}} of {{Semantic SLAM Technologies Applied}} to the {{Open World}}},
  author = {Miao, Le and Liu, Wen and Deng, Zhongliang and Miao, Le and Liu, Wen and Deng, Zhongliang},
  date = {2025-08-12},
  journaltitle = {Sensors},
  volume = {25},
  number = {16},
  publisher = {publisher},
  issn = {1424-8220},
  doi = {10.3390/s25164994},
  url = {https://www.mdpi.com/1424-8220/25/16/4994},
  urldate = {2025-12-11},
  abstract = {With the growing demand for autonomous robotic operations in complex and unstructured environments, traditional semantic SLAM systems—which rely on cl...},
  langid = {english},
  keywords = {multimodal sensor fusion,open-world perception,robotic perception,semantic SLAM,zero-shot learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/NBHH6UZS/Miao et al. - 2025 - A Frontier Review of Semantic SLAM Technologies Applied to the Open World.pdf}
}

@online{PIGEONVLMDrivenObject,
  title = {{{PIGEON}}: {{VLM-Driven Object Navigation}} via {{Points}} of {{Interest SelectionPreprint}}. {{Work}} in {{Progress}}.},
  url = {https://arxiv.org/html/2511.13207v1},
  urldate = {2025-12-11},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/P4ZNIYZ5/2511.html}
}

@online{qiuLearningGeneralizableFeature2024,
  title = {Learning {{Generalizable Feature Fields}} for {{Mobile Manipulation}}},
  author = {Qiu, Ri-Zhao and Hu, Yafei and Song, Yuchen and Yang, Ge and Fu, Yang and Ye, Jianglong and Mu, Jiteng and Yang, Ruihan and Atanasov, Nikolay and Scherer, Sebastian and Wang, Xiaolong},
  date = {2024-11-26},
  eprint = {2403.07563},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.07563},
  url = {http://arxiv.org/abs/2403.07563},
  urldate = {2025-12-11},
  abstract = {An open problem in mobile manipulation is how to represent objects and scenes in a unified manner so that robots can use both for navigation and manipulation. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherent at an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We quantitatively evaluate GeFF's ability for open-vocabulary object-/part-level manipulation and show that GeFF outperforms point-based baselines in runtime and storage-accuracy trade-offs, with qualitative examples of semantics-aware navigation and articulated object manipulation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Preprint. Project website is at: https://geff-b1.github.io/},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ZQ7PJHJN/Qiu et al. - 2024 - Learning Generalizable Feature Fields for Mobile Manipulation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/BFJCQT64/2403.html}
}

@article{quinApproachesEfficientlyDetecting2021,
  title = {Approaches for {{Efficiently Detecting Frontier Cells}} in {{Robotics Exploration}}},
  author = {Quin, Phillip and Nguyen, Dac and Vu, Thanh and Alempijevic, Alen and Paul, Gavin},
  date = {2021-02-25},
  journaltitle = {Frontiers in Robotics and AI},
  shortjournal = {Frontiers in Robotics and AI},
  volume = {8},
  pages = {616470},
  doi = {10.3389/frobt.2021.616470},
  abstract = {Many robot exploration algorithms that are used to explore office, home, or outdoor environments, rely on the concept of frontier cells. Frontier cells define the border between known and unknown space. Frontier-based exploration is the process of repeatedly detecting frontiers and moving towards them, until there are no more frontiers and therefore no more unknown regions. The faster frontier cells can be detected, the more efficient exploration becomes. This paper proposes several algorithms for detecting frontiers. The first is called Naïve Active Area (NaïveAA) frontier detection and achieves frontier detection in constant time by only evaluating the cells in the active area defined by scans taken. The second algorithm is called Expanding-Wavefront Frontier Detection (EWFD) and uses frontiers from the previous timestep as a starting point for searching for frontiers in newly discovered space. The third approach is called Frontier-Tracing Frontier Detection (FTFD) and also uses the frontiers from the previous timestep as well as the endpoints of the scan, to determine the frontiers at the current timestep. Algorithms are compared to state-of-the-art algorithms such as Naïve, WFD, and WFD-INC. NaïveAA is shown to operate in constant time and therefore is suitable as a basic benchmark for frontier detection algorithms. EWFD and FTFD are found to be significantly faster than other algorithms.},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/A6KDFCUD/Quin et al. - 2021 - Approaches for Efficiently Detecting Frontier Cells in Robotics Exploration.pdf}
}

@online{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.00020},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2025-12-13},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/WBDSAGZM/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/QHYY9RBJ/2103.html}
}

@online{ramakrishnanPONIPotentialFunctions2022,
  title = {{{PONI}}: {{Potential Functions}} for {{ObjectGoal Navigation}} with {{Interaction-free Learning}}},
  shorttitle = {{{PONI}}},
  author = {Ramakrishnan, Santhosh Kumar and Chaplot, Devendra Singh and Al-Halah, Ziad and Malik, Jitendra and Grauman, Kristen},
  date = {2022-06-17},
  eprint = {2201.10029},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.10029},
  url = {http://arxiv.org/abs/2201.10029},
  urldate = {2025-12-11},
  abstract = {State-of-the-art approaches to ObjectGoal navigation rely on reinforcement learning and typically require significant computational resources and time for learning. We propose Potential functions for ObjectGoal Navigation with Interaction-free learning (PONI), a modular approach that disentangles the skills of `where to look?' for an object and `how to navigate to (x, y)?'. Our key insight is that `where to look?' can be treated purely as a perception problem, and learned without environment interactions. To address this, we propose a network that predicts two complementary potential functions conditioned on a semantic map and uses them to decide where to look for an unseen object. We train the potential function network using supervised learning on a passive dataset of top-down semantic maps, and integrate it into a modular framework to perform ObjectGoal navigation. Experiments on Gibson and Matterport3D demonstrate that our method achieves the state-of-the-art for ObjectGoal navigation while incurring up to 1,600x less computational cost for training. Code and pre-trained models are available: https://vision.cs.utexas.edu/projects/poni/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 8 pages + supplementary. Accepted in CVPR 2022},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/SCUSH2CB/Ramakrishnan et al. - 2022 - PONI Potential Functions for ObjectGoal Navigation with Interaction-free Learning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/EY8AD5U3/2201.html}
}

@online{ramrakhyaPIRLNavPretrainingImitation2023,
  title = {{{PIRLNav}}: {{Pretraining}} with {{Imitation}} and {{RL Finetuning}} for {{ObjectNav}}},
  shorttitle = {{{PIRLNav}}},
  author = {Ramrakhya, Ram and Batra, Dhruv and Wijmans, Erik and Das, Abhishek},
  date = {2023-03-26},
  eprint = {2301.07302},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.07302},
  url = {http://arxiv.org/abs/2301.07302},
  urldate = {2025-12-11},
  abstract = {We study ObjectGoal Navigation -- where a virtual robot situated in a new environment is asked to navigate to an object. Prior work has shown that imitation learning (IL) using behavior cloning (BC) on a dataset of human demonstrations achieves promising results. However, this has limitations -- 1) BC policies generalize poorly to new states, since the training mimics actions not their consequences, and 2) collecting demonstrations is expensive. On the other hand, reinforcement learning (RL) is trivially scalable, but requires careful reward engineering to achieve desirable behavior. We present PIRLNav, a two-stage learning scheme for BC pretraining on human demonstrations followed by RL-finetuning. This leads to a policy that achieves a success rate of \$65.0\textbackslash\%\$ on ObjectNav (\$+5.0\textbackslash\%\$ absolute over previous state-of-the-art). Using this BC\$\textbackslash rightarrow\$RL training recipe, we present a rigorous empirical analysis of design choices. First, we investigate whether human demonstrations can be replaced with `free' (automatically generated) sources of demonstrations, e.g. shortest paths (SP) or task-agnostic frontier exploration (FE) trajectories. We find that BC\$\textbackslash rightarrow\$RL on human demonstrations outperforms BC\$\textbackslash rightarrow\$RL on SP and FE trajectories, even when controlled for same BC-pretraining success on train, and even on a subset of val episodes where BC-pretraining success favors the SP or FE policies. Next, we study how RL-finetuning performance scales with the size of the BC pretraining dataset. We find that as we increase the size of BC-pretraining dataset and get to high BC accuracies, improvements from RL-finetuning are smaller, and that \$90\textbackslash\%\$ of the performance of our best BC\$\textbackslash rightarrow\$RL policy can be achieved with less than half the number of BC demonstrations. Finally, we analyze failure modes of our ObjectNav policies, and present guidelines for further improving them.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 8 pages + supplement},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/44TK5T7E/Ramrakhya et al. - 2023 - PIRLNav Pretraining with Imitation and RL Finetuning for ObjectNav.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/KVXYYXVM/2301.html}
}

@online{RoboticExplorationUsing,
  title = {Robotic {{Exploration Using Generalized Behavioral Entropy}}},
  url = {https://ieeexplore-1ieee-1org-100033czj02b7.han.technikum-wien.at/document/10608408},
  urldate = {2025-12-11},
  abstract = {This letter presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception. To do this, we introduce a measure of uncertainty that we term “Behavioral entropy”, which builds on Prelec's probability weighting from Behavioral Economics. We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon's and Renyi's. In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here. Then we use Behavioral entropy to define a new type of utility function that can guide a frontier-based environment exploration process. The approach's benefits are illustrated and compared in a Proof-of-Concept and ROS-Unity simulation environment with a Clearpath Warthog robot. We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies.},
  langid = {american},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/MTWWXC4V/10608408.html}
}

@inproceedings{ruanTaxonomySemanticInformation2022,
  title = {A {{Taxonomy}} of {{Semantic Information}} in {{Robot-Assisted Disaster Response}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Safety}}, {{Security}}, and {{Rescue Robotics}} ({{SSRR}})},
  author = {Ruan, Tianshu and Wang, Hao and Stolkin, Rustam and Chiou, Manolis},
  date = {2022-11-08},
  eprint = {2210.00125},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {285--292},
  doi = {10.1109/SSRR56537.2022.10018727},
  url = {http://arxiv.org/abs/2210.00125},
  urldate = {2025-12-11},
  abstract = {This paper proposes a taxonomy of semantic information in robot-assisted disaster response. Robots are increasingly being used in hazardous environment industries and emergency response teams to perform various tasks. Operational decision-making in such applications requires a complex semantic understanding of environments that are remote from the human operator. Low-level sensory data from the robot is transformed into perception and informative cognition. Currently, such cognition is predominantly performed by a human expert, who monitors remote sensor data such as robot video feeds. This engenders a need for AI-generated semantic understanding capabilities on the robot itself. Current work on semantics and AI lies towards the relatively academic end of the research spectrum, hence relatively removed from the practical realities of first responder teams. We aim for this paper to be a step towards bridging this divide. We first review common robot tasks in disaster response and the types of information such robots must collect. We then organize the types of semantic features and understanding that may be useful in disaster operations into a taxonomy of semantic information. We also briefly review the current state-of-the-art semantic understanding techniques. We highlight potential synergies, but we also identify gaps that need to be bridged to apply these ideas. We aim to stimulate the research that is needed to adapt, robustify, and implement state-of-the-art AI semantics methods in the challenging conditions of disasters and first responder scenarios.},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/BF73MJ3E/Ruan et al. - 2022 - A Taxonomy of Semantic Information in Robot-Assisted Disaster Response.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/JBWAHDQ4/2210.html}
}

@online{topiwalaFrontierBasedExploration2018,
  title = {Frontier {{Based Exploration}} for {{Autonomous Robot}}},
  author = {Topiwala, Anirudh and Inani, Pranav and Kathpal, Abhishek},
  date = {2018-06-10},
  eprint = {1806.03581},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.03581},
  url = {http://arxiv.org/abs/1806.03581},
  urldate = {2025-12-11},
  abstract = {Exploration is process of selecting target points that yield the biggest contribution to a specific gain function at an initially unknown environment. Frontier-based exploration is the most common approach to exploration, wherein frontiers are regions on the boundary between open space and unexplored space. By moving to a new frontier, we can keep building the map of the environment, until there are no new frontiers left to detect. In this paper, an autonomous frontier-based exploration strategy, namely Wavefront Frontier Detector (WFD) is described and implemented on Gazebo Simulation Environment as well as on hardware platform, i.e. Kobuki TurtleBot using Robot Operating System (ROS). The advantage of this algorithm is that the robot can explore large open spaces as well as small cluttered spaces. Further, the map generated from this technique is compared and validated with the map generated using turtlebot\_teleop ROS Package.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8AUGCNII/Topiwala et al. - 2018 - Frontier Based Exploration for Autonomous Robot.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WIHJKPVN/1806.html}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-12-11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 15 pages, 5 figures},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8LCESDHM/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WHZ65CNY/1706.html}
}

@online{yamazakiOpenFusionRealtimeOpenVocabulary2023,
  title = {Open-{{Fusion}}: {{Real-time Open-Vocabulary 3D Mapping}} and {{Queryable Scene Representation}}},
  shorttitle = {Open-{{Fusion}}},
  author = {Yamazaki, Kashu and Hanyu, Taisei and Vo, Khoa and Pham, Thang and Tran, Minh and Doretto, Gianfranco and Nguyen, Anh and Le, Ngan},
  date = {2023-10-05},
  eprint = {2310.03923},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.03923},
  url = {http://arxiv.org/abs/2310.03923},
  urldate = {2025-12-11},
  abstract = {Precise 3D environmental mapping is pivotal in robotics. Existing methods often rely on predefined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, a groundbreaking approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pre-trained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D segmentation for open-vocabulary without necessitating additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: https://uark-aicv.github.io/OpenFusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/TXJ928CY/Yamazaki et al. - 2023 - Open-Fusion Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/5UNB552N/2310.html}
}

@online{yokoyamaVLFMVisionLanguageFrontier2023,
  title = {{{VLFM}}: {{Vision-Language Frontier Maps}} for {{Zero-Shot Semantic Navigation}}},
  shorttitle = {{{VLFM}}},
  author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv and Wang, Jiuguang and Bucher, Bernadette},
  date = {2023},
  doi = {10.48550/ARXIV.2312.03275},
  url = {https://arxiv.org/abs/2312.03275},
  urldate = {2025-12-11},
  abstract = {Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Robotics (cs.RO)}
}

@article{zhanSemanticExplorationDense2025,
  title = {Semantic {{Exploration}} and {{Dense Mapping}} of {{Complex Environments}} Using {{Ground Robot}} with {{Panoramic LiDAR-Camera Fusion}}},
  author = {Zhan, Xiaoyang and Zhou, Shixin and Yang, Qianqian and Zhao, Yixuan and Liu, Hao and Ramineni, Srinivas Chowdary and Shimada, Kenji},
  date = {2025-11},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {10},
  number = {11},
  eprint = {2505.22880},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {11196--11203},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2025.3609216},
  url = {http://arxiv.org/abs/2505.22880},
  urldate = {2025-12-11},
  abstract = {This paper presents a system for autonomous semantic exploration and dense semantic target mapping of a complex unknown environment using a ground robot equipped with a LiDAR-panoramic camera suite. Existing approaches often struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal. To fill this gap, we propose a complete system combining mapping and planning. We first redefine the task as completing both geometric coverage and semantic viewpoint observation. We then manage semantic and geometric viewpoints separately and propose a novel Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This enables explicit multi-view semantic inspection and voxel coverage without unnecessary repetition. Building on this, we develop a hierarchical planner to ensure efficient global coverage. In addition, we propose a Safe Aggressive Exploration State Machine, which allows aggressive exploration behavior while ensuring the robot's safety. Our system includes a plug-and-play semantic target mapping module that integrates seamlessly with state-of-the-art SLAM algorithms for pointcloud-level dense semantic target mapping. We validate our approach through extensive experiments in both realistic simulations and complex real-world environments. Simulation results show that our planner achieves faster exploration and shorter travel distances while guaranteeing a specified number of multi-view inspections. Real-world experiments further confirm the system's effectiveness in achieving accurate dense semantic object mapping of unstructured environments.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  note = {Comment: Accepted by IEEE Robotics and Automation Letters},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/869W3E5V/Zhan et al. - 2025 - Semantic Exploration and Dense Mapping of Complex Environments using Ground Robot with Panoramic LiD.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/V5CCNKQI/2505.html}
}

@online{zhouESCExplorationSoft2023,
  title = {{{ESC}}: {{Exploration}} with {{Soft Commonsense Constraints}} for {{Zero-shot Object Navigation}}},
  shorttitle = {{{ESC}}},
  author = {Zhou, Kaiwen and Zheng, Kaizhi and Pryor, Connor and Shen, Yilin and Jin, Hongxia and Getoor, Lise and Wang, Xin Eric},
  date = {2023-07-06},
  eprint = {2301.13166},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.13166},
  url = {http://arxiv.org/abs/2301.13166},
  urldate = {2025-12-11},
  abstract = {The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method improves significantly over baselines, and achieves new state-of-the-art results for zero-shot object navigation (e.g., 288\% relative Success Rate improvement than CoW on MP3D).},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/LS4TJXLV/Zhou et al. - 2023 - ESC Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/AKX9RGIY/2301.html}
}

@online{zouSegmentEverythingEverywhere2023,
  title = {Segment {{Everything Everywhere All}} at {{Once}}},
  author = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  date = {2023-07-11},
  eprint = {2304.06718},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.06718},
  url = {http://arxiv.org/abs/2304.06718},
  urldate = {2025-12-11},
  abstract = {In this work, we present SEEM, a promptable and interactive model for segmenting everything everywhere all at once in an image, as shown in Fig.1. In SEEM, we propose a novel decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal segmentation interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata: i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks; iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from decoder to image features; and iv) Semantic-awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. Notably, our single SEEM model achieves competitive performance across interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity for generalization to novel prompts or their combinations, rendering it a readily universal image segmentation interface.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JK3PPLKG/Zou et al. - 2023 - Segment Everything Everywhere All at Once.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/ABMEYVHQ/2304.html}
}
