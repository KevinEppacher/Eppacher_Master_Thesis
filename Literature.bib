@article{almadhounSurveyInspectingStructures2016,
  title = {A Survey on Inspecting Structures Using Robotic Systems},
  author = {Almadhoun, Randa and Taha, Tarek and Seneviratne, Lakmal and Dias, Jorge and Cai, Guowei},
  date = {2016-11-28},
  journaltitle = {International Journal of Advanced Robotic Systems},
  shortjournal = {International Journal of Advanced Robotic Systems},
  volume = {13},
  doi = {10.1177/1729881416663664},
  abstract = {Advancements in robotics and autonomous systems are being deployed nowadays in many application domains such as search and rescue, industrial automation, domestic services and healthcare. These systems are developed to tackle tasks in some of the most challenging, labour intensive and dangerous environments. Inspecting structures (e.g. bridges, buildings, ships, wind turbines and aircrafts) is considered a hard task for humans to perform and of critical importance since missing any details could affect the structure’s performance and integrity. Additionally, structure inspection is time and resource intensive and should be performed as efficiently and accurately as possible. Inspecting various structures has been reported in the literature using different robotic platforms to: inspect difficult to reach areas and detect various types of faults and anomalies. Typically, inspection missions involve performing three main tasks: coverage path planning, shape, model or surface reconstruction and the actual inspection of the structure. Coverage path planning ensures the generation of an optimized path that guarantees the complete coverage of the structure of interest in order to gather highly accurate information to be used for shape/model reconstruction. This article aims to provide an overview of the recent work and breakthroughs in the field of coverage path planning and model reconstruction, with focus on 3D reconstruction, for the purpose of robotic inspection.},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/UCI3I6EJ/Almadhoun et al. - 2016 - A survey on inspecting structures using robotic systems.pdf}
}

@online{buschOneMapFind2025,
  title = {One {{Map}} to {{Find Them All}}: {{Real-time Open-Vocabulary Mapping}} for {{Zero-shot Multi-Object Navigation}}},
  shorttitle = {One {{Map}} to {{Find Them All}}},
  author = {Busch, Finn Lukas and Homberger, Timon and Ortega-Peimbert, Jesús and Yang, Quantao and Andersson, Olov},
  date = {2025-03-03},
  eprint = {2409.11764},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.11764},
  url = {http://arxiv.org/abs/2409.11764},
  urldate = {2025-12-11},
  abstract = {The capability to efficiently search for objects in complex environments is fundamental for many real-world robot applications. Recent advances in open-vocabulary vision models have resulted in semantically-informed object navigation methods that allow a robot to search for an arbitrary object without prior training. However, these zero-shot methods have so far treated the environment as unknown for each consecutive query. In this paper we introduce a new benchmark for zero-shot multi-object navigation, allowing the robot to leverage information gathered from previous searches to more efficiently find new objects. To address this problem we build a reusable open-vocabulary feature map tailored for real-time object search. We further propose a probabilistic-semantic map update that mitigates common sources of errors in semantic feature extraction and leverage this semantic uncertainty for informed multi-object exploration. We evaluate our method on a set of object navigation tasks in both simulation as well as with a real robot, running in real-time on a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art approaches both on single and multi-object navigation tasks. Additional videos, code and the multi-object navigation benchmark will be available on https://finnbsch.github.io/OneMap.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/5TN5PZLW/Busch et al. - 2025 - One Map to Find Them All Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/U2WXYRMQ/2409.html}
}

@online{chaplotObjectGoalNavigation2020,
  title = {Object {{Goal Navigation}} Using {{Goal-Oriented Semantic Exploration}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
  date = {2020-07-02},
  eprint = {2007.00643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.00643},
  url = {http://arxiv.org/abs/2007.00643},
  urldate = {2025-12-11},
  abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Winner of the CVPR 2020 AI-Habitat Object Goal Navigation Challenge. See the project webpage at https://devendrachaplot.github.io/projects/semantic-exploration.html},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/S9CD9YI9/Chaplot et al. - 2020 - Object Goal Navigation using Goal-Oriented Semantic Exploration.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DGZ6KSBI/2007.html}
}

@article{gargSemanticsRoboticMapping2020,
  title = {Semantics for {{Robotic Mapping}}, {{Perception}} and {{Interaction}}: {{A Survey}}},
  shorttitle = {Semantics for {{Robotic Mapping}}, {{Perception}} and {{Interaction}}},
  author = {Garg, Sourav and Sünderhauf, Niko and Dayoub, Feras and Morrison, Douglas and Cosgun, Akansel and Carneiro, Gustavo and Wu, Qi and Chin, Tat-Jun and Reid, Ian and Gould, Stephen and Corke, Peter and Milford, Michael},
  date = {2020},
  journaltitle = {Foundations and Trends® in Robotics},
  shortjournal = {FNT in Robotics},
  volume = {8},
  number = {1--2},
  eprint = {2101.00443},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--224},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000059},
  url = {http://arxiv.org/abs/2101.00443},
  urldate = {2025-12-11},
  abstract = {For robots to navigate and interact more richly with the world around them, they will likely require a deeper understanding of the world in which they operate. In robotics and related research fields, the study of understanding is often referred to as semantics, which dictates what does the world "mean" to a robot, and is strongly tied to the question of how to represent that meaning. With humans and robots increasingly operating in the same world, the prospects of human-robot interaction also bring semantics and ontology of natural language into the picture. Driven by need, as well as by enablers like increasing availability of training data and computational resources, semantics is a rapidly growing research area in robotics. The field has received significant attention in the research literature to date, but most reviews and surveys have focused on particular aspects of the topic: the technical research issues regarding its use in specific robotic topics like mapping or segmentation, or its relevance to one particular application domain like autonomous driving. A new treatment is therefore required, and is also timely because so much relevant research has occurred since many of the key surveys were published. This survey therefore provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity, in which semantics are extracted, used, or both. Within these broad categories we survey dozens of major topics including fundamentals from the computer vision field and key robotics research areas utilizing semantics, including mapping, navigation and interaction with the world. The survey also covers key practical considerations, including enablers like increased data availability and improved computational hardware, and major application areas where...},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 81 pages, 1 figure, published in Foundations and Trends in Robotics, 2020},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/C2YANZ7K/Garg et al. - 2020 - Semantics for Robotic Mapping, Perception and Interaction A Survey.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/NFYR4VZ2/2101.html}
}

@online{guConceptGraphsOpenVocabulary3D2023,
  title = {{{ConceptGraphs}}: {{Open-Vocabulary 3D Scene Graphs}} for {{Perception}} and {{Planning}}},
  shorttitle = {{{ConceptGraphs}}},
  author = {Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and family=Melo, given=Celso Miguel, prefix=de, useprefix=false and Tenenbaum, Joshua B. and Torralba, Antonio and Shkurti, Florian and Paull, Liam},
  date = {2023-09-28},
  eprint = {2309.16650},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16650},
  url = {http://arxiv.org/abs/2309.16650},
  urldate = {2025-12-11},
  abstract = {For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc )},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/2ZBH5Z2M/Gu et al. - 2023 - ConceptGraphs Open-Vocabulary 3D Scene Graphs for Perception and Planning.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/XFV25V23/2309.html}
}

@online{huangVisualLanguageMaps2023,
  title = {Visual {{Language Maps}} for {{Robot Navigation}}},
  author = {Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  date = {2023-03-08},
  eprint = {2210.05714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.05714},
  url = {http://arxiv.org/abs/2210.05714},
  urldate = {2025-12-11},
  abstract = {Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., "in between the sofa and TV" or "three meters to the right of the chair") directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Accepted at the 2023 IEEE International Conference on Robotics and Automation (ICRA). Project page: https://vlmaps.github.io},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/G4DABBR7/Huang et al. - 2023 - Visual Language Maps for Robot Navigation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/DVA7KMXT/2210.html}
}

@article{kansoSemanticSLAMComprehensive2025,
  title = {Semantic {{SLAM}}: {{A}} Comprehensive Survey of Methods and Applications},
  shorttitle = {Semantic {{SLAM}}},
  author = {Kanso, Houssein and Singh, Abhilasha and Zarif, Etaf El and Almohammed, Nooruldeen and Mounsef, Jinane and Maalouf, Noel and Arain, Bilal},
  date = {2025-12-01},
  journaltitle = {Intelligent Systems with Applications},
  shortjournal = {Intelligent Systems with Applications},
  volume = {28},
  pages = {200591},
  issn = {2667-3053},
  doi = {10.1016/j.iswa.2025.200591},
  url = {https://www.sciencedirect.com/science/article/pii/S2667305325001176},
  urldate = {2025-12-11},
  abstract = {This paper surveys the different approaches in semantic Simultaneous Localization and Mapping (SLAM), exploring how the incorporation of semantic information has enhanced performance in both indoor and outdoor settings, while highlighting key advancements in the field. It also identifies existing gaps and proposes potential directions for future improvements to address these issues. We provide a detailed review of the fundamentals of semantic SLAM, illustrating how incorporating semantic data enhances scene understanding and mapping accuracy. The paper presents semantic SLAM methods and core techniques that contribute to improved robustness and precision in mapping. A comprehensive overview of commonly used datasets for evaluating semantic SLAM systems is provided, along with a discussion of performance metrics used to assess their efficiency and accuracy. To demonstrate the reliability of semantic SLAM methodologies, we reproduce selected results from existing studies offering insights into the reproducibility of these approaches. The paper also addresses key challenges such as real-time processing, dynamic scene adaptation, and scalability while highlighting future research directions. Unlike prior surveys, this paper uniquely combines (i) a systematic taxonomy of semantic SLAM approaches across different sensing modalities and environments, (ii) a comparative review of datasets and evaluation metrics, and (iii) a reproducibility study of selected methods. To our knowledge, this is the first survey that integrates methods, datasets, evaluation practices, and application insights into a single comprehensive review, thereby offering a unified reference for researchers and practitioners. In conclusion, this review underscores the vital role of semantic SLAM in driving advancements in autonomous systems and intelligent navigation by analyzing recent developments, validating findings, and highlighting future research directions.},
  keywords = {Autonomous systems,Dynamic environments,Object-level SLAM,Semantic mapping,Semantic SLAM,Visual SLAM},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JYLEZSMP/Kanso et al. - 2025 - Semantic SLAM A comprehensive survey of methods and applications.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/2C2DQTME/S2667305325001176.html}
}

@article{miaoFrontierReviewSemantic2025,
  title = {A {{Frontier Review}} of {{Semantic SLAM Technologies Applied}} to the {{Open World}}},
  author = {Miao, Le and Liu, Wen and Deng, Zhongliang and Miao, Le and Liu, Wen and Deng, Zhongliang},
  date = {2025-08-12},
  journaltitle = {Sensors},
  volume = {25},
  number = {16},
  publisher = {publisher},
  issn = {1424-8220},
  doi = {10.3390/s25164994},
  url = {https://www.mdpi.com/1424-8220/25/16/4994},
  urldate = {2025-12-11},
  abstract = {With the growing demand for autonomous robotic operations in complex and unstructured environments, traditional semantic SLAM systems—which rely on cl...},
  langid = {english},
  keywords = {multimodal sensor fusion,open-world perception,robotic perception,semantic SLAM,zero-shot learning},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/NBHH6UZS/Miao et al. - 2025 - A Frontier Review of Semantic SLAM Technologies Applied to the Open World.pdf}
}

@online{qiuLearningGeneralizableFeature2024,
  title = {Learning {{Generalizable Feature Fields}} for {{Mobile Manipulation}}},
  author = {Qiu, Ri-Zhao and Hu, Yafei and Song, Yuchen and Yang, Ge and Fu, Yang and Ye, Jianglong and Mu, Jiteng and Yang, Ruihan and Atanasov, Nikolay and Scherer, Sebastian and Wang, Xiaolong},
  date = {2024-11-26},
  eprint = {2403.07563},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.07563},
  url = {http://arxiv.org/abs/2403.07563},
  urldate = {2025-12-11},
  abstract = {An open problem in mobile manipulation is how to represent objects and scenes in a unified manner so that robots can use both for navigation and manipulation. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherent at an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We quantitatively evaluate GeFF's ability for open-vocabulary object-/part-level manipulation and show that GeFF outperforms point-based baselines in runtime and storage-accuracy trade-offs, with qualitative examples of semantics-aware navigation and articulated object manipulation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Preprint. Project website is at: https://geff-b1.github.io/},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/ZQ7PJHJN/Qiu et al. - 2024 - Learning Generalizable Feature Fields for Mobile Manipulation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/BFJCQT64/2403.html}
}

@online{RoboticExplorationUsing,
  title = {Robotic {{Exploration Using Generalized Behavioral Entropy}}},
  url = {https://ieeexplore-1ieee-1org-100033czj02b7.han.technikum-wien.at/document/10608408},
  urldate = {2025-12-11},
  abstract = {This letter presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception. To do this, we introduce a measure of uncertainty that we term “Behavioral entropy”, which builds on Prelec's probability weighting from Behavioral Economics. We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon's and Renyi's. In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here. Then we use Behavioral entropy to define a new type of utility function that can guide a frontier-based environment exploration process. The approach's benefits are illustrated and compared in a Proof-of-Concept and ROS-Unity simulation environment with a Clearpath Warthog robot. We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies.},
  langid = {american},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/MTWWXC4V/10608408.html}
}

@inproceedings{ruanTaxonomySemanticInformation2022,
  title = {A {{Taxonomy}} of {{Semantic Information}} in {{Robot-Assisted Disaster Response}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Safety}}, {{Security}}, and {{Rescue Robotics}} ({{SSRR}})},
  author = {Ruan, Tianshu and Wang, Hao and Stolkin, Rustam and Chiou, Manolis},
  date = {2022-11-08},
  eprint = {2210.00125},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {285--292},
  doi = {10.1109/SSRR56537.2022.10018727},
  url = {http://arxiv.org/abs/2210.00125},
  urldate = {2025-12-11},
  abstract = {This paper proposes a taxonomy of semantic information in robot-assisted disaster response. Robots are increasingly being used in hazardous environment industries and emergency response teams to perform various tasks. Operational decision-making in such applications requires a complex semantic understanding of environments that are remote from the human operator. Low-level sensory data from the robot is transformed into perception and informative cognition. Currently, such cognition is predominantly performed by a human expert, who monitors remote sensor data such as robot video feeds. This engenders a need for AI-generated semantic understanding capabilities on the robot itself. Current work on semantics and AI lies towards the relatively academic end of the research spectrum, hence relatively removed from the practical realities of first responder teams. We aim for this paper to be a step towards bridging this divide. We first review common robot tasks in disaster response and the types of information such robots must collect. We then organize the types of semantic features and understanding that may be useful in disaster operations into a taxonomy of semantic information. We also briefly review the current state-of-the-art semantic understanding techniques. We highlight potential synergies, but we also identify gaps that need to be bridged to apply these ideas. We aim to stimulate the research that is needed to adapt, robustify, and implement state-of-the-art AI semantics methods in the challenging conditions of disasters and first responder scenarios.},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/BF73MJ3E/Ruan et al. - 2022 - A Taxonomy of Semantic Information in Robot-Assisted Disaster Response.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/JBWAHDQ4/2210.html}
}

@online{topiwalaFrontierBasedExploration2018,
  title = {Frontier {{Based Exploration}} for {{Autonomous Robot}}},
  author = {Topiwala, Anirudh and Inani, Pranav and Kathpal, Abhishek},
  date = {2018-06-10},
  eprint = {1806.03581},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.03581},
  url = {http://arxiv.org/abs/1806.03581},
  urldate = {2025-12-11},
  abstract = {Exploration is process of selecting target points that yield the biggest contribution to a specific gain function at an initially unknown environment. Frontier-based exploration is the most common approach to exploration, wherein frontiers are regions on the boundary between open space and unexplored space. By moving to a new frontier, we can keep building the map of the environment, until there are no new frontiers left to detect. In this paper, an autonomous frontier-based exploration strategy, namely Wavefront Frontier Detector (WFD) is described and implemented on Gazebo Simulation Environment as well as on hardware platform, i.e. Kobuki TurtleBot using Robot Operating System (ROS). The advantage of this algorithm is that the robot can explore large open spaces as well as small cluttered spaces. Further, the map generated from this technique is compared and validated with the map generated using turtlebot\_teleop ROS Package.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/8AUGCNII/Topiwala et al. - 2018 - Frontier Based Exploration for Autonomous Robot.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/WIHJKPVN/1806.html}
}

@online{yamazakiOpenFusionRealtimeOpenVocabulary2023,
  title = {Open-{{Fusion}}: {{Real-time Open-Vocabulary 3D Mapping}} and {{Queryable Scene Representation}}},
  shorttitle = {Open-{{Fusion}}},
  author = {Yamazaki, Kashu and Hanyu, Taisei and Vo, Khoa and Pham, Thang and Tran, Minh and Doretto, Gianfranco and Nguyen, Anh and Le, Ngan},
  date = {2023-10-05},
  eprint = {2310.03923},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.03923},
  url = {http://arxiv.org/abs/2310.03923},
  urldate = {2025-12-11},
  abstract = {Precise 3D environmental mapping is pivotal in robotics. Existing methods often rely on predefined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, a groundbreaking approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pre-trained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D segmentation for open-vocabulary without necessitating additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: https://uark-aicv.github.io/OpenFusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/TXJ928CY/Yamazaki et al. - 2023 - Open-Fusion Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/5UNB552N/2310.html}
}

@online{yokoyamaVLFMVisionLanguageFrontier2023,
  title = {{{VLFM}}: {{Vision-Language Frontier Maps}} for {{Zero-Shot Semantic Navigation}}},
  shorttitle = {{{VLFM}}},
  author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv and Wang, Jiuguang and Bucher, Bernadette},
  date = {2023},
  doi = {10.48550/ARXIV.2312.03275},
  url = {https://arxiv.org/abs/2312.03275},
  urldate = {2025-12-11},
  abstract = {Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Robotics (cs.RO)}
}

@article{zhanSemanticExplorationDense2025,
  title = {Semantic {{Exploration}} and {{Dense Mapping}} of {{Complex Environments}} Using {{Ground Robot}} with {{Panoramic LiDAR-Camera Fusion}}},
  author = {Zhan, Xiaoyang and Zhou, Shixin and Yang, Qianqian and Zhao, Yixuan and Liu, Hao and Ramineni, Srinivas Chowdary and Shimada, Kenji},
  date = {2025-11},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {10},
  number = {11},
  eprint = {2505.22880},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {11196--11203},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2025.3609216},
  url = {http://arxiv.org/abs/2505.22880},
  urldate = {2025-12-11},
  abstract = {This paper presents a system for autonomous semantic exploration and dense semantic target mapping of a complex unknown environment using a ground robot equipped with a LiDAR-panoramic camera suite. Existing approaches often struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal. To fill this gap, we propose a complete system combining mapping and planning. We first redefine the task as completing both geometric coverage and semantic viewpoint observation. We then manage semantic and geometric viewpoints separately and propose a novel Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This enables explicit multi-view semantic inspection and voxel coverage without unnecessary repetition. Building on this, we develop a hierarchical planner to ensure efficient global coverage. In addition, we propose a Safe Aggressive Exploration State Machine, which allows aggressive exploration behavior while ensuring the robot's safety. Our system includes a plug-and-play semantic target mapping module that integrates seamlessly with state-of-the-art SLAM algorithms for pointcloud-level dense semantic target mapping. We validate our approach through extensive experiments in both realistic simulations and complex real-world environments. Simulation results show that our planner achieves faster exploration and shorter travel distances while guaranteeing a specified number of multi-view inspections. Real-world experiments further confirm the system's effectiveness in achieving accurate dense semantic object mapping of unstructured environments.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  note = {Comment: Accepted by IEEE Robotics and Automation Letters},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/869W3E5V/Zhan et al. - 2025 - Semantic Exploration and Dense Mapping of Complex Environments using Ground Robot with Panoramic LiD.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/V5CCNKQI/2505.html}
}

@online{zouSegmentEverythingEverywhere2023,
  title = {Segment {{Everything Everywhere All}} at {{Once}}},
  author = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  date = {2023-07-11},
  eprint = {2304.06718},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.06718},
  url = {http://arxiv.org/abs/2304.06718},
  urldate = {2025-12-11},
  abstract = {In this work, we present SEEM, a promptable and interactive model for segmenting everything everywhere all at once in an image, as shown in Fig.1. In SEEM, we propose a novel decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal segmentation interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata: i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks; iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from decoder to image features; and iv) Semantic-awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. Notably, our single SEEM model achieves competitive performance across interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity for generalization to novel prompts or their combinations, rendering it a readily universal image segmentation interface.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevin/snap/zotero-snap/common/Zotero/storage/JK3PPLKG/Zou et al. - 2023 - Segment Everything Everywhere All at Once.pdf;/home/kevin/snap/zotero-snap/common/Zotero/storage/ABMEYVHQ/2304.html}
}
