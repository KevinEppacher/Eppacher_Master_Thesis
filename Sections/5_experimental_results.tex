% chktex-file 44

\chapter{Results and Discussion}
\label{ch:results}

This chapter answers the research questions defined in Chapter~\ref{ch:introduction} by
evaluating the proposed multi-object, open-vocabulary exploration framework in a sequence of
experiments. Experiment~1 addresses \textbf{RQ1} through a benchmark-style comparison on HM3Dv2.
Experiments~2--4 provide ablations that isolate key design choices of the system:
(i) the exploration-memory weighting (\textbf{RQ2}),
(ii) the semantic memory granularity via OpenFusion retrieval depth (\textbf{RQ3}),
and (iii) multi-source semantic fusion for detection robustness (\textbf{RQ4}).
Finally, Experiment~5 evaluates computational footprint and real-world feasibility (\textbf{RQ5})
under deployment conditions.

% ###########################################################
% # Experiment 1
% ###########################################################
\section{Experiment 1: Benchmarking on Matterport Scenes}
\label{sec:experiments:results:1}

This experiment evaluates the navigation success and efficiency of the proposed framework on
HM3Dv2~\cite{ramakrishnanHabitatMatterport3DDataset2021} and compares it to representative state-of-the-art methods. Table~\ref{tab:hm3dv2_comparison} summarizes \ac{SR} and \ac{SPL} for prior work and for \ac{SAGE}. Note that the experimental setup is not identical to all baselines, as the evaluation is performed in Isaac Sim with the pipeline described in Chapter~\ref{ch:implementation}, whereas several prior results are reported under Habitat-based evaluation protocols. Therefore, the comparison should be interpreted as an indicative reference rather than a strictly controlled re-evaluation.

Within this evaluation setting, \ac{SAGE} achieves a mean \ac{SPL} of 63.6\% and a mean \ac{SR} of 77.5\%. Compared to the next best reported \ac{SPL} in Table~\ref{tab:hm3dv2_comparison}, this corresponds to an increase of 26.2 percentage points in \ac{SPL}, while maintaining competitive success rates.

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{lcccc}
        \toprule
        Approach
        & \multicolumn{2}{c}{\textsc{Training}}
        & \multicolumn{2}{c}{\textsc{HM3Dv2}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & Locom. & Sem.
        & SPL $\uparrow$ & SR $\uparrow$ \\
        \midrule
        PIRLNav~\cite{ramrakhyaPIRLNavPretrainingImitation2023}
            & \cmark & \cmark & 27.1 & 64.1 \\
        ZSON~\cite{majumdarZSONZeroShotObjectGoal2023}
            & \cmark & \cmark & 12.6 & 25.5 \\
        \midrule
        ESC~\cite{zhouESCExplorationSoft2023}
            & \xmark & \xmark & 22.3 & 39.2 \\
        VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}
            & \cmark & \xmark & 30.4 & 52.5 \\
        OneMap~\cite{buschOneMapFind2025}
            & \xmark & \xmark & \underline{37.4} & 55.8 \\
        PIGEON-ZeroShot~\cite{pengPIGEONVLMDrivenObject2025}
            & \cmark & \cmark & 36.8 & \textbf{79.2} \\
        \midrule
        \textbf{SAGE (this work)}
            & \xmark & \xmark
            & \textbf{63.6}
            & \underline{77.5} \\
        \bottomrule
    \end{tabular}
    \caption{Quantitative comparison of navigation performance on HM3Dv2.}
    \label{tab:hm3dv2_comparison}
\end{table}


These results establish a strong reference point for the subsequent experiments, in which individual components of the framework and key hyperparameters are analyzed in isolation. All following ablation studies therefore build upon the configuration evaluated in this benchmark and aim to explain which design choices contribute most to the observed performance gains.

Figure~\ref{fig:results:rq1:example:bed_path} illustrates the evaluation setup for a representative episode by visualizing the executed trajectory and the corresponding geodesic shortest path used in the \ac{SPL} computation. The associated detection overlay and further qualitative examples for additional prompts are provided in Appendix~\ref{ch:appendix:experimental_results}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ1/00848-ziup5kvtCCR/bed_paths.png}
    \caption{Example trajectory visualization for Scene \texttt{00848-ziup5kvtCCR} and target class \texttt{bed}. The plot compares the executed path with the geodesic shortest path used to compute \ac{SPL}. Start, end, and target positions are indicated; the corresponding detection overlay is provided in Appendix~\ref{ch:appendix:experimental_results}.}
    \label{fig:results:rq1:example:bed_path}
\end{figure}


% ###########################################################
% # Experiment 2
% ###########################################################
\section{Experiment 2: Impact of Exploration-Memory Weighting}
\label{sec:experiments:results:2}

Experiment~2 addresses \textbf{RQ2} by analyzing how the weighting between live exploration signals
and accumulated semantic memory affects navigation performance and failure characteristics.
Let $\lambda_{\mathrm{exploration}} \in [0,1]$ denote the fusion weight that increases reliance on
reactive exploration cues (frontier-based semantic scoring). Figures~\ref{fig:results:rq2:sr} and~\ref{fig:results:rq2:spl}
report \ac{SR} and \ac{SPL} as a function of $\lambda_{\mathrm{exploration}}$.

The results indicate that extreme configurations are suboptimal. For small
$\lambda_{\mathrm{exploration}}$ (memory-dominant behavior), performance becomes sensitive to noisy or
stale semantic hypotheses in the map. For large $\lambda_{\mathrm{exploration}}$ (exploration-dominant
behavior), the agent is more reactive but tends to incur redundant motion and observations because
previously acquired knowledge is underutilized. In the evaluated scenes, intermediate weights yield
the most stable trade-off between reactivity and exploitation of accumulated information.

\begin{figure}[h!]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ2/sr.png}
        \subcaption{\ac{SR}}
        \label{fig:results:rq2:sr}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ2/spl.png}
        \subcaption{\ac{SPL}}
        \label{fig:results:rq2:spl}
    \end{minipage}

    \caption{Navigation performance as a function of the exploration-memory weighting in Experiment~2.
    SR captures task success, while SPL reflects navigation efficiency conditioned on success.}
    \label{fig:results:rq2:performance}
\end{figure}

To contextualize these trends, Figures~\ref{fig:results:rq2:failure_mode_breakdown_across_scenes}
and~\ref{fig:results:rq2:failure_mode_comparison} summarize failure-mode distributions across weight
configurations. Across all settings, misdetections remain the dominant failure class, primarily driven
by premature commitment to visually or semantically similar distractors (\texttt{Stopped at wrong object}).
Failure cases in which the target is not observed (\texttt{Did not see goal}) are attributable to incomplete coverage or occlusion, whereas \texttt{Ignored target} cases are consistent with conservative decision thresholding under ambiguity.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Images/05_experimental_results/RQ2/failure_mode_breakdown.png}
    \caption{Aggregate failure mode distribution across all exploration-memory weight configurations for Experiment~2.}
    \label{fig:results:rq2:failure_mode_breakdown_across_scenes}
\end{figure}

Figures~\ref{fig:results:rq2:exp_60_failure_mode} and~\ref{fig:results:rq2:exp_40_failure_mode} compare
two representative operating points, $\lambda_{\mathrm{exploration}}=0.6$ and $\lambda_{\mathrm{exploration}}=0.4$. At $\lambda_{\mathrm{exploration}}=0.6$, the system relies more strongly on reactive exploration cues, which reduces over-commitment to the semantic memory but may still produce misdetections when semantically similar distractors are encountered.
At $\lambda_{\mathrm{exploration}}=0.4$, the policy remains reactive while placing relatively more trust in memory cues. This setting benefits from accumulated evidence when frontier scores are uncertain, while still allowing new observations to correct spurious hypotheses. Empirically, this balance reduces premature commitment to incorrect targets.

\begin{figure}[h!]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ2/exp_60_failure_mode_breakdown.png}
        \subcaption{Exploration weight $\lambda_{\mathrm{exploration}} = 0.6$}
        \label{fig:results:rq2:exp_60_failure_mode}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ2/exp_40_failure_mode_breakdown.png}
        \subcaption{Exploration weight $\lambda_{\mathrm{exploration}} = 0.4$}
        \label{fig:results:rq2:exp_40_failure_mode}
    \end{minipage}

    \caption{Failure mode breakdowns for two exploration-memory weighting configurations in Experiment~2.
    Intermediate exploration weights reduce premature commitment to noisy semantic hypotheses by combining
    persistent memory cues with corrective exploratory evidence.}
    \label{fig:results:rq2:failure_mode_comparison}
\end{figure}

At the extremes, pure memory-driven behavior ($\lambda_{\mathrm{exploration}}=0$) and pure exploration-driven behavior ($\lambda_{\mathrm{exploration}}=1$) exhibit similarly low \ac{SPL}, indicating inefficient navigation in both cases. Without sufficient exploration, memory-dominant behavior can over-commit to false positives stored in the map and may fail to recover when the hypothesis is wrong. Conversely, exploration-dominant behavior adapts to new evidence but does not capitalize on previously acquired information, increasing redundant search. The slightly higher \ac{SR} observed for exploration-dominant settings suggests that adaptability to new observations is preferable to rigid reliance on semantic memory when semantic hypotheses are noisy.

Overall, these results indicate that the interaction between exploration and memory is non-linear: semantic memory supports efficient long-horizon search, but excessive reliance on it increases susceptibility to uncorrected false positives and incomplete evidence. Conversely, relying exclusively on exploration leads to unnecessarily long trajectories due to limited reuse of accumulated knowledge. The most robust performance is obtained for $\lambda_{\mathrm{exploration}} \in [0.6, 0.8]$. Consequently, $\lambda_{\mathrm{exploration}}=0.6$ is selected for all subsequent experiments.

\section{Experiment 3: Sensitivity to Semantic Map Granularity}
\label{sec:experiments:results:3}

Experiment~3 addresses \textbf{RQ3} by evaluating robustness to the granularity of semantic retrieval used to construct the OpenFusion-based semantic memory. The system uses OpenFusion to fuse \ac{VLM} embeddings into a volumetric representation. Granularity is controlled via the retrieval depth parameter \textit{top-k}, which retrieves the $k$ most similar semantic entries from the \ac{SEEM} embedding dictionary inside OpenFusion prior to fusion. Smaller \textit{top-k} values yield sharper but potentially incomplete semantic hypotheses, whereas larger \textit{top-k} values increase map density at the cost of higher noise and semantic ambiguity.

To compensate for noise under larger \textit{top-k}, $\lambda_{\mathrm{exploration}}$ is increased across
configurations, shifting trust from memory-based cues toward frontier-driven exploration. The
parameter \textit{top-k} is varied from 5 to 25 in increments of 5, and performance is evaluated using
\ac{SR} and \ac{SPL} with fixed starting poses to isolate semantic granularity effects.

Figure~\ref{fig:results:rq3:map_granularity_example} illustrates the qualitative effect of different top-$k$ values on the resulting semantic map. As expected, lower top-$k$ values yield sparser but cleaner semantic representations, while higher
top-$k$ values produce denser maps that contain more semantic cues but also increased noise. In particular, higher top-$k$ retrievals introduce additional false positives, as semantically similar but incorrect objects may be assigned non-negligible relevance scores. This effect is exemplified by the prompt \texttt{"couch"}, for which unrelated objects such as a stove or table are incorrectly highlighted at higher granularity levels.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ3/map_granularity_example.png}
    \caption{Qualitative effect of semantic map granularity on the OpenFusion memory.
    Increasing the top-$k$ retrieval depth densifies the semantic map but also introduces
    additional noise and false positive associations.}
    \label{fig:results:rq3:map_granularity_example}
\end{figure}

Across all 60 multi-object episodes, comprising a total of 324 object-level queries, navigation performance remains relatively stable under substantial variations in semantic map granularity. Figures~\ref{fig:results:rq3:sr} and~\ref{fig:results:rq3:spl} show the \ac{SR} and \ac{SPL} as a function of top-$k$ for two exploration-memory configurations: a balanced strategy with 60\% exploration and a memory-dominant strategy with 100\% exploitation. 

Consistent with the findings of Experiment~2, the balanced configuration (60\% exploration) consistently outperforms pure exploitation across all top-$k$ values for both \ac{SR} and \ac{SPL}. While the absolute best performance is achieved at a top-$k$ value of 15, the overall performance differences across granularity levels remain modest, indicating that the proposed framework is robust to variations in semantic map density and noise.

Notably, the \ac{IQR} for pure exploitation is substantially larger than for the balanced configuration. This increased variance suggests that exclusive reliance on semantic memory amplifies sensitivity to noisy semantic cues introduced at higher top-$k$ values, whereas incorporating exploratory evidence stabilizes performance.

\begin{figure}[h!]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ3/sr.png}
        \subcaption{\ac{SR} as a function of semantic map granularity}
        \label{fig:results:rq3:sr}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ3/spl.png}
        \subcaption{\ac{SPL} as a function of semantic map granularity}
        \label{fig:results:rq3:spl}
    \end{minipage}

    \caption{Impact of semantic map retrieval granularity (top-$k$) on navigation
    performance in Experiment~3. Results are shown for a balanced exploration-memory
    configuration (60\% exploration) and a memory-dominant configuration (100\%
    exploitation).}
    \label{fig:results:rq3:performance}
\end{figure}

To further analyze the effect of semantic map granularity on system robustness, Figure~\ref{fig:results:rq3:failure_mode_comparison} compares the failure mode distributions for the balanced and pure exploitation configurations. Overall failure rates are substantially lower for the balanced configuration (18.0\%) than for pure
exploitation (34.2\%).

Interestingly, the dominant failure mode for both configurations is \texttt{Stopped at wrong object}, with comparable rates of 8.0\% and 8.1\% for balanced and pure exploitation, respectively. This consistency indicates that the detection pipeline itself behaves similarly across configurations and that misdetections are not directly caused by the exploration-memory weighting.

In contrast, pure exploitation exhibits markedly higher rates of \texttt{Did not see goal} (12.8\%) and \texttt{Ignored target} (6.0\%) failures compared to the balanced configuration (2.0\% and 7.3\%, respectively). These failures are primarily associated with insufficient environment coverage and conservative target selection in the absence of corrective exploratory behavior.

Furthermore, navigation failures occur substantially more often under pure exploitation (7.4\%) than under the balanced configuration (0.7\%). Navigation failures are defined as situations in which the robot becomes trapped between obstacles, no valid navigation plan can be generated, or the selected plan leads into a dead-end that moves the robot farther away from the target. 

The pronounced reduction in navigation failures under the balanced configuration is attributed to the exploration component of the proposed framework, which incorporates costmap-based information to penalize frontiers located near obstacles and to bias
navigation toward safer, more traversable regions. In contrast to purely similarity-driven approaches such as \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, the proposed method explicitly integrates geometric and navigational constraints into
frontier scoring, thereby improving path feasibility and overall navigation robustness.

\begin{figure}[h!]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ3/exp_60_failure_mode_breakdown.png}
        \subcaption{Balanced configuration (60\% exploration)}
        \label{fig:results:rq3:exp_60_failure_mode}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ3/exp_0_failure_mode_breakdown.png}
        \subcaption{Pure exploitation (100\% exploitation)}
        \label{fig:results:rq3:exp_0_failure_mode}
    \end{minipage}

    \caption{Failure mode breakdowns for Experiment~3 under different
    exploration-memory weighting strategies. Increased reliance on semantic memory
    amplifies observation- and navigation-related failures, whereas balanced
    exploration mitigates premature commitment to noisy semantic hypotheses.}
    \label{fig:results:rq3:failure_mode_comparison}
\end{figure}

The best-performing configurations in Experiment~3 are consistent with the results of Experiment~2. In both experiments, the highest \ac{SR} and \ac{SPL} are achieved for top-$k$ values in the range of 10 to 15. This alignment is expected, as Experiment~2 was conducted using a top-$k$ value of 10, which was selected empirically during preliminary testing and used as a reasonable initial configuration for subsequent experiments.

% Overall, these results demonstrate that the proposed framework is robust to substantial variations in semantic map granularity. While semantic memory density influences noise levels, balanced integration of exploration and memory prevents degradation in
% navigation performance. Excessive reliance on memory alone leads to higher failure rates due to uncorrected false positives and incomplete environment coverage. In contrast, maintaining exploratory feedback enables the system to compensate for noisy
% semantic cues, confirming the importance of hybrid exploration-memory strategies for long-horizon semantic navigation.

Overall, the results show that coarse semantic retrieval can degrade performance when the policy
over-trusts the memory, but that balanced exploration can mitigate these effects by re-acquiring
evidence through additional viewpoints and observations.

\section{Experiment 4: Effect of Multi-Source Semantic Fusion}
\label{sec:experiments:results:4}

A key component of the proposed framework is the multi-source fusion strategy introduced in Chapter~\ref{ch:methods} Section~\ref{sec:methods:fusion_strategy}. Detecting target objects reliably in cluttered, real-world environments is inherently challenging due to occlusions, varying lighting conditions, and visual ambiguities. Relying on a single detection source increases susceptibility to false positives and negatives, which can mislead navigation decisions. To mitigate this, the proposed method fuses detection cues from multiple sources: (a) live detections from an open-vocabulary detector (YOLO-E~\cite{wangYOLOERealTimeSeeing2025}), (b) semantic similarity score from the \ac{VLM}~\cite{liBLIP2BootstrappingLanguageImage2023}, and (c) accumulated temporal evidence from the semantic memory (OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}). This experiment evaluates the effectiveness of this multi-source fusion strategy compared to single-source detection and pairwise combinations, with a Noisy-Or fusion approach as described in Chapter~\ref{ch:methods} Section~\ref{sec:methods:fusion_strategy:multi_source_detection_fusion}. Experiment~\ref{sec:experiments:results:4} isolates the perception and decision layer by evaluating how multi-source semantic fusion affects detection robustness, false positives, and threshold sensitivity, independent of exploration strategy.

Table~\ref{tab:results:rq4:detection_performance} reports detection performance across different fusion strategies and decision thresholds. Results are shown for three operating points corresponding to increasing levels of conservativeness: a low
threshold ($\tau = 0.5$), a medium threshold ($\tau = 0.6$), and a high threshold ($\tau = 0.8$). For each threshold, four fusion variants are compared: single-source detection using YOLO-E, detection combined with \ac{VLM}-based semantic similarity, detection combined with semantic memory evidence, and the proposed multi-source fusion integrating all three cues via a Noisy-Or formulation.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l | c | c c c c | c c c c}
        \toprule
        Fusion Variant & $\tau$ &
        $P\,\uparrow$ &
        $R\,\uparrow$ &
        $F1\,\uparrow$ &
        $FPR\,\downarrow$ &
        TP & FP & FN & TN \\
        \midrule
        % ===== tau = 0.5 =====
        Single source detection
            & 0.5 & \textbf{0.910} & 0.909 & 0.910 & \textbf{0.364} & 528 & 52 & 53 & 91 \\
        Detection + VLM score
            & 0.5 & 0.903 & \textbf{0.928} & \textbf{0.915} & 0.406 & 539 & 58 & 42 & 85 \\
        Detection + Memory
            & 0.5 & \underline{0.909} & 0.914 & \underline{0.912} & \underline{0.371} & 531 & 53 & 50 & 90 \\
        Multi-source Fusion
            & 0.5 & 0.906 & \underline{0.917} & \underline{0.912} & 0.385 & 533 & 55 & 48 & 88 \\
        \midrule
        % ===== tau = 0.6 =====
        Single source detection
            & 0.6 & \textbf{0.914} & 0.892 & 0.902 & \textbf{0.343} & 518 & 49 & 63 & 94 \\
        Detection + VLM score
            & 0.6 & \underline{0.910} & \textbf{0.910} & \textbf{0.910} & \underline{0.364} & 529 & 52 & 52 & 91 \\
        Detection + Memory
            & 0.6 & \underline{0.910} & \underline{0.905} & \underline{0.908} & \underline{0.364} & 526 & 52 & 55 & 91 \\
        Multi-source Fusion
            & 0.6 & \underline{0.910} & \textbf{0.910} & \textbf{0.910} & \underline{0.364} & 529 & 52 & 52 & 91 \\
        \midrule
        % ===== tau = 0.8 =====
        Single source detection
            & 0.8 & \textbf{0.942} & 0.636 & 0.759 & \textbf{0.161} & 375 & 23 & 215 & 120 \\
        Detection + VLM score
            & 0.8 & 0.912 & \underline{0.876} & \underline{0.894} & 0.343 & 509 & 49 & 72 & 94 \\
        Detection + Memory
            & 0.8 & \underline{0.913} & 0.799 & 0.852 & \underline{0.308} & 464 & 44 & 117 & 99 \\
        Multi-source Fusion
            & 0.8 & \underline{0.915} & \textbf{0.890} & \textbf{0.902} & 0.343 & 525 & 49 & 65 & 94 \\
        \bottomrule
    \end{tabular}
    \caption{Detection performance metrics across fusion strategies and thresholds.
    Arrows indicate whether higher ($\uparrow$) or lower ($\downarrow$) values are better.
    For each threshold $\tau$, the best-performing value per metric is highlighted in bold and the second-best value is underlined.}
    \label{tab:results:rq4:detection_performance}
\end{table}

At the low threshold ($\tau = 0.5$), all variants achieve high recall (0.909--0.928),
indicating that most true target observations are accepted. Differences
between fusion strategies are relatively small in this regime. Single-source detection
achieves the highest precision and the lowest false-positive rate, while incorporating
semantic similarity improves recall at the cost of increased false positives. The
multi-source fusion yields a balanced trade-off, with slightly reduced precision compared
to detection-only but improved recall relative to the single-source baseline.

At the medium threshold ($\tau = 0.6$), performance across all fusion variants becomes
more balanced. Precision values cluster around $0.91$, and recall differences are
reduced. In this regime, both the detection+VLM variant and the full multi-source fusion
achieve the highest \ac{F1}-scores, indicating an effective balance between sensitivity
and specificity. The corresponding confusion-matrix counts show that fusion-based
approaches reduce false negatives compared to detection-only, while maintaining
comparable false-positive rates.

At the high threshold ($\tau = 0.8$), clear differences between fusion strategies emerge.
Single-source detection attains the highest precision ($0.942$) and the lowest
false-positive rate, but suffers from a substantial drop in recall ($0.636$), resulting
in a large number of false negatives. In contrast, all fusion-based variants
significantly improve recall under this conservative operating point. The proposed
multi-source fusion achieves the highest recall ($0.890$) and \ac{F1}-score ($0.902$),
while maintaining precision above $0.915$. This improvement corresponds to a pronounced
reduction in false negatives compared to detection-only, with only a moderate increase
in false positives.

Figure~\ref{fig:results:rq4:confusion_matrices} provides a qualitative comparison of
the detection behavior at a conservative operating point ($\tau = 0.8$) for the
proposed multi-source fusion strategy and the detection-only baseline.
The detection-only configuration exhibits a pronounced imbalance between precision
and recall: while most predicted positives are correct, a large number of true target
instances are rejected, resulting in a high false-negative count.
This behavior reflects the brittleness of relying on a single detection signal under
strict thresholding, where ambiguous or partially occluded targets are frequently
missed.

\begin{figure}[h!]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ4/cm_multi_source_fusion.png}
        \subcaption{Multi-source semantic fusion ($\tau = 0.8$)}
        \label{fig:results:rq4:cm_fusion}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ4/cm_detection_only.png}
        \subcaption{Detection-only baseline ($\tau = 0.8$)}
        \label{fig:results:rq4:cm_detection_only}
    \end{minipage}

    \caption{Confusion matrices comparing the proposed multi-source semantic fusion
    strategy with a detection-only baseline at a fixed decision threshold
    $\tau = 0.8$. While detection-only achieves high precision, it suffers from a large
    number of false negatives. In contrast, multi-source fusion substantially improves
    recall by reducing missed detections, while maintaining a comparable false-positive
    rate.}
    \label{fig:results:rq4:confusion_matrices}
\end{figure}

In contrast, the multi-source fusion strategy substantially reduces the number of false negatives by integrating complementary semantic cues from the \ac{VLM} similarity score and accumulated memory evidence. The additional contextual information allows the system to recover target instances that would otherwise fall below the detection confidence threshold. Importantly, this gain in recall is achieved without a disproportionate increase in false positives, indicating that the fusion mechanism does not merely relax the decision criterion but selectively reinforces consistent evidence across sources.

Figure~\ref{fig:results:rq4:pr_curve} illustrates the precision-recall characteristics
of detection-only and multi-source semantic fusion over a full sweep of the decision
threshold $\tau$. While both approaches achieve high precision at low recall levels,
the detection-only baseline exhibits a steeper precision drop as recall increases,
indicating reduced robustness when operating beyond conservative thresholds.
In contrast, the proposed multi-source fusion maintains consistently higher precision
over a broader recall range (0.6--0.9 Recall), demonstrating improved stability under varying confidence
requirements.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/05_experimental_results/RQ4/pr_curve.png}
    \caption{Precision-recall curves for detection-only and multi-source semantic fusion
    obtained by sweeping the decision threshold $\tau$ in Experiment~4. The multi-source
    fusion maintains higher precision over a wider recall range, indicating improved
    robustness compared to single-source detection.}
    \label{fig:results:rq4:pr_curve}
\end{figure}

This behavior reflects the complementary nature of the fused cues. When detection
confidence alone is insufficient, semantic similarity and accumulated memory evidence
reinforce plausible target hypotheses, allowing true positives to be recovered without
a disproportionate increase in false positives. As a result, the multi-source fusion
achieves a more favorable precision-recall trade-off, particularly in the mid- to
high-recall regime that is critical for reliable downstream decision-making.

In the deployed behavior tree, this trade-off is explicitly exploited through a
two-stage verification strategy. Candidate objects are initially detected using a
permissive threshold ($\tau = 0.6$) to trigger alignment and viewpoint refinement
toward the object. Final confirmation is then performed at a stricter threshold
($\tau = 0.8$), ensuring high-confidence approval before task completion.
The precision-recall characteristics observed in Figure~\ref{fig:results:rq4:pr_curve}
support this design choice, as multi-source fusion preserves recall at lower thresholds
while retaining high precision at conservative operating points.

Overall, the results demonstrate that while single-source detection performs well at
permissive thresholds, it becomes increasingly brittle as the decision threshold is
raised. Incorporating semantic similarity and memory evidence stabilizes detection
performance across thresholds, and the proposed multi-source fusion provides the most
robust trade-off between precision and recall, particularly in conservative regimes that
are critical for reliable downstream navigation decisions.

\section{Experiment 5: System Efficiency and Real-World Validation}
\label{sec:experiments:results:5}

This experiment addresses \textbf{RQ5} by evaluating the computational efficiency and real-time feasibility of the proposed system in a real-world deployment setting. Figures~\ref{fig:results:rq5:gpu_vram} and~\ref{fig:results:rq5:openfusion_max_voxel} (see Appendix~\ref{ch:appendix:results:details}) summarize the GPU memory consumption of the overall system and an example of a semantic Map for the given total voxel allocation, respectively.

The \textbf{SAGE} framework consists of three primary GPU-intensive components:
(a) the open-vocabulary detector YOLO-E~\cite{wangYOLOERealTimeSeeing2025},
(b) the \ac{VLM} BLIP-2~\cite{liBLIP2BootstrappingLanguageImage2023} for semantic similarity scoring, and
(c) the OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023} semantic memory module for persistent 3D fusion.
YOLO-E and BLIP-2 exhibit fixed GPU memory footprints of approximately 1200\,MB and 2280\,MB of \ac{VRAM}, respectively, and do not vary during runtime. In contrast to VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, which utilizes four different \ac{VLM} and detectors, which cumulatively require over 16\,GB of GPU memory,~\textbf{SAGE} only uses 3480\,MB for the same functionality, making it more suitable for consumer-grade hardware.

In contrast, the GPU memory consumption of OpenFusion depends on the number of allocated voxels in the volumetric map. In the evaluated configuration, a total of 191{,}941 voxels are allocated, resulting in a GPU memory usage of approximately 6122\,MB for the semantic memory. This configuration is sufficient to represent an apartment-scale environment, as illustrated in Figure~\ref{fig:results:rq5:openfusion_max_voxel}. Overall, the complete \textbf{SAGE} system requires approximately 9.6\,GB of GPU \ac{VRAM}, making it compatible with contemporary consumer-grade GPUs.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/05_experimental_results/RQ5/gpu_vram.png}
    \caption{GPU memory consumption of the \textbf{SAGE} system during runtime, decomposed by major components.}
    \label{fig:results:rq5:gpu_vram}
\end{figure}

Real-time performance is evaluated by measuring the effective processing rates of the system during deployment. Figure~\ref{fig:results:rq5:fps} reports the observed execution frequencies of the major processing loops. The architecture follows a deliberate multi-rate design comprising three distinct loops:
(a) a high-rate reactive exploration loop driven by the BLIP-2 ValueMap,
(b) a perception loop responsible for YOLO-E detections, and
(c) a low-rate persistent semantic mapping loop for OpenFusion-based fusion.

The reactive exploration loop operates at approximately 10\,Hz, enabling timely responses to newly observed semantic cues. The perception loop runs at approximately 3.5\,Hz, which is sufficient for stable object detection in indoor navigation scenarios. The persistent semantic mapping loop operates at a significantly lower frequency of approximately 0.36\,Hz, reflecting the high computational cost of volumetric fusion. This design choice ensures that computationally expensive operations do not impede system responsiveness.

Due to the comparatively high latency of OpenFusion, the semantic memory is queried only at the beginning of a new exploration step and during candidate object confirmation. Downstream processing stages, including clustering of semantic point clouds into graph nodes, are executed at a fixed rate of 10\,Hz, ensuring that higher-level decision-making components always have access to the most recent fused semantic information. The graph node fusion stage integrates signals from detection, semantic similarity for the value map, and memory at approximately 1.6\,Hz during exploration and 1.7\,Hz during detection, which was found to be sufficient for reliable navigation without excessive computational overhead.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/05_experimental_results/RQ5/fps.png}
    \caption{Observed execution frequencies of the main processing loops during real-world deployment.}
    \label{fig:results:rq5:fps}
\end{figure}

% \begin{table}[H]
% \centering
% \footnotesize
% \renewcommand{\arraystretch}{1.15}

% \begin{tabular}{
%     l 
%     l 
%     c 
%     c 
%     c
% }
% \hline
% \textbf{Component} & \textbf{ROS Topic} & \textbf{Mean [Hz]} & \textbf{Min--Max [Hz]} & \textbf{Std. Dev.} \\
% \hline

% \multicolumn{5}{l}{\textit{Reactive Perception}} \\[2pt]
% YOLO-E Detector 
% & \texttt{/yoloe/overlay} 
% & 6.2 
% & 2.9 -- 13.0 
% & 0.06 \\

% \multicolumn{5}{l}{\textit{Semantic Scoring}} \\[2pt]
% Value Map Similarity 
% & \texttt{/value\_map/cosine\_similarity} 
% & 10.0 
% & 9.7 -- 10.3 
% & 0.0015 \\

% \multicolumn{5}{l}{\textit{Graph-Based Reasoning}} \\[2pt]
% Exploration Graph 
% & \texttt{/exploration\_graph\_nodes/graph\_nodes} 
% & 6.3 
% & 4.9 -- 7.4 
% & 0.02 \\

% Exploitation Graph 
% & \texttt{/exploitation\_graph\_nodes/graph\_nodes} 
% & 10.0 
% & 9.6 -- 10.6 
% & 0.002 \\

% Fused Exploration Graph 
% & \texttt{/fused/exploration\_graph\_nodes/graph\_nodes} 
% & 1.6 
% & 1.3 -- 2.0 
% & 0.04 \\

% Fused Detection Graph 
% & \texttt{/fused/detection\_graph\_nodes/graph\_nodes} 
% & 1.7 
% & 1.5 -- 2.0 
% & 0.05 \\

% \multicolumn{5}{l}{\textit{Persistent 3D Semantic Mapping}} \\[2pt]
% OpenFusion Query Cloud 
% & \texttt{/openfusion\_ros/query\_pointcloud\_xyzi} 
% & 0.32 
% & 0.19 -- 0.36 
% & 1.09 \\

% \hline
% \end{tabular}

% \caption{Observed topic publication frequencies during real-world deployment. 
% The system follows a deliberate multi-rate design: reactive perception and exploitation 
% operate at high frequency, while persistent semantic fusion is performed sparsely to 
% limit computational overhead.}
% \label{tab:rq5_topic_frequencies}
% \end{table}

% 3346 MB -> 206915 Voxels
% 5368 MB -> 285685 Voxels

\newpage