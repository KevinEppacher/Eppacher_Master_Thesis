% chktex-file 44

\chapter{Methods}
\label{ch:methods}
This chapter presents the methodology of the proposed hybrid semantic exploration system, 
designed to mitigate the key limitations identified in Chapter~\ref{ch:sota}, namely the 
absence of persistent semantic memory, the reliance on single-source semantic detections, 
and the lack of an explicit and controllable trade-off between semantic exploration and 
exploitation in existing approaches.

The proposed system integrates zero-shot semantic perception with frontier-based exploration and persistent 3D semantic mapping into a unified, closed-loop decision-making framework. Semantic evidence acquired during exploration is continuously fused into a long-term spatial memory, while exploration behavior is adaptively modulated based on the reliability of accumulated semantic beliefs.

An overview of the system architecture is provided in Section~\ref{sec:methods:system_overview}, followed by detailed descriptions of the core components: semantic frontier exploration (Section~\ref{sec:methods:semantic_frontier}), promptable zero-shot detection (Section~\ref{sec:methods:zero_shot_detection}), persistent semantic 3D mapping (Section~\ref{sec:methods:persistent_mapping}), the multi-source fusion strategy governing semantic belief updates (Section~\ref{sec:methods:fusion_strategy}), and the behavior tree used to orchestrate semantic-guided exploration and navigation (Section~\ref{sec:methods:behavior_tree}).


\section{System Overview}
\label{sec:methods:system_overview}

The proposed system follows a modular hybrid architecture that tightly couples semantic perception,
geometry-driven exploration, and persistent semantic mapping to enable robust open-vocabulary
object-guided exploration. Figure~\ref{fig:system_overview} provides a high-level overview of the system components and their data flow. The exploration task is specified by a user-provided natural language prompt, which defines the semantic target and conditions the detection, exploration, and fusion modules throughout system execution.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/03_methods/sage_pipeline.png}
    \caption{Overview of the SAGE system architecture for open-vocabulary semantic exploration.
    RGB-D observations and robot poses are processed by three parallel modules: 
    (a) a persistent semantic mapping backend (OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}) that maintains a semantic 3D map, 
    (b) a semantic frontier exploration module that scores geometric frontiers based on language-guided relevance, and 
    (c) a promptable zero-shot detection pipeline for object hypotheses.
    All semantic hypotheses are represented as graph nodes, filtered by relevance maps, and fused using a multi-source fusion strategy.
    A behavior tree orchestrates exploration, verification, and navigation actions, while low-level motion planning and execution are handled by the ROS~2 Navigation Stack~\cite{macenskiDesksROSMaintainers2023}.}
\label{fig:system_overview}
\end{figure}

Robot observations at time $t$ are represented as RGB-D measurements $O_t = \{I_t, D_t\}$, where $I_t$ denotes the RGB image and $D_t$ the depth map. The robot pose in the world frame is denoted by $P_t$. To reduce computational load, observations are temporally and spatially downsampled prior to further processing.

The pre-processed observations are then fed into three parallel modules.
(a) The \textit{Memory Module} takes as input the current observations $O_t$ and pose $P_t$, obtained from the ROS~2 SLAM Toolbox~\cite{macenskiSLAMToolboxSLAM2021}, and updates the persistent semantic 3D map $M_t$.
(b) The \textit{Exploration Module} uses $O_t$ and $P_t$ to generate semantic frontiers $F_t$ from the current exploration occupancy grid.
(c) The \textit{Detection Module} processes $O_t$ to produce promptable zero-shot detections $D_t^{\text{det}}$.

Each module outputs a set of graph nodes representing semantic hypotheses.
Specifically, the memory module produces memory graph nodes $G_t^{\text{mem}}$ derived from the persistent map $M_t$, the exploration module outputs exploration graph nodes $G_t^{\text{exp}}$ corresponding to semantic frontiers $F_t$, and the detection module outputs detection graph nodes $G_t^{\text{det}}$ obtained from $D_t^{\text{det}}$.
This unified graph abstraction enables heterogeneous semantic hypotheses to be compared, filtered, and ranked using a common interface.

Prior to fusion, graph nodes are filtered using a relevance map to suppress nodes located in already explored areas. The remaining nodes are fused using the multi-source fusion strategy described in Section~\ref{sec:methods:fusion_strategy}, resulting in a unified set of weighted graph nodes $G_t^{\text{fused}}$.

Finally, the behavior tree described in Section~\ref{sec:methods:behavior_tree} selects the next high-level action based on $G_t^{\text{fused}}$, either navigating toward high-value frontiers for continued exploration or approaching detected objects for verification.
Low-level motion planning, obstacle avoidance, and execution are handled by the ROS~2 Navigation Stack~\cite{macenskiDesksROSMaintainers2023}.


\section{Semantic Frontier Exploration}
\label{sec:methods:semantic_frontier}

Semantic frontier exploration extends classical frontier-based exploration by incorporating
semantic relevance derived from vision-language models, enabling task-driven exploration guided by
a user-defined semantic prompt. Instead of exploring unknown space uniformly, the robot prioritizes frontiers that are more likely to yield observations relevant to the target concept~\cite{topiwalaFrontierBasedExploration2018, yokoyamaVLFMVisionLanguageFrontier2023 alamaRayFrontsOpenSetSemantic2025}. Figure~\ref{fig:sage_maps} illustrates the intermediate map representations and processing stages used to construct the semantic frontier map.

\subsection*{Exploration Occupancy Maps}

The system maintains three distinct 2D occupancy grids for navigation and exploration:
(a) a SLAM map used for localization and navigation~\cite{macenskiSLAMToolboxSLAM2021},
(b) an exploration map used exclusively for frontier detection, and
(c) an inflated map that suppresses narrow passages and noisy frontier artifacts.
This separation decouples stable navigation from task-specific exploration decisions and prevents
semantic exploration logic from modifying the navigation map.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/03_methods/semantic_frontier_exploration/sage_maps.png}
    \caption{Overview of the map representations used for semantic frontier exploration.
    The SLAM map is used for localization and navigation, while the exploration map encodes
    task-specific explored and unexplored regions for frontier detection~\cite{macenskiSLAMToolboxSLAM2021}.
    An inflated map is used to suppress narrow structures and reduce spurious frontiers.
    The resulting frontier map is combined with a semantic value map to prioritize exploration
    toward semantically relevant regions.}
    \label{fig:sage_maps}
\end{figure}

Rather than running a separate SLAM instance for exploration, the exploration map is derived
directly from the SLAM occupancy grid~\cite{macenskiSLAMToolboxSLAM2021}.
Given the robot pose $P_t$ and the SLAM map $M_{\text{SLAM}}$, free space is raycast from the robot
into the occupancy grid, marking traversed cells as explored while preserving unknown regions beyond
sensor reach.
This raycasting process is applied to all recorded robot poses accumulated during the current task,
yielding an exploration map $M_{\text{exp}}$ that reflects the explored workspace.

When a new semantic search task is initiated, all stored poses are cleared and the exploration map
is rebuilt from scratch, while the SLAM map remains unchanged.
This design ensures that exploration decisions are conditioned solely on task-relevant semantic
information and prevents bias from previously explored but semantically irrelevant regions.

\subsection*{Frontier Detection and Calculation}
\begin{itemize}
    \item Detection of frontiers on a 2D occupancy grid to identify candidate regions for exploration.
    \item Application of classical frontier-based exploration algorithms extended with semantic information.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/03_methods/frontier_example.png}
    \caption{Frontier detection on occupancy grid}
    \label{fig:frontier_detection}
\end{figure}

\textbf{Value Map Generation using Vision-Language Models}
\begin{itemize}
    \item Computation of value maps by evaluating cosine similarity between text queries and scene observations.
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=\textwidth]{Images/03_methods/blip2_cosine_sim.png}
        \caption{System architecture}
        \label{fig:system_overview}
    \end{figure}

    \begin{figure}[h!]
        \centering
        \includegraphics[width=\textwidth]{Images/03_methods/value_map_example.png}
        \caption{Value map example}
        \label{fig:value_map_example}
    \end{figure}

    \item Dynamic update of value maps as new observations are integrated.
\end{itemize}

\section{Persistent Semantic 3D Mapping} 
\label{sec:methods:persistent_mapping}

\subsection*{Global Map Construction with Open-Fusion}
\begin{itemize}
    \item Incremental creation of a global semantic point cloud map integrating RGB-D observations over time.
    \item Registration of observations using robot poses to maintain a consistent world representation.
    \item Association of semantic labels with 3D points based on query relevance scores.
\end{itemize}

\subsection*{Semantic Clustering and Graph Node Generation}
\begin{itemize}
    \item Clustering of points with similar semantic labels to form object-level hypotheses.
    \item Construction of semantic graph nodes representing detected object instances with aggregated confidence scores.
    \item Maintenance of the semantic graph as a persistent memory for multi-object search tasks.
\end{itemize}

\section{Promptable Zero-Shot Detection} 
\label{sec:methods:zero_shot_detection}
\begin{itemize}
    \item In this work YOLO-E \cite{yoloe} is used as the promptable zero-shot detection model.
    \item YOLO-E has the following advantages:
    \begin{itemize}
        \item High inference speed suitable for real-time applications.
        \item Ability to handle open-vocabulary object detection based on text prompts.
        \item Integration of both visual and textual information for robust detection.
        \item Pre-trained on large-scale datasets, enabling zero-shot generalization to unseen object categories.
    \end{itemize}
\end{itemize}


\subsection*{Open-Vocabulary Object Detection with YOLO-E}
\begin{itemize}
    \item Utilization of the YOLO-E model for open-vocabulary object detection based on text prompts.
    \item Extraction of 2D bounding boxes and associated confidence scores for detected objects.
    \item Segmentation of detected objects to isolate relevant pixels for 3D localization.
\end{itemize}

\subsection*{Depth-Based 3D Localization}
\begin{itemize}
    \item With camera intrinsics and depth information, the 2D bounding boxes ans segmentation masks are projected into 3D space.
    \item Calculation of 3D coordinates for each detected object using depth values within the bounding box.
    \item Semantic detection pointclouds are passed are then clustered and the centroid of each cluster is computed to obtain robust 3D object locations.
    \item For each cluster, the mean of the confidence scores of the associated 2D detections is calculated to assign a confidence score to the 3D localization.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{Images/03_methods/vlm_detector.png}
    \caption{YOLO-E detection to graph node 3D localization}
    \label{fig:vlm_detector}
\end{figure}


\section{Fusion Strategy}
\label{sec:methods:fusion_strategy}
\subsection*{Explorationâ€“Memory Weighting}
\begin{itemize}
    \item Exploration and memory graph nodes are fused and weighted as follows:
    \begin{itemize}
        \item Proximity weighting: Nodes closer to the robot's current position are given higher weights, similar to~\cite{bourgaultInformationBasedAdaptive2002}.
        \item Exploration vs Memory: Nodes from the exploration source are prioritized over memory nodes to encourage discovery of new information, similar to~\cite{ramakrishnanPONIPotentialFunctions2022}.
        \item Costmap weighting: Nodes located in areas with lower navigation costs are favored to optimize path planning and navigation efficiency, similar to~\cite{bourgaultInformationBasedAdaptive2002}.
    \end{itemize}
\end{itemize}

\subsection*{Multi-Source Detection Fusion}
\begin{itemize}
    \item Detection graph nodes are weighted based on:
    \begin{itemize}
        \item YOLO-E confidence scores: Higher confidence detections are given more weight.
        \item BLIP-2 value map: Detections with higher semantic relevance to the text prompt are prioritized.
        \item The nearer detection graph nodes are to memory graph nodes, the higher their weight.
    \end{itemize}
\end{itemize}

\subsection*{Relevance Filtering and Node Suppression}
\begin{itemize}
    \item Each source's graph nodes are filtered based on a relevance threshold to eliminate graph nodes within the fov map.
    \item Relevance map is build over time
    \item If a graph node is located in an area that has already been explored and found to be irrelevant to the prompt, it is suppressed.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{Images/03_methods/3d_relevance_map.png}
    \caption{Fusion strategy for exploration, detection, and memory graph nodes}
    \label{fig:3d_relevance_map}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{Images/03_methods/relevance_map_example.png}
    \caption{Fusion strategy for exploration, detection, and memory graph nodes}
    \label{fig:relevance_map_example}
\end{figure}

\section{Behavior Tree for Semantic-Guided Exploration} 
\label{sec:methods:behavior_tree}

\subsection*{High-Level Task Structure}
\begin{itemize}
    \item The behavior tree (BT) is designed to manage the high-level task structure for semantic-guided exploration.
    \item The BT consists of the following main components:
    \begin{itemize}
        \item Initialization: Clearing Maps, Publishing Prompts
        \item Detection Branch: If object is detected over a threshold, navigate to it, realign to object take picture
        \item Exploration Branch: While object not detected, perform semantic frontier exploration navigating to highest valued frontiers or memory nodes
        \item Termination: If object found, end mission; If time limit reached, end mission
        \item Behavior tree is called with a ros2 action server, which returns on termination success or failure, and actual path taken
    \end{itemize}
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/03_methods/sage_bt_flowchart.png}
    \caption{System architecture}
    \label{fig:system_overview}
\end{figure}

\subsection*{Integration with Navigation Stack}
\begin{itemize}
    \item Navigation stack used for low-level path planning and obstacle avoidance.
    \item Action used: navigate\_to\_pose, Spin
\end{itemize}
    
\newpage