% chktex-file 44

\chapter{Introduction}

The introduction of Transformer-based architectures~\cite{vaswaniAttentionAllYou2023} has opened new opportunities for integrating high-level semantic reasoning with low-level geometric navigation in robotics. Traditional robotic exploration methods have primarily focused on mapping unknown environments using geometric cues, often neglecting the rich semantic information available in visual and linguistic modalities~\cite{topiwalaFrontierBasedExploration2018}. However, recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled robots to interpret and act upon complex, open-ended instructions provided in natural language. These developments have catalyzed a shift toward semantic reasoning and zero-shot generalization, allowing robots to understand unseen concepts beyond fixed, task-specific datasets.

Consequently, new applications have emerged that require robots not only to explore and map their surroundings but also to reason about the semantic structure and relationships within them. In service robotics, for instance, a mobile agent may be instructed to locate a specific object based on high-level descriptions such as \textit{``find the red chair in the living room''}, rather than relying on a limited set of predefined categories such as those from COCO~\cite{linMicrosoftCOCOCommon2015}. Similarly, in search and rescue operations, robots may be tasked with locating missing persons based on vague or incomplete contextual information, such as the assumption that an individual might be found \textit{``in the bathroom''}. In industrial inspection, autonomous agents must identify structural anomalies or specific components within unstructured and partially observable environments, while in warehouse automation, robots must locate items or storage units that may not be consistently labeled or fully visible.

Across these domains, the integration of semantic understanding with autonomous navigation is becoming increasingly essential. Robots must be capable of interpreting abstract human instructions, reasoning about both spatial and semantic context, and conducting efficient searches in dynamic, partially known environments. These challenges motivate the development of a unified framework that bridges geometric exploration with semantic scene understanding, enabling autonomous agents to perform open-vocabulary, goal-directed exploration guided by high-level semantic input.

\section{Problem Statement}

Traditional geometric exploration techniques are widely used for mapping unknown environments by identifying frontiers in either two or three dimensions and navigating toward the frontier with the highest expected information gain. Such methods, including those based on occupancy grids or point cloud representations, are particularly effective for coverage and mapping tasks, and have been successfully extended to multi-robot systems for large-scale exploration. However, these approaches remain primarily geometry-driven and do not incorporate semantic understanding of the environment. Consequently, they are suboptimal for goal-directed exploration tasks, where the objective is to locate specific objects or regions of interest defined by high-level semantic criteria rather than unexplored geometry.

To address this limitation, recent research has focused on integrating semantic perception into exploration frameworks. Instead of relying solely on LiDAR or depth sensors for geometric mapping, the use of RGB imagery enables semantic reasoning about the scene and the objects contained within it. Table~\ref{tab:introduction:semantic_exploration_approaches} summarizes representative works that leverage either pretrained vision-language models or reinforcement learning-based policies to guide robots toward regions that are semantically relevant to a given target description.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\textwidth}{
        |>{\raggedright\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\raggedright\arraybackslash}X|}
        \hline
        \textbf{Approach} & \textbf{Training Required} & \textbf{Real-Time} & \textbf{Semantic Reasoning Model} \\ \hline
        \textbf{VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}} & \xmark~(zero-shot) & \cmark & BLIP-2 + GroundingDINO + SAM \\ \hline
        \textbf{SemUtil~\cite{chenHowNotTrain2023}} & \xmark~(training-free) & \cmark & Mask R-CNN + CLIP + BERT \\ \hline
        \textbf{ESC~\cite{zhouESCExplorationSoft2023}} & \xmark~(zero-shot) & \cmark & GLIP + DeBERTa / ChatGPT reasoning \\ \hline
        \textbf{LGX~\cite{dorbalaCanEmbodiedAgent2024}} & \xmark~(zero-shot) & \cmark & GPT-3 + GLIP + BLIP \\ \hline
        \textbf{CoW~\cite{gadreCoWsPastureBaselines2022}} & \xmark~(zero-shot) & \cmark & CLIP similarity scoring \\ \hline
        \textbf{ZSON~\cite{majumdarZSONZeroShotObjectGoal2023}} & \cmark~(RL pretraining) & \xmark & CLIP-based RL policy \\ \hline
        \textbf{PONI~\cite{ramakrishnanPONIPotentialFunctions2022}} & \cmark~(supervised) & \xmark & Learned potential-field network \\ \hline
        \textbf{PIRLNav~\cite{ramrakhyaPIRLNavPretrainingImitation2023}} & \cmark~(BC + RL) & \xmark & DINO-based CNN-RNN policy \\ \hline
    \end{tabularx}
    \caption{Overview of semantic zero-shot and trained exploration approaches.}
    \label{tab:introduction:semantic_exploration_approaches}
\end{table}

Approaches such as ZSON~\cite{majumdarZSONZeroShotObjectGoal2023}, PONI~\cite{ramakrishnanPONIPotentialFunctions2022}, and PIRLNav~\cite{ramrakhyaPIRLNavPretrainingImitation2023} employ deep reinforcement learning (DRL) or supervised training to develop navigation policies capable of generalizing to unseen objects. Although these methods achieve promising results in simulation, they require extensive offline training and exhibit limited adaptability to novel environments or object categories not encountered during training. In contrast, zero-shot methods such as VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, SemUtil~\cite{chenHowNotTrain2023}, ESC~\cite{zhouESCExplorationSoft2023}, LGX~\cite{dorbalaCanEmbodiedAgent2024}, and CoW~\cite{gadreCoWsPastureBaselines2022} leverage pretrained vision-language models to perform semantic exploration without additional training. These models allow real-time decision-making by exploiting semantic cues extracted from RGB imagery, guiding robots toward areas likely to contain the target object or region.

Some approaches, such as ESC~\cite{zhouESCExplorationSoft2023} and LGX~\cite{dorbalaCanEmbodiedAgent2024}, further incorporate large language models (LLMs) for commonsense reasoning and instruction understanding, enabling higher-level interpretation of complex tasks. However, this comes at the cost of increased computational demand and potential latency, which limits their applicability to resource-constrained mobile platforms. While VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023} achieves real-time operation, it relies on multiple heavy models (GroundingDINO, SAM, and BLIP-2) that require high-end GPUs with up to 16~GB of VRAM, making deployment on embedded systems challenging.

Overall, these methods primarily focus on short-term semantic reasoning without persistent memory. They lack mechanisms for long-term storage or recall of previously acquired semantic information, leading to redundant exploration and decreased efficiency in multi-object search tasks. A persistent global memory would enable robots to exploit past observations, recall detected objects, and avoid revisiting known regionsâ€”thereby improving navigation efficiency and scalability in open-vocabulary, real-world environments.


\textbf{Current solutions:}

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\textwidth}{
        |>{\raggedright\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\raggedright\arraybackslash}X
        |>{\raggedright\arraybackslash}X|}
        \hline
        \textbf{Approach} & \textbf{Training Required} & \textbf{Real-Time} & \textbf{Memory Representation} & \textbf{Exploration Integration} \\ \hline
        \textbf{OneMap} & \xmark~(zero-shot) & \cmark & 2D probabilistic feature field & \cmark~(frontier-based) \\ \hline
        \textbf{ConceptGraphs} & \xmark~(pretrained models) & \xmark & 3D scene graph & \cmark~(LLM-planner) \\ \hline
        \textbf{SemExp} & \cmark~(RL + supervised) & \xmark & 2D semantic occupancy map & \cmark~(learned policy) \\ \hline
        \textbf{GeFF} & \cmark~(ScanNet pretrain) & \cmark & Implicit 3D feature field & \xmark~(passive) \\ \hline
        \textbf{RayFronts} & \xmark~(foundation model) & \cmark & Hybrid voxel + ray field & \cmark~(semantic frontier) \\ \hline
        \textbf{VLMaps} & \xmark~(pretrained LSeg/CLIP) & \xmark & 2.5D open-vocab grid & \cmark~(frontier-compatible) \\ \hline
        \textbf{Pigeon} & \cmark~(RLVR fine-tune) & \cmark & Point-of-Interest snapshot memory & \cmark~(reasoning-aware) \\ \hline
    \end{tabularx}
    \caption{Overview of Persistent or Memory-Based Semantic Mapping Approaches.}
    \label{tab:introduction:persistent_mapping_approaches}
\end{table}


Despite notable progress, these methods often suffer from heavy GPU requirements, offline training procedures, or noisy semantic maps that reduce navigation reliability. Some further depend heavily on object detectors, which can introduce false positives in open-vocabulary settings.

\bigskip
\textbf{Core Gaps in Existing Work:}

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\textwidth}{
        |>{\raggedright\arraybackslash}X
        |>{\raggedright\arraybackslash}X
        |>{\raggedright\arraybackslash}X|}
        \hline
        \textbf{Limitation} & \textbf{Example Works} & \textbf{Implication} \\ \hline
        \textbf{No persistent memory} & VLFM, CoW, LGX, ESC & No long-term fusion or recall; repeated exploration of known areas. \\ \hline
        \textbf{Offline training required} & ZSON, PONI, PIRLNav, SemExp & Heavy RL/supervised training; poor adaptability to new scenes. \\ \hline
        \textbf{No balance between exploration and memory} & OneMap, RayFronts, VLMaps & Either passive mapping or short-term exploration; inefficient search. \\ \hline
        \textbf{No zero-shot exploration} & VLFM, CoW, LGX & Detect novel objects but fail to explore unseen regions strategically. \\ \hline
        \textbf{Premapping needed} & ConceptGraphs, VLMaps, GeFF & Depend on pre-recorded data; not suited for online autonomy. \\ \hline
        \textbf{Limited robustness} & PONI, SemExp, PIRLNav & Closed-set categories; fragile under real-world variation. \\ \hline
        \textbf{Low real-world applicability} & ConceptGraphs, VLMaps & High GPU cost or simulation-only; not deployable on mobile robots. \\ \hline
    \end{tabularx}
    \caption{Identified Gaps in Existing Semantic Exploration Frameworks.}
    \label{tab:introduction:identified_gaps}
\end{table}

\textbf{Derived Requirements from the State of the Art:}
\begin{itemize}
    \item Zero-shot frontier exploration framework.
    \item Persistent 3D semantic mapping with confidence-based fusion.
    \item Use of pretrained models only (no additional training required).
    \item Hybrid fusion strategy to balance exploration and memory.
    \item Multi-source detection fusion for robust object identification.
    \item GPU-efficient design for real-time deployment on mobile robots.
    \item Modular architecture for easy adaptation and extension.
    \item Independence from fixed, pre-trained object category sets.
\end{itemize}

\section{Scientific Contribution}

This work contributes to the state of the art by introducing a hybrid semantic exploration framework that integrates zero-shot semantic frontier scoring with persistent 3D scene representation, enabling autonomous search guided by open-vocabulary text queries. The system combines real-time semantic reasoning during exploration with a long-term spatial memory, allowing the robot to dynamically balance between discovering new information and exploiting previously acquired knowledge.

Unlike previous approaches that focus exclusively on either geometric frontiers or static semantic maps, the proposed framework continuously fuses information from multiple semantic sources to maintain a unified, confidence-based world model. Adaptive weighting enables the robot to adjust its behavior between exploration and exploitation according to the reliability of recent observations and the stability of stored semantic memory.

The framework further investigates how the quality and granularity of the underlying semantic information influence task success, navigation efficiency, and robustness. By systematically varying the trust between exploration and memory components, this work provides new insights into how semantic reasoning and persistent mapping can be effectively combined for open-vocabulary, multi-object search in dynamic environments.

To evaluate the contribution of the proposed system, the following research questions are formulated:

\begin{enumerate}
    \item \textbf{How does integrating zero-shot semantic exploration and persistent 3D semantic mapping affect multi-object search performance and navigation efficiency compared to existing methods?} \\
    \textit{Metrics:} Performance is quantified in terms of task success and path efficiency, measured through Success Rate (SR), Success per Path Length (SPL), and Multi-Object Success Rate (MSR) relative to representative state-of-the-art systems such as OneMap, VLFM, and Pigeon.
    
    \item \textbf{How does the interaction between live exploration and accumulated semantic memory influence overall system performance?} \\
    \textit{Metrics:} The weighting factor between exploration and memory is varied during graph node fusion to assess impacts on SR and SPL, identifying optimal trade-offs between reactivity and exploitation.
    
    \item \textbf{How does the granularity of semantic map retrieval affect map quality, and can dynamic weighting between exploration and memory compensate for potential noise?} \\
    \textit{Metrics:} The semantic granularity in the 3D semantic mapper is varied while adjusting exploration weight to evaluate effects on SR and SPL\@.
    
    \item \textbf{How does multi-source fusion of detection confidence, semantic similarity, and memory confidence impact detection robustness and false-positive suppression during exploration?} \\
    \textit{Metrics:} Precision, Recall, F1-Score, Confusion Matrix, and SR under different fusion weight configurations across COCO, open-vocabulary, and zero-shot classes.
    
    \item \textbf{What is the computational footprint and real-world robustness of the hybrid framework?} \\
    \textit{Metrics:} Frames per second (FPS), GPU/CPU usage, inference latency, and detection stability under sensor noise during physical deployment on a mobile robot.
\end{enumerate}

These research questions guide the design of the experimental evaluation, where each question is systematically addressed through targeted ablation studies, comparative benchmarks, and real-world validation presented in Chapter~\ref{ch:results}.

\section{Thesis Structure}
