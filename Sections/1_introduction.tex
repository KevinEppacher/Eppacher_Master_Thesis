% chktex-file 44

\chapter{Introduction}

The introduction of Transformer-based architectures~\cite{vaswaniAttentionAllYou2023} has opened new opportunities for integrating high-level semantic reasoning with low-level geometric navigation in robotics. Traditional robotic exploration methods have primarily focused on mapping unknown environments using geometric cues, often neglecting the rich semantic information available in visual and linguistic modalities~\cite{topiwalaFrontierBasedExploration2018}. However, recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled robots to interpret and act upon complex, open-ended instructions provided in natural language. These developments have catalyzed a shift toward semantic reasoning and zero-shot generalization, allowing robots to understand unseen concepts beyond fixed, task-specific datasets.

Consequently, new applications have emerged that require robots not only to explore and map their surroundings but also to reason about the semantic structure and relationships within them. In service robotics, for instance, a mobile agent may be instructed to locate a specific object based on high-level descriptions such as \textit{``find the red chair in the living room''}, rather than relying on a limited set of predefined categories such as those from COCO~\cite{linMicrosoftCOCOCommon2015}. Similarly, in search and rescue operations, robots may be tasked with locating missing persons based on vague or incomplete contextual information, such as the assumption that an individual might be found \textit{``in the bathroom''}. In industrial inspection, autonomous agents must identify structural anomalies or specific components within unstructured and partially observable environments, while in warehouse automation, robots must locate items or storage units that may not be consistently labeled or fully visible.

Across these domains, the integration of semantic understanding with autonomous navigation is becoming increasingly essential. Robots must be capable of interpreting abstract human instructions, reasoning about both spatial and semantic context, and conducting efficient searches in dynamic, partially known environments. These challenges motivate the development of a unified framework that bridges geometric exploration with semantic scene understanding, enabling autonomous agents to perform open-vocabulary, goal-directed exploration guided by high-level semantic input.

\section{Problem Statement}

Geometric exploration techniques are widely used for mapping unknown environments, by identifieng frontiers (2D or 3D for Drones), and navigating towards the frontier with the highest information gain, which can be determined with several algorithms. Especially useful are swarms of robots exploring large areas collaboratively. However, these methods are primarily designed for coverage and mapping tasks, rather than for goal-directed exploration based on semantic understanding. As a result, they often lead to inefficient search behavior when the objective is to locate specific objects or regions of interest defined by high-level semantic criteria.

Therefore, instead of using only LiDAR or depth sensors for geometric mapping, incorporating RGB camera input allows for semantic reasoning about the environment. In table\ref{tab:introduction:semantic_exploration_approaches} several recent works are summarized that leverage the use of either pretrained vision-language models or reinforcement learning based policies, guiding the robot towards areas that are more likely to contain the target object or region based on semantic cues.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\textwidth}{
        |>{\raggedright\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\raggedright\arraybackslash}X|}
        \hline
        \textbf{Approach} & \textbf{Training Required} & \textbf{Real-Time} & \textbf{Semantic Reasoning Model} \\ \hline
        \textbf{VLFM \cite{yokoyamaVLFMVisionLanguageFrontier2023}} & \xmark~(zero-shot) & \cmark & BLIP-2 + GroundingDINO + SAM \\ \hline
        \textbf{SemUtil \cite{chenHowNotTrain2023}} & \xmark~(training-free) & \cmark & Mask R-CNN + CLIP + BERT \\ \hline
        \textbf{ESC \cite{zhouESCExplorationSoft2023}} & \xmark~(zero-shot) & \cmark & GLIP + DeBERTa / ChatGPT reasoning \\ \hline
        \textbf{LGX \cite{dorbalaCanEmbodiedAgent2024}} & \xmark~(zero-shot) & \cmark & GPT-3 + GLIP + BLIP \\ \hline
        \textbf{CoW \cite{gadreCoWsPastureBaselines2022}} & \xmark~(zero-shot) & \cmark & CLIP similarity scoring \\ \hline
        \textbf{ZSON \cite{majumdarZSONZeroShotObjectGoal2023}} & \cmark~(RL pretraining) & \xmark & CLIP-based RL policy \\ \hline
        \textbf{PONI \cite{ramakrishnanPONIPotentialFunctions2022}} & \cmark~(supervised) & \xmark & Learned potential-field network \\ \hline
        \textbf{PIRLNav \cite{ramrakhyaPIRLNavPretrainingImitation2023}} & \cmark~(BC + RL) & \xmark & DINO-based CNN-RNN policy \\ \hline
    \end{tabularx}
    \caption{Overview of Semantic Zero-Shot and Trained Exploration Approaches}
    \label{tab:introduction:semantic_exploration_approaches}
\end{table}

ZSON \cite{majumdarZSONZeroShotObjectGoal2023}, PONI \cite{ramakrishnanPONIPotentialFunctions2022}, and PIRLNav \cite{ramrakhyaPIRLNavPretrainingImitation2023} utilize deep reinforcement learning (DRL) or supervised learning to train exploration policies that can generalize to unseen objects. While these methods demonstrate promising results, they often require extensive offline training and may struggle to adapt to new environments or object categories not encountered during training. In contrast, VLFM \cite{yokoyamaVLFMVisionLanguageFrontier2023}, SemUtil \cite{chenHowNotTrain2023}, ESC \cite{zhouESCExplorationSoft2023}, LGX \cite{dorbalaCanEmbodiedAgent2024}, and CoW \cite{gadreCoWsPastureBaselines2022} leverage pretrained vision-language models to perform zero-shot semantic exploration without additional training. These approaches enable real-time decision-making based on semantic cues extracted from RGB images, allowing robots to navigate towards areas likely to contain the target object or region. ESC \cite{zhouESCExplorationSoft2023} and LGX \cite{dorbalaCanEmbodiedAgent2024} further incorporate large language models (LLMs) to enhance reasoning capabilities, enabling more sophisticated interpretation of high-level instructions, but at the cost of increased computational requirements and potential latency issues. Even if VLFM \cite{yokoyamaVLFMVisionLanguageFrontier2023} real-time capable, it uses multiple heavy models (GroundingDINO, SAM, BLIP-2) that are difficult to run on mobile robots with limited compute resources, especially it runs on a 16 GB VRAM GPU (if all models are used at once). These approaches primarily focus on short-term exploration strategies, often neglecting the importance of persistent memory for long-term efficiency in multi-object search tasks and storing previously acquired semantic information within a unified world model. A global memory would allow the robot to recall previously explored areas and detected objects, reducing redundant exploration and improving overall search efficiency.


\textbf{Current solutions:}

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\textwidth}{
        |>{\raggedright\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\raggedright\arraybackslash}X
        |>{\raggedright\arraybackslash}X|}
        \hline
        \textbf{Approach} & \textbf{Training Required} & \textbf{Real-Time} & \textbf{Memory Representation} & \textbf{Exploration Integration} \\ \hline
        \textbf{OneMap} & \xmark~(zero-shot) & \cmark & 2D probabilistic feature field & \cmark~(frontier-based) \\ \hline
        \textbf{ConceptGraphs} & \xmark~(pretrained models) & \xmark & 3D scene graph & \cmark~(LLM-planner) \\ \hline
        \textbf{SemExp} & \cmark~(RL + supervised) & \xmark & 2D semantic occupancy map & \cmark~(learned policy) \\ \hline
        \textbf{GeFF} & \cmark~(ScanNet pretrain) & \cmark & Implicit 3D feature field & \xmark~(passive) \\ \hline
        \textbf{RayFronts} & \xmark~(foundation model) & \cmark & Hybrid voxel + ray field & \cmark~(semantic frontier) \\ \hline
        \textbf{VLMaps} & \xmark~(pretrained LSeg/CLIP) & \xmark & 2.5D open-vocab grid & \cmark~(frontier-compatible) \\ \hline
        \textbf{Pigeon} & \cmark~(RLVR fine-tune) & \cmark & Point-of-Interest snapshot memory & \cmark~(reasoning-aware) \\ \hline
    \end{tabularx}
    \caption{Overview of Persistent or Memory-Based Semantic Mapping Approaches.}
    \label{tab:introduction:persistent_mapping_approaches}
\end{table}


Despite notable progress, these methods often suffer from heavy GPU requirements, offline training procedures, or noisy semantic maps that reduce navigation reliability. Some further depend heavily on object detectors, which can introduce false positives in open-vocabulary settings.

\bigskip
\textbf{Core Gaps in Existing Work:}

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\textwidth}{
        |>{\raggedright\arraybackslash}X
        |>{\raggedright\arraybackslash}X
        |>{\raggedright\arraybackslash}X|}
        \hline
        \textbf{Limitation} & \textbf{Example Works} & \textbf{Implication} \\ \hline
        \textbf{No persistent memory} & VLFM, CoW, LGX, ESC & No long-term fusion or recall; repeated exploration of known areas. \\ \hline
        \textbf{Offline training required} & ZSON, PONI, PIRLNav, SemExp & Heavy RL/supervised training; poor adaptability to new scenes. \\ \hline
        \textbf{No balance between exploration and memory} & OneMap, RayFronts, VLMaps & Either passive mapping or short-term exploration; inefficient search. \\ \hline
        \textbf{No zero-shot exploration} & VLFM, CoW, LGX & Detect novel objects but fail to explore unseen regions strategically. \\ \hline
        \textbf{Premapping needed} & ConceptGraphs, VLMaps, GeFF & Depend on pre-recorded data; not suited for online autonomy. \\ \hline
        \textbf{Limited robustness} & PONI, SemExp, PIRLNav & Closed-set categories; fragile under real-world variation. \\ \hline
        \textbf{Low real-world applicability} & ConceptGraphs, VLMaps & High GPU cost or simulation-only; not deployable on mobile robots. \\ \hline
    \end{tabularx}
    \caption{Identified Gaps in Existing Semantic Exploration Frameworks.}
    \label{tab:introduction:identified_gaps}
\end{table}

\textbf{Derived Requirements from the State of the Art:}
\begin{itemize}
    \item Zero-shot frontier exploration framework.
    \item Persistent 3D semantic mapping with confidence-based fusion.
    \item Use of pretrained models only (no additional training required).
    \item Hybrid fusion strategy to balance exploration and memory.
    \item Multi-source detection fusion for robust object identification.
    \item GPU-efficient design for real-time deployment on mobile robots.
    \item Modular architecture for easy adaptation and extension.
    \item Independence from fixed, pre-trained object category sets.
\end{itemize}

\section{Scientific Contribution}

This work contributes to the state of the art by introducing a hybrid semantic exploration framework that integrates zero-shot semantic frontier scoring with persistent 3D scene representation, enabling autonomous search guided by open-vocabulary text queries. The system combines real-time semantic reasoning during exploration with a long-term spatial memory, allowing the robot to dynamically balance between discovering new information and exploiting previously acquired knowledge.

Unlike previous approaches that focus exclusively on either geometric frontiers or static semantic maps, the proposed framework continuously fuses information from multiple semantic sources to maintain a unified, confidence-based world model. Adaptive weighting enables the robot to adjust its behavior between exploration and exploitation according to the reliability of recent observations and the stability of stored semantic memory.

The framework further investigates how the quality and granularity of the underlying semantic information influence task success, navigation efficiency, and robustness. By systematically varying the trust between exploration and memory components, this work provides new insights into how semantic reasoning and persistent mapping can be effectively combined for open-vocabulary, multi-object search in dynamic environments.

To evaluate the contribution of the proposed system, the following research questions are formulated:

\begin{enumerate}
    \item \textbf{How does integrating zero-shot semantic exploration and persistent 3D semantic mapping affect multi-object search performance and navigation efficiency compared to existing methods?} \\
    \textit{Metrics:} Performance is quantified in terms of task success and path efficiency, measured through Success Rate (SR), Success per Path Length (SPL), and Multi-Object Success Rate (MSR) relative to representative state-of-the-art systems such as OneMap, VLFM, and Pigeon.
    
    \item \textbf{How does the interaction between live exploration and accumulated semantic memory influence overall system performance?} \\
    \textit{Metrics:} The weighting factor between exploration and memory is varied during graph node fusion to assess impacts on SR and SPL, identifying optimal trade-offs between reactivity and exploitation.
    
    \item \textbf{How does the granularity of semantic map retrieval affect map quality, and can dynamic weighting between exploration and memory compensate for potential noise?} \\
    \textit{Metrics:} The semantic granularity in the 3D semantic mapper is varied while adjusting exploration weight to evaluate effects on SR and SPL\@.
    
    \item \textbf{How does multi-source fusion of detection confidence, semantic similarity, and memory confidence impact detection robustness and false-positive suppression during exploration?} \\
    \textit{Metrics:} Precision, Recall, F1-Score, Confusion Matrix, and SR under different fusion weight configurations across COCO, open-vocabulary, and zero-shot classes.
    
    \item \textbf{What is the computational footprint and real-world robustness of the hybrid framework?} \\
    \textit{Metrics:} Frames per second (FPS), GPU/CPU usage, inference latency, and detection stability under sensor noise during physical deployment on a mobile robot.
\end{enumerate}

These research questions guide the design of the experimental evaluation, where each question is systematically addressed through targeted ablation studies, comparative benchmarks, and real-world validation presented in Chapter~\ref{ch:results}.

\section{Thesis Structure}
