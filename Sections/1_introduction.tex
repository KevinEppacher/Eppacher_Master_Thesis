% chktex-file 44

\chapter{Introduction}

The introduction of Transformer-based architectures~\cite{vaswaniAttentionAllYou2023} has opened new opportunities for integrating high-level semantic reasoning with low-level geometric navigation in robotics. Traditional robotic exploration methods have primarily focused on mapping unknown environments using geometric cues, often neglecting the rich semantic information available in visual and linguistic modalities~\cite{topiwalaFrontierBasedExploration2018}. However, recent advances in \acp{LLM}~\cite{brownLanguageModelsAre2020} and \acp{VLM}~\cite{radfordLearningTransferableVisual2021} have enabled robots to interpret and act upon complex, open-ended instructions expressed in natural language~\cite{brohanRT2VisionLanguageActionModels2023}. These developments mark a transition from deterministically modeled and explicitly programmed robotic systems toward zero-shot generalizable behavior, enabling robots to reason about previously unseen concepts beyond fixed, task-specific datasets.

Consequently, new applications have emerged that require robots not only to explore and map their surroundings but also to reason about the semantic structure and relationships within them. In service robotics, for instance, a mobile agent may be instructed to locate a specific object based on high-level descriptions such as \textit{``find the red chair in the living room''}, rather than relying on a limited set of predefined categories such as those from \ac{COCO}~\cite{linMicrosoftCOCOCommon2015}. Similarly, in \ac{SAR} operations~\cite{schwaigerUGVCBRNUnmannedGround2024}, robots may be tasked with locating missing persons based on vague or incomplete contextual information, such as the assumption that an individual might be found \textit{``in the bathroom''}. In industrial inspection, autonomous agents must identify structural anomalies or specific components within unstructured and partially observable environments, while in warehouse automation, robots must locate items or storage units that may not be consistently labeled or fully visible.

Across these domains, the integration of semantic understanding with autonomous navigation is an active area of research~\cite{chenHowNotTrain2023,yokoyamaVLFMVisionLanguageFrontier2023,zhouESCExplorationSoft2023,majumdarZSONZeroShotObjectGoal2023}. Robots must be capable of interpreting abstract human instructions. Traditional geometric exploration approaches~\cite{lluviaActiveMappingRobot2021,topiwalaFrontierBasedExploration2018,bourgaultInformationBasedAdaptive2002} focus on exploring unknown environments by maximizing information gain about the spatial structure. However, in semantic exploration tasks, the robot must reason jointly about spatial and semantic context to strategically locate target objects or regions of interest~\cite{yokoyamaVLFMVisionLanguageFrontier2023,zhouESCExplorationSoft2023,gadreCoWsPastureBaselines2022,alamaRayFrontsOpenSetSemantic2025}.

Current research increasingly leverages pretrained \acp{VLM} to extract semantic cues from RGB images, enabling zero-shot reasoning about novel objects and scenes~\cite{chenHowNotTrain2023,majumdarZSONZeroShotObjectGoal2023,yokoyamaVLFMVisionLanguageFrontier2023}. However, many of these approaches rely on short-term or episodic semantic representations and lack mechanisms for persistent spatial memory or principled integration of semantic information into long-term exploration decisions. Furthermore, the dynamic and partially known nature of real-world environments necessitates efficient search strategies that balance exploration of unknown areas with exploitation of previously acquired knowledge~\cite{buschOneMapFind2025,guConceptGraphsOpenVocabulary3D2023,huangVisualLanguageMaps2023,PIGEONVLMDrivenObject}.

These challenges motivate the development of a unified framework that bridges geometric exploration with semantic scene understanding by balancing semantic frontier-based exploration with long-term semantic memory, enabling autonomous agents to perform open-vocabulary, goal-directed exploration guided by high-level semantic input.

\section{Language-Guided Exploration}

Traditional geometric exploration techniques are widely used for mapping unknown environments by identifying frontiers in either two or three dimensions and navigating toward the frontier with the highest expected information gain~\cite{bourgaultInformationBasedAdaptive2002,quinApproachesEfficientlyDetecting2021}. Such methods, including those based on occupancy grids~\cite{thrunProbabilisticRobotics2006} or point cloud representations~\cite{liu3DPointCloud2021}, are particularly effective for coverage and mapping tasks and have been successfully extended to multi-robot systems for large-scale exploration~\cite{tellaroliFrontierBasedExplorationMultiRobot2024}. However, these approaches remain primarily geometry-driven and do not incorporate semantic understanding of the environment. Consequently, they are suboptimal for goal-directed exploration tasks, where the objective is to locate specific objects or regions of interest defined by high-level semantic criteria rather than unexplored geometry~\cite{alamaRayFrontsOpenSetSemantic2025}.

To address this limitation, recent research has focused on integrating semantic perception into exploration frameworks~\cite{yokoyamaVLFMVisionLanguageFrontier2023,chenHowNotTrain2023,zhouESCExplorationSoft2023,dorbalaCanEmbodiedAgent2024,gadreCoWsPastureBaselines2022,majumdarZSONZeroShotObjectGoal2023,ramakrishnanPONIPotentialFunctions2022,ramrakhyaPIRLNavPretrainingImitation2023}. Instead of relying solely on LiDAR or depth sensors for geometric mapping, the use of RGB imagery enables semantic reasoning about the scene and the objects contained within it. Table~\ref{tab:introduction:semantic_exploration_approaches} summarizes representative works that
leverage either pretrained \acp{VLM} or learning-based navigation policies trained via
Behavioral Cloning (BC) or Reinforcement Learning (RL),
to guide robots toward regions that are semantically relevant to a given target
description in natural language.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\textwidth}{
        |>{\raggedright\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\raggedright\arraybackslash}X|}
        \hline
        \textbf{Approach} & \textbf{Training Required} & \textbf{Real-Time} & \textbf{Semantic Reasoning Model} \\ \hline
        \textbf{\ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023}} & \xmark~(zero-shot) & \cmark & BLIP-2 + GroundingDINO + SAM \\ \hline
        \textbf{SemUtil~\cite{chenHowNotTrain2023}} & \xmark~(training-free) & \cmark & Mask R-CNN + CLIP + BERT \\ \hline
        \textbf{ESC~\cite{zhouESCExplorationSoft2023}} & \xmark~(zero-shot) & \cmark & GLIP + DeBERTa / ChatGPT reasoning \\ \hline
        \textbf{LGX~\cite{dorbalaCanEmbodiedAgent2024}} & \xmark~(zero-shot) & \cmark & GPT-3 + GLIP + BLIP \\ \hline
        \textbf{CoW~\cite{gadreCoWsPastureBaselines2022}} & \xmark~(zero-shot) & \cmark & CLIP similarity scoring \\ \hline
        \textbf{ZSON~\cite{majumdarZSONZeroShotObjectGoal2023}} & \cmark~(\ac{RL} pretraining) & \xmark & CLIP-based RL policy \\ \hline
        \textbf{PONI~\cite{ramakrishnanPONIPotentialFunctions2022}} & \cmark~(supervised) & \xmark & Learned potential-field network \\ \hline
        \textbf{PIRLNav~\cite{ramrakhyaPIRLNavPretrainingImitation2023}} & \cmark~(\ac{BC} + \ac{RL}) & \xmark & DINO-based CNN-RNN policy \\ \hline
    \end{tabularx}
    \caption{Overview of semantic zero-shot and trained exploration approaches. All of these methods are capable of navigating to a goal described in natural language, however, they differ in zero-shot applicability to new scenes and real-time capability.}
    \label{tab:introduction:semantic_exploration_approaches}
\end{table}

Approaches such as ZSON~\cite{majumdarZSONZeroShotObjectGoal2023}, PONI~\cite{ramakrishnanPONIPotentialFunctions2022}, and PIRLNav~\cite{ramrakhyaPIRLNavPretrainingImitation2023} employ deep reinforcement learning (DRL) or supervised training to develop navigation policies capable of generalizing to unseen objects. Although these methods achieve promising results in simulation, they require extensive offline training and exhibit limited adaptability to previously unseen environments or object categories not encountered during training. In contrast, zero-shot methods such as \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, SemUtil~\cite{chenHowNotTrain2023}, ESC~\cite{zhouESCExplorationSoft2023}, LGX~\cite{dorbalaCanEmbodiedAgent2024}, and CoW~\cite{gadreCoWsPastureBaselines2022} leverage pretrained \acp{VLM} to perform semantic exploration without additional training. These models enable real-time decision-making by exploiting semantic cues extracted from RGB imagery, guiding robots toward areas likely to contain the target object or region.

Some approaches, such as ESC~\cite{zhouESCExplorationSoft2023} and LGX~\cite{dorbalaCanEmbodiedAgent2024}, further integrate \acp{LLM} for commonsense reasoning and high-level task interpretation, enabling a more contextual understanding of complex instructions. However, this comes at the cost of increased computational demand and potential inference latency, which limits their applicability on resource-constrained mobile platforms. While \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023} achieves real-time performance, it relies on multiple computationally expensive foundation models (GroundingDINO~\cite{liuGroundingDINOMarrying2024}, \ac{SAM}~\cite{kirillovSegmentAnything2023}, and \ac{BLIP-2}~\cite{liBLIP2BootstrappingLanguageImage2023}) that require high-end GPUs with up to 16~GB of VRAM, which is impractical for embedded robotic systems.

Overall, these language-guided exploration methods primarily focus on short-term semantic reasoning and lack persistent memory. They do not maintain long-term storage or recall mechanisms for previously acquired semantic knowledge, leading to redundant exploration and reduced efficiency in multi-object and chained search tasks. 

\section{Language-Embedded Semantic Mapping}
A language-embedded semantic map serves as a persistent spatial memory that jointly encodes
the geometric structure of the environment and its semantic content. Semantic information
can be incorporated either by associating discrete object classes~\cite{chaplotObjectGoalNavigation2020}, inferred by a visual
perception backbone, with their spatial locations, or by embedding high-dimensional visual
representations into a spatial map~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023,guConceptGraphsOpenVocabulary3D2023,jatavallabhulaConceptFusionOpensetMultimodal2023}, such as a projected voxel grid. In the latter case, the
visual embeddings capture semantic properties of objects and regions beyond fixed category
labels~\cite{yokoyamaVLFMVisionLanguageFrontier2023,yamazakiOpenFusionRealtimeOpenVocabulary2023,guConceptGraphsOpenVocabulary3D2023}. This abstraction allows semantic information to be reused across tasks and time horizons, rather than being tied to a single perception or navigation episode~\cite{buschOneMapFind2025}.

Such semantic representations can be queried using natural language prompts, enabling robots
to reason about the presence, distribution, and spatial relationships of objects or regions of
interest based on high-level descriptions~\cite{zhouESCExplorationSoft2023,alamaRayFrontsOpenSetSemantic2025}. By leveraging either object class information or continuous visual embeddings, a robot can guide its navigation toward areas with a high
likelihood of containing a specified target~\cite{huangVisualLanguageMaps2023,buschOneMapFind2025}, thereby improving search efficiency and task success rates. Furthermore, language-embedded semantic maps can be combined with high-level
reasoning modules, such as Large Language Models (\acp{LLM}), to infer object relationships,
contextual cues, and action sequences required to accomplish more complex, multi-step goals~\cite{zhouESCExplorationSoft2023,dorbalaCanEmbodiedAgent2024}.

Maintaining such semantic representations persistently over time enables robots to exploit
past observations, recall previously detected objects, and avoid redundant exploration of
already known regions. This form of long-term global memory improves navigation efficiency,
scalability, and robustness in open-vocabulary, real-world environments~\cite{buschOneMapFind2025,PIGEONVLMDrivenObject}. 
Table~\ref{tab:introduction:persistent_mapping_approaches} summarizes representative works
that incorporate persistent or memory-based semantic mapping to enhance exploration
capabilities.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\textwidth}{
        |>{\raggedright\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\centering\arraybackslash}X
        |>{\raggedright\arraybackslash}X
        |>{\raggedright\arraybackslash}X|}
        \hline
        \textbf{Approach} & \textbf{Training Required} & \textbf{Real-Time} & \textbf{Memory Representation} & \textbf{Exploration Integration} \\ \hline
        \textbf{\ac{OneMap}~\cite{buschOneMapFind2025}} & \xmark~(zero-shot) & \cmark & 2D probabilistic feature field & \cmark~(frontier-based) \\ \hline
        \textbf{\ac{ConceptGraphs}~\cite{guConceptGraphsOpenVocabulary3D2023}} & \xmark~(pretrained models) & \xmark & 3D scene graph & \cmark~(LLM-planner) \\ \hline
        \textbf{\ac{SemExp}~\cite{chaplotObjectGoalNavigation2020}} & \cmark~(RL + supervised) & \xmark & 2D semantic occupancy map & \cmark~(learned policy) \\ \hline
        \textbf{\ac{GeFF}~\cite{qiuLearningGeneralizableFeature2024}} & \cmark~(ScanNet pretrain) & \cmark & Implicit 3D feature field & \xmark~(passive) \\ \hline
        \textbf{RayFronts~\cite{alamaRayFrontsOpenSetSemantic2025}} & \xmark~(foundation model) & \cmark & Hybrid voxel + ray field & \xmark~(planner-agnostic) \\ \hline
        \textbf{\ac{VLMaps}~\cite{huangVisualLanguageMaps2023}} & \xmark~(pretrained LSeg/CLIP) & \xmark & 2.5D open-vocab grid & \cmark~(frontier-compatible) \\ \hline
        \textbf{Pigeon~\cite{PIGEONVLMDrivenObject}} & \cmark~(RLVR fine-tune) & \cmark & Point-of-Interest snapshot memory & \cmark~(reasoning-aware) \\ \hline
    \end{tabularx}
    \caption{This table provides an overview of representative persistent or memory-based semantic
        mapping approaches, comparing their training requirements, real-time capability, underlying
        memory representations, and the extent to which semantic memory is integrated into exploration
        or planning.}
    \label{tab:introduction:persistent_mapping_approaches}
\end{table}

methods such as SemExp~\cite{chaplotObjectGoalNavigation2020}, Pigeon~\cite{PIGEONVLMDrivenObject}, and GeFF~\cite{qiuLearningGeneralizableFeature2024} rely on offline training to construct persistent semantic representations. In the case of policy-learning approaches, this dependence on extensive training limits adaptability to previously seen environments and unseen object categories. Other approaches, such as \ac{ConceptGraphs}~\cite{guConceptGraphsOpenVocabulary3D2023} and \ac{VLMaps}~\cite{huangVisualLanguageMaps2023}, construct persistent open-vocabulary maps using pretrained foundation models but often require pre-mapping and lack real-time performance, which restricts their use in dynamic or large-scale settings.

While \ac{OneMap}~\cite{buschOneMapFind2025} achieves real-time performance on embedded hardware such as the Jetson Orin AGX, its computational cost limits operation to approximately 2~Hz, which may be insufficient for high-speed navigation tasks. Additionally, noise in depth perception directly degrades the quality of the probabilistic feature map, reducing overall semantic reliability. The detector used in \ac{OneMap}~\cite{buschOneMapFind2025} relies solely on \ac{VLM}-based CLIP image-text similarity, which makes it prone to false positives under open-vocabulary conditions~\cite{zareianOpenVocabularyObjectDetection2021}. 

\ac{GeFF}~\cite{qiuLearningGeneralizableFeature2024} provides a compact implicit 3D representation by distilling CLIP-aligned features into a neural field, enabling both geometric and semantic understanding. However, it requires pretraining on large-scale datasets such as ScanNet, limiting its direct generalization to arbitrary environments. RayFronts~\cite{alamaRayFrontsOpenSetSemantic2025} introduces a hybrid 3D representation that combines voxel-based semantics with ray-based frontier expansion, offering real-time operation and high efficiency for open-set semantic search. Nevertheless, its computational complexity grows with environment size, and its planner-agnostic design prevents it from actively guiding exploration toward semantically relevant regions.

Table~\ref{tab:introduction:identified_gaps} summarizes the key limitations observed across existing semantic exploration frameworks. These gaps illustrate the need for a unified approach that combines zero-shot semantic understanding, persistent spatial memory, and real-time exploration to achieve robust, scalable autonomy in complex environments.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\textwidth}{
        |>{\raggedright\arraybackslash}X
        |>{\raggedright\arraybackslash}X
        |>{\raggedright\arraybackslash}X|}
        \hline
        \textbf{Limitation} & \textbf{Example Works} & \textbf{Implication} \\ \hline
        \textbf{No persistent memory} & \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, CoW~\cite{gadreCoWsPastureBaselines2022}, LGX~\cite{dorbalaCanEmbodiedAgent2024}, ESC~\cite{zhouESCExplorationSoft2023} & No long-term fusion or recall; repeated exploration of known areas. \\ \hline
        \textbf{Offline training required} & ZSON~\cite{majumdarZSONZeroShotObjectGoal2023}, PONI~\cite{ramakrishnanPONIPotentialFunctions2022}, PIRLNav~\cite{ramrakhyaPIRLNavPretrainingImitation2023}, \ac{SemExp}~\cite{chaplotObjectGoalNavigation2020} & Heavy RL/supervised training; poor adaptability to new scenes. \\ \hline
        \textbf{No balance between exploration and memory} & \ac{OneMap}~\cite{buschOneMapFind2025}, RayFronts~\cite{alamaRayFrontsOpenSetSemantic2025}, \ac{VLMaps}~\cite{huangVisualLanguageMaps2023} & Either passive mapping or short-term exploration; inefficient search. \\ \hline
        \textbf{No zero-shot exploration} & \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, CoW~\cite{gadreCoWsPastureBaselines2022}, LGX~\cite{dorbalaCanEmbodiedAgent2024} & Detect novel objects but fail to explore unseen regions strategically. \\ \hline
        \textbf{Premapping needed} & \ac{ConceptGraphs}~\cite{guConceptGraphsOpenVocabulary3D2023}, \ac{VLMaps}~\cite{huangVisualLanguageMaps2023}, \ac{GeFF}~\cite{qiuLearningGeneralizableFeature2024} & Depend on pre-recorded data; not suited for online autonomy. \\ \hline
        \textbf{Limited robustness} & PONI~\cite{ramakrishnanPONIPotentialFunctions2022}, \ac{SemExp}~\cite{chaplotObjectGoalNavigation2020}, PIRLNav~\cite{ramrakhyaPIRLNavPretrainingImitation2023} & Closed-set categories; fragile under real-world variation. \\ \hline
        \textbf{Low real-world applicability} & \ac{ConceptGraphs}~\cite{guConceptGraphsOpenVocabulary3D2023}, \ac{VLMaps}~\cite{huangVisualLanguageMaps2023}, Pigeon~\cite{PIGEONVLMDrivenObject} & High GPU cost or simulation-only evaluation; limited deployability on mobile robots. \\ \hline
    \end{tabularx}
    \caption{This table summarizes key limitations of existing semantic exploration frameworks,
providing representative example works for each limitation and outlining their practical
implications for autonomous navigation and exploration.}

    \label{tab:introduction:identified_gaps}
\end{table}

These observations reveal several fundamental challenges that are not yet adequately
addressed by existing semantic exploration frameworks.

First, many approaches lack the ability to perform zero-shot exploration while maintaining
a persistent and incrementally updated semantic representation of the environment
\cite{yokoyamaVLFMVisionLanguageFrontier2023,chenHowNotTrain2023,zhouESCExplorationSoft2023,
gadreCoWsPastureBaselines2022,ramrakhyaPIRLNavPretrainingImitation2023,
majumdarZSONZeroShotObjectGoal2023,ramakrishnanPONIPotentialFunctions2022}.
As a result, semantic information is often either discarded after individual navigation
episodes or requires pre-mapped environments, limiting applicability in unknown or
changing scenes.

Second, a strong reliance on computationally expensive deep reinforcement learning or
supervised training pipelines remains common. This dependence restricts adaptability to
new environments and object categories and poses significant challenges for deployment on
resource-constrained robotic platforms
\cite{majumdarZSONZeroShotObjectGoal2023,
gadreCoWsPastureBaselines2022,
ramakrishnanPONIPotentialFunctions2022}.

Third, existing methods struggle to robustly handle the reliability of semantic information
during exploration. Semantic maps constructed from open-vocabulary perception are inherently
noisy and uncertain
\cite{radfordLearningTransferableVisual2021,zareianOpenVocabularyObjectDetection2021},
and indiscriminate reliance on semantic memory can lead to inefficient navigation
\cite{huangVisualLanguageMaps2023}, as robots may pursue low-confidence or spurious cues
instead of exploring informative regions. In such cases, poorly calibrated use of semantic
memory may negate the potential benefits of semantic guidance
\cite{ramakrishnanPONIPotentialFunctions2022}.

Conversely, approaches that rely exclusively on semantic-driven exploration may overlook
structurally plausible regions suggested by prior observations, resulting in reduced task
success. This challenge is further amplified in dynamic environments, where previously
stored semantic information may become outdated or misleading over time.

Fourth, real-world deployment introduces additional challenges related to computational
efficiency, robustness to sensor noise, and environmental variability. Many current systems
struggle to sustain reliable real-time performance under embedded hardware constraints,
leading to increased latency or unstable inference behavior
\cite{guConceptGraphsOpenVocabulary3D2023,yokoyamaVLFMVisionLanguageFrontier2023}.

Finally, several existing approaches rely on single-source detection or similarity pipelines,
which are prone to false positives and semantic ambiguities under open-vocabulary conditions.
The lack of multi-source semantic validation and memory-aware reasoning limits robustness
and consistency during long-term autonomous operation
\cite{buschOneMapFind2025,gadreCoWsPastureBaselines2022,
dorbalaCanEmbodiedAgent2024,yokoyamaVLFMVisionLanguageFrontier2023}.


\section{Scientific Contribution}

This work contributes to the state of the art by introducing a hybrid semantic exploration framework that integrates zero-shot semantic frontier scoring with persistent 3D scene representation, enabling autonomous search guided by open-vocabulary text queries. The system combines real-time semantic reasoning during exploration with a long-term spatial memory, allowing the robot to dynamically balance between discovering new information and exploiting previously acquired knowledge.

Unlike previous approaches that focus exclusively on either geometric frontiers or static semantic maps, the proposed framework continuously fuses information from multiple semantic sources to maintain a unified, confidence-based world model. Adaptive weighting enables the robot to adjust its behavior between exploration and exploitation according to the reliability of recent observations and the stability of stored semantic memory.

The framework further investigates how the quality and granularity of the underlying semantic information influence task success, navigation efficiency, and robustness. By systematically varying the trust between exploration and memory components, this work provides new insights into how semantic reasoning and persistent mapping can be effectively combined for open-vocabulary, multi-object search in dynamic environments.

To evaluate the contribution of the proposed system, the following research questions are formulated:

\begin{enumerate}
    \item \textbf{How does integrating zero-shot semantic exploration and persistent 3D semantic mapping affect multi-object search performance and navigation efficiency compared to existing methods?} \\
    \textit{Metrics:} Performance is quantified in terms of task success and path efficiency, measured through \ac{SR}, \ac{SPL}, and \ac{MSR} relative to representative state-of-the-art systems such as \ac{OneMap}~\cite{buschOneMapFind2025}, \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, and Pigeon~\cite{PIGEONVLMDrivenObject}.
    
    \item \textbf{How does the interaction between live exploration and accumulated semantic memory influence overall system performance?} \\
    \textit{Metrics:} The weighting factor between exploration and memory is varied during graph node fusion to assess impacts on \ac{SR} and \ac{SPL}, identifying optimal trade-offs between reactivity and exploitation.
    
    \item \textbf{How does the granularity of semantic map retrieval affect map quality, and can dynamic weighting between exploration and memory compensate for potential noise?} \\
    \textit{Metrics:} The semantic granularity in the 3D semantic mapper is varied while adjusting exploration weight to evaluate effects on \ac{SR} and \ac{SPL}\@.
    
    \item \textbf{How does multi-source fusion of detection confidence, semantic similarity, and memory confidence impact detection robustness and false-positive suppression during exploration?} \\
    \textit{Metrics:} Precision, Recall, F1-Score, Confusion Matrix, and \ac{SR} under different fusion weight configurations across \ac{COCO}, open-vocabulary, and zero-shot classes.
    
    \item \textbf{What is the computational footprint and real-world robustness of the hybrid framework?} \\
    \textit{Metrics:} \ac{FPS}, GPU/CPU usage, inference latency, and detection stability under sensor noise during physical deployment on a mobile robot.
\end{enumerate}

These research questions guide the design of the experimental evaluation, where each question is systematically addressed through targeted ablation studies, comparative benchmarks, and real-world validation presented in Chapter~\ref{ch:results}.

\section{Thesis Structure}

This work is structured as follows: Chapter~\ref{ch:sota} describes the state-of-the-art in semantic exploration, persistent mapping, and object detection. Chapter~\ref{ch:methods} describes the methods used, for hybrid semantic exploration, persistent 3D mapping, promptable zero-shot detection, and multi-source fusion strategies. Chapter ~\ref{ch:implementation} details the practical implementation of the proposed approach, covering the simulation and real-world setup, datasets, software stack, and hardware configuration. Chapter~\ref{ch:results} presents the experimental evaluation, including ablation studies, comparative benchmarks, and real-world validation. Finally, Chapter~\ref{ch:conclusion} concludes the thesis with a summary of findings, discussion of limitations, and suggestions for future research directions.