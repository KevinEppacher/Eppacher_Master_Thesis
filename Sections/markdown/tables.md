
| **Approach** | **Training Required** | **Real-Time** | **Sensors** | **Semantic Reasoning Model** |
|:--------------|:--------------------:|:--------------:|:------------|:-----------------------------|
| **VLFM** | ✗ *(Zero-shot; no task-specific training)* | ✓ *(deployed on Boston Dynamics Spot in real-time with RTX 4090 laptop)* | RGB-D camera + odometry | BLIP-2 vision-language model for cosine similarity scoring + YOLOv7 (COCO) + GroundingDINO (open-vocab) + MobileSAM (mask extraction) |
| **SemUtil (StructNav)** | ✗ *(Training-free modular pipeline)* | ✓ *(Runs in simulation at interactive rates; real-time capable on GPU hardware)* | RGB-D camera + visual odometry | Combines pre-trained **Mask R-CNN** (for semantic segmentation) with **BERT** and **CLIP** embeddings for category-to-category spatial priors; semantic frontiers scored using spatial scene graphs and language priors |
| **ESC (Exploration with Soft Commonsense Constraints)** | ✗ *(Zero-shot; no navigation or environment training)* | ✓ *(PSL + frontier-based exploration in simulation, efficient inference at ~0.25 s per step)* | RGB-D + GPS / visual odometry | **GLIP** for open-world visual grounding (objects & rooms), **DeBERTa-v3** / **ChatGPT** for commonsense reasoning, integrated via **Probabilistic Soft Logic (PSL)** for frontier selection and semantic exploration |
| **LGX (Language-Guided Exploration)** | ✗ *(Zero-shot; no policy or navigation training)* | ✓ *(Evaluated in simulation and real-world on TurtleBot2; ~8–10 Hz perception loop)* | RGB-D + odometry (RoboTHOR / RTAB-Map) | Combines **GPT-3** (commonsense reasoning for exploration) with **GLIP** (open-vocabulary object grounding), **YOLO** and **BLIP** (for semantic scene extraction and caption-based prompts) |
| **CoW (CLIP on Wheels)** | ✗ *(Zero-shot; no fine-tuning or task-specific training)* | ✓ *(Runs in real-time on a Boston Dynamics Spot with an onboard GPU laptop)* | RGB-D camera + odometry (depth used for 3D projection) | **CLIP (ViT-B/16)** for vision-language similarity scoring between camera frames and textual prompts; integrates **frontier exploration** and **localization-based target search** |    
| **ZSON (Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings)** | ✓ *(Trained on ImageNav only; no ObjectNav or human annotations)* | ✗ *(Inference via RL policy; simulation-based, not real-time deployment)* | RGB only *(no depth)* | **CLIP (ResNet-50 backbone)** creates a unified multimodal embedding space for image and text goals; navigation policy trained via **DD-PPO reinforcement learning** on image-goals, enabling zero-shot transfer to text-based object-goals |
| **PONI** | ✓ (supervised training of potential-function network on passive semantic maps) | ✗ (not intended for real-time deployment; planning in simulation / offline maps) | RGB-D + odometry / pose | Semantic “potential-function network”: uses a top-down semantic map (per-cell object categories) as input; no external VLM — reasoning based on semantic map structure + geometry |
| **PIRLNav (Pretraining with Imitation and RL Finetuning)** | ✓ *(Two-stage BC + RL pipeline trained on 77k human demonstrations)* | ✗ *(Simulation only; no real-time deployment, large-scale DD-PPO training)* | RGB + GPS/Compass *(no depth used; egocentric perception only)* | CNN + RNN policy using **ResNet-50 backbone** pretrained with **DINO**, fine-tuned via **Behavior Cloning** and **Reinforcement Learning (DD-PPO)**; no external VLM—semantic understanding emerges implicitly from visual encoder |

| **Approach** | **Training Required** | **Real-Time (GPU VRAM)** | **Sensors** | **Semantic Reasoning Model** | **Memory Representation** | **Query Modality** | **Semantic Generalization** | **Exploration Integration** | **Remarks / Limitations** |
|:--------------|:--------------------:|:-------------------------:|:------------|:-----------------------------|:---------------------------|:--------------------|:------------------|:-----------------------------|:-----------------------------|
| **OneMap** | ✗ *(Zero-shot; no fine-tuning or environment training)* | ✓ *(~2 Hz on Jetson Orin AGX; moderate VRAM usage ~8–12 GB)* | RGB-D + LiDAR + odometry | **SED encoder (ConvNeXt backbone)** aligned to **CLIP** feature space for patch-level open-vocab features | **2D probabilistic feature field (belief map)** storing CLIP-aligned embeddings with per-cell uncertainty | Text prompts via CLIP text encoder (cosine similarity per map cell) | **Open-vocabulary / zero-shot** (CLIP-aligned features) | ✓ *(Frontier-based semantic exploration + uncertainty-aware goal selection)* | Real-time on embedded GPU; currently 2D map (not volumetric); performance drops with map size; sensitive to depth noise |
| **ConceptGraphs** | ✗ (No training or fine-tuning; uses off-the-shelf foundation models) | ✗ (Offline mapping; LVLM and LLM inference ≈ > 20 GB VRAM per scene) | RGB-D (RealSense / Azure Kinect) + LiDAR (VLP-16) | **Segment Anything + CLIP + LLaVA + GPT-4** pipeline (SAM for masks, CLIP for features, LLaVA for object captions, GPT-4 for reasoning) | **3D Scene Graph** — object-centric graph (nodes = objects with point cloud + embedding; edges = LLM-derived relations like “on”, “in”) | Text and image prompts parsed via LLM planner; supports complex affordance and negation queries | **Open-vocabulary / zero-shot** (CLIP & LLaVA features + LLM captioning) | ✓ (Used for navigation and object search via LLM planner + costmap integration for traversability) | High semantic accuracy (~70 % node precision, 90 % edge precision); heavy inference cost (LLaVA + GPT-4 calls); offline operation; not continuous; LVLM caption errors on small objects; no temporal fusion yet:contentReference[oaicite:4]{index=4}:contentReference[oaicite:5]{index=5} |
| **SemExp (Goal-Oriented Semantic Exploration)** | ✓ *(supervised + reinforcement learning for semantic policy)* | ✗ *(trained in Habitat simulator; ~10M frames, GPU cluster; not real-time)* | RGB-D + odometry | **Mask R-CNN (MS-COCO pretrained)** for object detection & segmentation; semantic map + **CNN-based goal policy** trained via **PPO** | **2D top-down semantic map** with explicit obstacle, exploration, and category channels (episodic memory) | Discrete goal category (closed-set) | **Closed-set** (6–15 known categories) | ✓ *(learned goal-oriented policy on top of semantic map)* | High success rate (CVPR 2020 winner); but closed-vocab and trained policy; not zero-shot; limited to discrete goals; relies on accurate segmentation and depth |
| **GeFF (Generalizable Feature Fields)** | ✓ (Pre-training on ScanNet with CLIP feature distillation; no fine-tuning for deployment) | ✓ (~0.4 s latency on Jetson Orin AGX; ≈ 12 GB VRAM) | RGB-D + VIO poses + (optional in-wrist camera for fine details) | **Gen-NeRF + MaskCLIP distillation** (CLIP-aligned feature fields; implicit NeRF encoder with SDF depth regularization) | **Implicit 3D Feature Field** – sparse latent volume encoding geometry and semantics with on-the-fly decoding | Text queries via CLIP embeddings (+ conditional CLIP for part-level prompts) | **Open-vocabulary / zero-shot** (CLIP alignment for scene semantics and parts) | ✓ *(Supports semantic-aware navigation and grasp planning; real-time updates for dynamic scenes)* | Unified geometry + semantics for navigation and manipulation; real-time on mobile robot; no explicit frontier planner; requires ScanNet pre-training; SDF model limited to room-scale (≤ 60 m²) scenes; no multi-agent support. |
| **RayFronts** | ✗ (No fine-tuning; uses pre-trained RADIO v2.5 with SIGLIP adapter) | ✓ (Real-time 8.84 Hz on Jetson Orin AGX; ~12 GB VRAM) | RGB-D (+ LiDAR or Stereo Depth) | **RADIO (ViT + CLIP + DINOv2 + SAM)** with NACLIP-style attention for dense language alignment:contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1} | **Hybrid 3D Map = Semantic Voxels + Ray Frontiers** (OpenVDB occupancy grid + multi-directional semantic rays for beyond-range entities):contentReference[oaicite:2]{index=2}:contentReference[oaicite:3]{index=3} | Text + Image Queries (e.g., “red building”, “road cracks”, “metal stairs”) | **Open-vocabulary / Zero-shot** (CLIP-aligned SIGLIP features supporting fine-grained and long-tail concepts):contentReference[oaicite:4]{index=4}:contentReference[oaicite:5]{index=5} | ✓ (Explicit frontier integration for semantic exploration; planner-agnostic metric for search volume reduction – SCVR):contentReference[oaicite:6]{index=6}:contentReference[oaicite:7]{index=7} | Unifies in-range and beyond-range mapping; fine-grained and efficient; runs at 17.5 Hz encoder throughput (Fig. 5 p. 7); 16.5× faster than Trident-3D with 1.34× higher mIoU; highest SCVR efficiency (2.2× baselines); higher memory usage due to ray bins:contentReference[oaicite:8]{index=8}:contentReference[oaicite:9]{index=9} |
| **VLMaps (Visual Language Maps for Robot Navigation)** | ✗ (uses pretrained **LSeg** and **CLIP**; no fine-tuning) | ✗ (offline map fusion; real-time navigation feasible after map creation) | RGB-D + odometry | **LSeg + CLIP + LLM (Codex/GPT)** for open-vocab segmentation, spatial grounding, and language decomposition | **2.5D top-down grid map** fused with **dense visual-language embeddings** (CLIP feature space averaged per cell) | Text-based (spatial & object-centric commands, e.g., “between the sofa and the TV”) | **Open-vocabulary / zero-shot** via CLIP and LSeg feature alignment | ✓ (Compatible with frontier-based exploration and multi-robot map sharing; generates open-vocab obstacle maps for different embodiments) | Enables spatial reasoning beyond object-centric goals (e.g., “3 m to the right of the chair”); outperforms CoW and LM-Nav in SR (59%) and SPL; sensitive to reconstruction noise, odometry drift, and similar-object ambiguity. |
| **PIGEON (VLM-Driven Object Navigation via Points of Interest Selection)** | ✓ *(RLVR fine-tuning on Qwen-2.5-VL after zero-shot initialization)* | ✓ *(8.8 Hz on Jetson Orin AGX; ~12 GB VRAM)* | RGB-D (+ optional LiDAR or stereo) | **PIGEON-VL (Qwen-2.5-VL + GPT-4o + SAM + YOLOv7 + GroundingDINO)** | **PoI Snapshot Memory:** compact sequence of navigable Points of Interest storing multi-view RGB, pose, and visual semantics:contentReference[oaicite:4]{index=4}:contentReference[oaicite:5]{index=5} | Text + Image (object-level instructions, e.g., “find potted plant”) | **Open-vocabulary (zero-shot)** via VLM alignment + RLVR reasoning | ✓ *(PoI-based exploration with reasoning-aware frontier selection; integrates low-level A* and RNN navigation)*:contentReference[oaicite:6]{index=6} | Outperforms all prior zero-shot baselines on HM3D and MP3D (+11% SR); memory-efficient and reasoning-capable; PoI-based memory outperforms frontier-based methods; relies on VLM inference (GPT-4o) and RLVR reward design; 2D-only navigation (no multi-floor reasoning) |