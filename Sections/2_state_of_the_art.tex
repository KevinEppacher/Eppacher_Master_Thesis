% chktex-file 44

\chapter{State of the Art}
\label{ch:sota}


In this chapter, the current state of research in semantic multi-object search, map reconstruction, and object detection is reviewed. The goal is to identify strengths and limitations of existing methods and establish the technological context for the proposed hybrid approach. The chapter is divided into three key areas: approaches for searching multiple objects semantically, techniques for building and maintaining persistent semantic maps, and recent advances in object detection and promptable models for open-vocabulary tasks.

\section{Geometric Exploration}

A \textit{frontier} is defined as the boundary between known free space and unknown regions of the environment~\cite{lluviaActiveMappingRobot2021}. Frontier-based exploration relies on the principle that unexplored areas adjacent to known free regions provide the highest potential information gain. To identify such frontiers, the robot must maintain a global representation of the environment, typically an occupancy grid or voxel map, where each cell is classified as \textit{free}, \textit{occupied}, or \textit{unknown}, based on sensor observations from \ac{LiDAR} or RGB-D cameras. The robot then iteratively selects and navigates to frontiers to expand its knowledge of the environment.

\citeauthor{quinApproachesEfficientlyDetecting2021}~\cite{quinApproachesEfficientlyDetecting2021} evaluated three commonly used frontier extraction methods that differ primarily in computational efficiency and scalability.  
The first approach, known as the \textit{Naïve Active Area (NaïveAA)} method, evaluates every cell in the occupancy grid to determine whether it is free and has at least one unknown neighbor. Although this approach is conceptually simple and accurate for small-scale maps, it becomes computationally expensive for larger environments and often produces small, fragmented frontier clusters.

The second approach, the \textit{Wavefront Frontier Detector (WFD)}~\cite{quinApproachesEfficientlyDetecting2021,topiwalaFrontierBasedExploration2018}, improves efficiency by using a breadth-first search (BFS) to identify connected frontier regions without exhaustively scanning the entire map. Unlike the NaïveAA method, WFD directly extracts continuous frontier clusters rather than treating each frontier cell individually, significantly reducing redundant computations.

The third method, the \textit{Frontier-Tracing Frontier Detection (FTFD)}~\cite{quinApproachesEfficientlyDetecting2021}, further enhances performance by incrementally updating frontier information using only the most recent sensor observations. Instead of re-evaluating the full map, FTFD initiates a BFS from previously known frontier cells that remain within the active area and from the endpoints of the latest sensor rays. Newly visible free-space cells along the scan boundary are evaluated as potential frontiers, while outdated frontier cells that are now occupied or re-observed are removed. By restricting computation to the local scan perimeter, FTFD achieves significantly faster update rates than NaïveAA and WFD, supporting real-time frontier detection even in large-scale environments.

After frontiers have been extracted, a selection strategy determines which frontier the robot should explore next. Simple heuristics such as \textit{nearest-frontier selection} minimize travel distance but can lead to oscillatory behavior between nearby frontiers. Alternatively, selecting the \textit{largest frontier} favors unexplored regions of higher spatial extent, reducing dead-end visits but increasing traversal cost.  
To address these trade-offs, \citeauthor{bourgaultInformationBasedAdaptive2002}~\cite{bourgaultInformationBasedAdaptive2002} introduced a \textit{utility-based frontier selection} framework that combines multiple criteria, such as distance, frontier size, and expected information gain, into a unified objective function. This approach enables more balanced decision-making, improving overall exploration efficiency and map completeness. Many subsequent works have built upon this foundation, incorporating additional factors such as energy consumption, obstacle density, and dynamic environment considerations into the utility function~\cite{lluviaActiveMappingRobot2021}.

However, all these methods are primarily designed for geometric exploration without incorporating semantic understanding. As a result, they are optimized for complete map coverage rather than goal-directed exploration tasks. In scenarios where a robot must locate specific objects or regions based on semantic cues, purely geometric frontier selection often leads to inefficient search behavior and unnecessary traversal. This motivates the integration of semantic reasoning into the frontier-based exploration process, where frontiers can be prioritized not only by geometric utility but also by their semantic relevance to the task objective.

\section{Vision-Language-Guided Exploration}

In contrast to geometric exploration, which aims to maximize map coverage using the shortest possible path and time, semantic exploration focuses on efficiently locating specific objects or regions of interest described in high-level semantic terms. The objective is to minimize path length and exploration time while prioritizing areas likely to contain relevant targets rather than achieving complete spatial coverage.  

Recent research has introduced \ac{DRL} and supervised learning approaches to train agents capable of performing semantic object search~\cite{majumdarZSONZeroShotObjectGoal2023,ramakrishnanPONIPotentialFunctions2022,ramrakhyaPIRLNavPretrainingImitation2023}.  
\citeauthor{majumdarZSONZeroShotObjectGoal2023}~\cite{majumdarZSONZeroShotObjectGoal2023} proposed \ac{ZSON}, a zero-shot object navigation framework that leverages the multimodal embedding space of CLIP~\cite{radfordLearningTransferableVisual2021} to guide exploration policies trained via reinforcement learning.  
During training, the agent’s observation space consists of previous actions and RGB images, while the navigation goal is represented by the CLIP embedding of an image containing the target object. The reward function is defined as:

\begin{equation}
r_t = r_{\text{success}} + r_{\text{angle-success}} - \Delta d_{tg} - \Delta a_{tg} + r_{\text{slack}},
\end{equation}

where $r_{\text{success}}$ provides a large positive reward for successfully locating the target object,  
$r_{\text{angle-success}}$ rewards correct orientation toward the object,  
$\Delta d_{tg}$ penalizes increased distance to the target,  
$\Delta a_{tg}$ penalizes angular deviation,  
and $r_{\text{slack}}$ applies a small negative reward to encourage efficiency.  
At inference time, the image-based embedding is replaced by the CLIP embedding of a text description of the target object, enabling zero-shot generalization to unseen objects.  
The agent’s action space includes four discrete actions: \textit{move forward}, \textit{turn left}, \textit{turn right}, and \textit{stop}.

While \ac{ZSON} demonstrates strong zero-shot generalization and effective object localization within trained domains, it lacks explicit mechanisms for structured exploration in unknown regions. Since the learned policy primarily directs the agent toward locations visually similar to previously encountered objects, it may become trapped in familiar areas and fail to discover new regions. Moreover, the reliance on \ac{DRL} training reduces interpretability and generalizability; retraining is often required when visual conditions, environment layouts, or sensor modalities differ significantly from the training distribution.

\citeauthor{ramakrishnanPONIPotentialFunctions2022}~\cite{ramakrishnanPONIPotentialFunctions2022} proposed \ac{PONI}, a map-based object-goal navigation framework that decouples high-level exploration decisions from low-level navigation control. \ac{PONI} learns a potential-field network that operates on a partial allocentric semantic map, where free space, occupied space, unknown regions, and detected object categories are encoded in distinct semantic channels. The network predicts two dense potential fields aligned with the current map: an \textit{area potential} \(U_t^a\), which promotes the exploration of large unexplored regions, and an \textit{object potential} \(U_t^o\), which estimates the likelihood of proximity to the target object.

The potential-field network is trained in a fully supervised, interaction-free manner using annotated semantic maps from simulated environments. During training, ground-truth area and object potentials are computed offline by exploiting complete knowledge of free space and object locations. The model is optimized to regress these potentials using a pixel-wise loss applied to frontier cells.  
At inference time, the network predicts both potentials from the current partial map without access to ground-truth information. Geometric frontiers are extracted from the occupancy map and scored by sampling the predicted potentials according to:

\begin{equation}
U_t(f) = \alpha \, U_t^a(f) + (1 - \alpha) \, U_t^o(f),
\end{equation}

where \(\alpha\) controls the trade-off between exploration and exploitation.  
The frontier with the highest score is selected as the next navigation goal, which is then reached using a deterministic path planner rather than a learned action policy.  

While \ac{PONI} achieves efficient and interpretable exploration behavior, it depends on category-level semantic inputs and assumes a fixed, closed set of object classes encountered during training. Consequently, it cannot generalize to unseen categories or open-vocabulary settings, and its reliance on dense semantic annotations limits applicability in real-world scenarios where such data are unavailable or noisy~\cite{ramakrishnanPONIPotentialFunctions2022}.


% \begin{table}[H]
%     \centering
%     \footnotesize
%     \renewcommand{\arraystretch}{1.0}
%     \setlength{\tabcolsep}{3pt}
%     \begin{tabularx}{\textwidth}{
%         |>{\raggedright\arraybackslash}X
%         |>{\centering\arraybackslash}X
%         |>{\centering\arraybackslash}X
%         |>{\raggedright\arraybackslash}X
%         |>{\raggedright\arraybackslash}X|}
%         \hline
%         \textbf{Approach} & \textbf{Training Required} & \textbf{Real-Time} & \textbf{Memory Representation} & \textbf{Exploration Integration} \\ \hline

%     \end{tabularx}
%     \caption{Overview of persistent or memory-based semantic mapping approaches.}
%     \label{tab:introduction:semantic_navigation_overview}
% \end{table}

























\begin{itemize}
    \item Review of methods targeting simultaneous or sequential search for multiple objects in unknown environments.
    \item Analysis of \dots
    \begin{itemize}
        \item VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}
        \item SemUtil~\cite{chen2023semutil}
        \item ESC~\cite{liu2023esc}
        \item LGX~\cite{shah2024lgx}
        \item CoW~\cite{qi2023cow}
        \item ZSON~\cite{shah2023zson}
        \item PONI~\cite{qi2024poni}
        \item PIRLNav~\cite{ma2024pirlnav}
    \end{itemize}

    regarding aspects such as:
    \begin{itemize}
        \item Training Required (Pretrained vs. Fine-tuned)
        \item Real-Time Capability
        \item VRAM Requirements
        \item Sensor Modalities (RGB, RGB-D, LiDAR)
        \item Semantic reasoning (BLIP2, CLIP, GPT-4V, etc.)
        \item Persistent Memory (None, 2D Map, 3D Map, Scene Graph)
    \end{itemize}

    \item Evaluation of performance metrics used in multi-object search:
    \begin{itemize}
        \item Success Rate (SR) 
        \item Success weighted by Path Length (SPL)
        \item MSR (Multi-object Success Rate)
    \end{itemize}
    \item Discussion of semantic exploration frameworks combining language models with spatial reasoning.
    \item Challenges of maintaining semantic context across multiple targets.
\end{itemize}

\section{Map Reconstruction and Persistent Semantic Mapping}
\begin{itemize}
    \item Overview of approaches to build and update semantic maps during exploration:
    \begin{itemize}
        \item 2D grid maps
        \item Pointclouds
        \item Voxel grids
        \item Octomaps
        \item Scene graphs
        \item Neural Radiance Fields (NeRFs)
        \item Feature Fields
    \end{itemize}
    \item Techniques for fusing sensor data into persistent 2D/3D representations:
    \begin{itemize}
        \item Storing Visual Embeddings (e.g., CLIP features) in 3D maps for semantic querying.
        \item Incremental updating of semantic labels based on new observations.
        \item Handling uncertainty and conflicting detections over time.
    \end{itemize}
    \item Comparison of representations (Octomaps, point clouds, voxel grids) in terms of:
    \begin{itemize}
        \item Memory efficiency.
        \item Ability to store semantic labels persistently.
    \end{itemize}
    \item Discussion of \dots
    \begin{itemize}
        \item ConceptGraphs
        \item ConceptGraph-Online
        \item OpenFusion
        \item Clio
        \item OpenScene
        \item GeFF
        \item CLIP-Fields
        \item ConceptFusion
        \item VLMaps
        \item LERF
    \end{itemize}
    as examples of global 3D semantic maps.
    \item Limitations in updating or correcting the map after wrong detections.
\end{itemize}