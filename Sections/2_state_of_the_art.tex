% chktex-file 44

\chapter{State of the Art}
\label{ch:sota}

In this chapter, the current state of research in semantic multi-object search, map reconstruction, and object detection is reviewed. The goal is to identify strengths and limitations of existing methods and establish the technological context for the proposed hybrid approach. The chapter is divided into three key areas: approaches for searching multiple objects semantically, techniques for building and maintaining persistent semantic maps, and recent advances in object detection and promptable models for open-vocabulary tasks.

\section{Geometric Exploration}

A \textit{frontier} is defined as the boundary between known free space and unknown regions of the environment~\cite{lluviaActiveMappingRobot2021}. Frontier-based exploration relies on the principle that unexplored areas adjacent to known free regions provide the highest potential information gain. To identify such frontiers, the robot must maintain a global representation of the environment, typically an occupancy grid or voxel map, where each cell is classified as \textit{free}, \textit{occupied}, or \textit{unknown}, based on sensor observations from \ac{LiDAR} or RGB-D cameras. The robot then iteratively selects and navigates to frontiers to expand its knowledge of the environment.

\citeauthor{quinApproachesEfficientlyDetecting2021}~\cite{quinApproachesEfficientlyDetecting2021} evaluated three commonly used frontier extraction methods that differ primarily in computational efficiency and scalability.  
The first approach, known as the \textit{Na誰ve Active Area (Na誰veAA)} method, evaluates every cell in the occupancy grid to determine whether it is free and has at least one unknown neighbor. Although this approach is conceptually simple and accurate for small-scale maps, it becomes computationally expensive for larger environments and often produces small, fragmented frontier clusters.

The second approach, the \textit{Wavefront Frontier Detector (WFD)}~\cite{quinApproachesEfficientlyDetecting2021,topiwalaFrontierBasedExploration2018}, improves efficiency by using a breadth-first search (BFS) to identify connected frontier regions without exhaustively scanning the entire map. Unlike the Na誰veAA method, WFD directly extracts continuous frontier clusters rather than treating each frontier cell individually, significantly reducing redundant computations.

The third method, the \textit{Frontier-Tracing Frontier Detection (FTFD)}~\cite{quinApproachesEfficientlyDetecting2021}, further enhances performance by incrementally updating frontier information using only the most recent sensor observations. Instead of re-evaluating the full map, FTFD initiates a BFS from previously known frontier cells that remain within the active area and from the endpoints of the latest sensor rays. Newly visible free-space cells along the scan boundary are evaluated as potential frontiers, while outdated frontier cells that are now occupied or re-observed are removed. By restricting computation to the local scan perimeter, FTFD achieves significantly faster update rates than Na誰veAA and WFD, supporting real-time frontier detection even in large-scale environments.

After frontiers have been extracted, a selection strategy determines which frontier the robot should explore next. Simple heuristics such as \textit{nearest-frontier selection} minimize travel distance but can lead to oscillatory behavior between nearby frontiers. Alternatively, selecting the \textit{largest frontier} favors unexplored regions of higher spatial extent, reducing dead-end visits but increasing traversal cost.  
To address these trade-offs, \citeauthor{bourgaultInformationBasedAdaptive2002}~\cite{bourgaultInformationBasedAdaptive2002} introduced a \textit{utility-based frontier selection} framework that combines multiple criteria, such as distance, frontier size, and expected information gain, into a unified objective function. This approach enables more balanced decision-making, improving overall exploration efficiency and map completeness. Many subsequent works have built upon this foundation, incorporating additional factors such as energy consumption, obstacle density, and dynamic environment considerations into the utility function~\cite{lluviaActiveMappingRobot2021}.

However, all these methods are primarily designed for geometric exploration without incorporating semantic understanding. As a result, they are optimized for complete map coverage rather than goal-directed exploration tasks. In scenarios where a robot must locate specific objects or regions based on semantic cues, purely geometric frontier selection often leads to inefficient search behavior and unnecessary traversal. This motivates the integration of semantic reasoning into the frontier-based exploration process, where frontiers can be prioritized not only by geometric utility but also by their semantic relevance to the task objective.

\section{Vision-Language-Guided Exploration}
\begin{itemize}
    \item Review of methods targeting simultaneous or sequential search for multiple objects in unknown environments.
    \item Analysis of \dots
    \begin{itemize}
        \item VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}
        \item SemUtil~\cite{chen2023semutil}
        \item ESC~\cite{liu2023esc}
        \item LGX~\cite{shah2024lgx}
        \item CoW~\cite{qi2023cow}
        \item ZSON~\cite{shah2023zson}
        \item PONI~\cite{qi2024poni}
        \item PIRLNav~\cite{ma2024pirlnav}
    \end{itemize}

    regarding aspects such as:
    \begin{itemize}
        \item Training Required (Pretrained vs. Fine-tuned)
        \item Real-Time Capability
        \item VRAM Requirements
        \item Sensor Modalities (RGB, RGB-D, LiDAR)
        \item Semantic reasoning (BLIP2, CLIP, GPT-4V, etc.)
        \item Persistent Memory (None, 2D Map, 3D Map, Scene Graph)
    \end{itemize}

    \item Evaluation of performance metrics used in multi-object search:
    \begin{itemize}
        \item Success Rate (SR) 
        \item Success weighted by Path Length (SPL)
        \item MSR (Multi-object Success Rate)
    \end{itemize}
    \item Discussion of semantic exploration frameworks combining language models with spatial reasoning.
    \item Challenges of maintaining semantic context across multiple targets.
\end{itemize}

\section{Map Reconstruction and Persistent Semantic Mapping}
\begin{itemize}
    \item Overview of approaches to build and update semantic maps during exploration:
    \begin{itemize}
        \item 2D grid maps
        \item Pointclouds
        \item Voxel grids
        \item Octomaps
        \item Scene graphs
        \item Neural Radiance Fields (NeRFs)
        \item Feature Fields
    \end{itemize}
    \item Techniques for fusing sensor data into persistent 2D/3D representations:
    \begin{itemize}
        \item Storing Visual Embeddings (e.g., CLIP features) in 3D maps for semantic querying.
        \item Incremental updating of semantic labels based on new observations.
        \item Handling uncertainty and conflicting detections over time.
    \end{itemize}
    \item Comparison of representations (Octomaps, point clouds, voxel grids) in terms of:
    \begin{itemize}
        \item Memory efficiency.
        \item Ability to store semantic labels persistently.
    \end{itemize}
    \item Discussion of \dots
    \begin{itemize}
        \item ConceptGraphs
        \item ConceptGraph-Online
        \item OpenFusion
        \item Clio
        \item OpenScene
        \item GeFF
        \item CLIP-Fields
        \item ConceptFusion
        \item VLMaps
        \item LERF
    \end{itemize}
    as examples of global 3D semantic maps.
    \item Limitations in updating or correcting the map after wrong detections.
\end{itemize}

\section{Object Detection and Promptable Models}
\begin{itemize}
    \item Review of traditional and open-vocabulary object detection methods.
    \item Analysis of grounding-capable detectors and segmentation models for zero-shot tasks.
    \item Specific evaluation of the following models for their suitability in semantic multi-object search:
    \begin{itemize}
        \item YOLOv7
        \item GroundingDINO
        \item MobileSAM
        \item GroundedSAM
        \item SEEM
        \item OWL-ViT
        \item MaskDINO
    \end{itemize}
    \item Discussion of promptable vision-language models supporting multi-modal queries (text, image, audio).
    \item Challenges with false positives in zero-shot settings and their implications for reliable multi-object detection.
\end{itemize}

