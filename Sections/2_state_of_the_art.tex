% chktex-file 44

\chapter{State of the Art}
\label{ch:sota}

This work introduces a hybrid semantic exploration framework that combines open-vocabulary
semantic perception with autonomous exploration and persistent spatial memory, which combines the fields of geometric exploration, vision-language-guided exploration, and semantic mapping based on the successes encountered by incorporating semantic information from large pretrained foundation models into traditional exploration and mapping pipelines. However, these novel semantic methods still rely on design and data representation patterns derived from classic geometric exploration and mapping methods.

Semantic exploration approaches differ fundamentally in how exploration behavior is generated.
Some methods learn end-to-end navigation policies using reinforcement learning or imitation learning,
where exploration strategies are implicitly encoded in a trained policy optimized via task-specific
reward functions~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023}. Other approaches adopt modular architectures that integrate pretrained vision-language models with classical mapping and planning techniques~\cite{yokoyamaVLFMVisionLanguageFrontier2023, buschOneMapFind2025,gadreCoWsPastureBaselines2022}.

Reinforcement learning and imitation learning approaches have demonstrated promising results in
semantic object search tasks by training agents to navigate toward target objects based on high-level
semantic cues~\cite{majumdarZSONZeroShotObjectGoal2023}, primarily within simulated environments. Their main advantage lies in avoiding hand-designed exploration heuristics, as complex behaviors can be learned directly from data through reward optimization~\cite{ghasemiComprehensiveSurveyReinforcement2025}. However, such policies typically require extensive training on large datasets (e.g. Replica~\cite{straubReplicaDatasetDigital2019}, Habitat~\cite{savvaHabitatPlatformEmbodied2019}), exhibit limited generalization to unseen environments or object~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023} categories, and lack interpretability due to their black-box nature~\cite{ghasemiComprehensiveSurveyReinforcement2025}.

In contrast, modular approaches leverage semantic representations provided by large-scale
pretrained \acp{VLM} to guide exploration decisions without task-specific retraining~\cite{alamaRayFrontsOpenSetSemantic2025}.
By combining explicit geometric mapping with semantic reasoning derived from foundation models,
these systems can achieve zero-shot generalization to novel objects and environments while retaining
interpretability and adaptability~\cite{yokoyamaVLFMVisionLanguageFrontier2023}. Several works extend this paradigm by constructing persistent semantic maps that retain knowledge of previously explored areas~\cite{buschOneMapFind2025,huangVisualLanguageMaps2023}, enabling more efficient multi-object search and the integration of high-level natural language instructions~\cite{guConceptGraphsOpenVocabulary3D2023}.

Finally, object detection and promptable vision-language models play a crucial role in enabling
open-vocabulary semantic understanding for exploration tasks. Recent advances in grounding-capable
detectors and segmentation models facilitate zero-shot object recognition based on text prompts,
allowing robots to identify and localize previously unseen objects~\cite{wangYOLOERealTimeSeeing2025,liGroundedLanguageImagePretraining2022,liuGroundingDINOMarrying2024}. These models form the semantic
foundation upon which hybrid exploration systems are built, enabling flexible object
search in diverse real-world environments.

\section{Frontier-Based Exploration}

Frontier-based exploration is a classical and commonly used approach for autonomous mapping and navigation in unknown environments~\cite{lluviaActiveMappingRobot2021}. A \textit{frontier} is defined as the boundary between known free space and unknown regions of the environment~\cite{lluviaActiveMappingRobot2021}. Frontier-based exploration relies on the principle that unexplored areas adjacent to known free regions provide the highest potential information gain. To identify such frontiers, the robot must maintain a global representation of the environment, typically an occupancy grid or voxel map, where each cell is classified as \textit{free}, \textit{occupied}, or \textit{unknown}, based on sensor observations from \ac{LiDAR} or RGB-D cameras. The robot then iteratively selects and navigates to frontiers to expand its knowledge of the environment.

\citeauthor{quinApproachesEfficientlyDetecting2021}~\cite{quinApproachesEfficientlyDetecting2021} evaluated three commonly used frontier extraction methods that differ primarily in computational efficiency and scalability.  
The first approach, known as the \textit{Naïve Active Area (NaïveAA)} method, evaluates every cell in the occupancy grid to determine whether it is free and has at least one unknown neighbor. Although this approach is conceptually simple and accurate for small-scale maps, it becomes computationally expensive for larger environments and often produces small, fragmented frontier clusters.

The second approach, the \textit{Wavefront Frontier Detector (WFD)}~\cite{quinApproachesEfficientlyDetecting2021,topiwalaFrontierBasedExploration2018}, improves efficiency by using a breadth-first search (BFS) to identify connected frontier regions without exhaustively scanning the entire map. Unlike the NaïveAA method, WFD directly extracts continuous frontier clusters rather than treating each frontier cell individually, significantly reducing redundant computations.

The third method, the \textit{Frontier-Tracing Frontier Detection (FTFD)}~\cite{quinApproachesEfficientlyDetecting2021}, further enhances performance by incrementally updating frontier information using only the most recent sensor observations. Instead of re-evaluating the full map, FTFD initiates a BFS from previously known frontier cells that remain within the active area and from the endpoints of the latest sensor rays. Newly visible free-space cells along the scan boundary are evaluated as potential frontiers, while outdated frontier cells that are now occupied or re-observed are removed. By restricting computation to the local scan perimeter, FTFD achieves significantly faster update rates than NaïveAA and WFD, supporting real-time frontier detection even in large-scale environments.

After frontiers have been extracted, a selection strategy determines which frontier the robot should explore next. Simple heuristics such as \textit{nearest-frontier selection} minimize travel distance but can lead to oscillatory behavior between nearby frontiers. Alternatively, selecting the \textit{largest frontier} favors unexplored regions of higher spatial extent, reducing dead-end visits but increasing traversal cost.  
To address these trade-offs, \citeauthor{bourgaultInformationBasedAdaptive2002}~\cite{bourgaultInformationBasedAdaptive2002} introduced a \textit{utility-based frontier selection} framework that combines multiple criteria, such as distance, frontier size, and expected information gain, into a unified objective function. This approach enables more balanced decision-making, improving overall exploration efficiency and map completeness. Many subsequent works have built upon this foundation, incorporating additional factors such as energy consumption, obstacle density, and dynamic environment considerations into the utility function~\cite{lluviaActiveMappingRobot2021}.

However, all these methods are primarily designed for geometric exploration without incorporating semantic understanding. As a result, they are optimized for complete map coverage rather than goal-directed exploration tasks. In scenarios where a robot must locate specific objects or regions based on semantic cues, purely geometric frontier selection often leads to inefficient search behavior and unnecessary traversal. This motivates the integration of semantic reasoning into the frontier-based exploration process, where frontiers can be prioritized not only by geometric utility but also by their semantic relevance to the task objective.

\section{RL-based Semantic Exploration}
\label{sec:sota:rl_semantic_exploration}

In contrast to geometric exploration, which aims to maximize map coverage using the shortest possible path and time, semantic exploration focuses on efficiently locating specific objects or regions of interest described in high-level semantic terms. The objective is to minimize path length and exploration time while prioritizing areas likely to contain relevant targets rather than achieving complete spatial coverage.  

Semantic exploration approaches differ fundamentally in how exploration behavior is generated and optimized. \ac{RL} and imitation learning methods have demonstrated promising results by training agents to navigate toward target objects based on high-level semantic cues~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023,ramakrishnanPONIPotentialFunctions2022}. In these approaches, an agent learns a policy that maps high-dimensional sensory observations to low-level control actions by optimizing a task-specific reward function, rather than relying on explicitly designed exploration heuristics~\cite{ghasemiComprehensiveSurveyReinforcement2025}.

In the context of semantic exploration, \ac{RL}-based methods are typically formulated as Markov Decision Processes or, more commonly in embodied navigation, as partially observable Markov Decision Processes (\acp{POMDP})~\cite{thrunProbabilisticRobotics2006}. At an abstract level, a \ac{POMDP} can be defined as a tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where $\mathcal{S}$ denotes the latent environment states, $\mathcal{A}$ the set of possible actions (e.g., move forward, turn left/right), $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ the state transition probability function, $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ the reward function, and $\gamma \in [0,1)$ the discount factor. The agent does not observe the full state directly but instead receives high-dimensional, partial observations (e.g., RGB images, depth measurements, or semantic representations), from which it must infer an internal belief about the environment~\cite{ghasemiComprehensiveSurveyReinforcement2025}.

The reward function encodes task objectives such as reaching a target object, minimizing
path length, or maintaining correct orientation, thereby implicitly shaping the agent’s
exploration behavior~\cite{ghasemiComprehensiveSurveyReinforcement2025}.
This paradigm has been successfully applied to semantic object-goal navigation, where agents
are trained to locate objects specified by high-level semantic descriptions
(e.g., object categories or language embeddings)~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023,ramakrishnanPONIPotentialFunctions2022}.

A key advantage of \ac{RL}-based semantic exploration lies in its flexibility. Complex navigation behaviors can be learned directly from interaction data without manually designing exploration strategies. However, this flexibility comes at the cost of extensive training requirements, limited interpretability, and reduced robustness to domain shifts, as policies often overfit to the visual statistics and dynamics of the training environments~\cite{ghasemiComprehensiveSurveyReinforcement2025,majumdarZSONZeroShotObjectGoal2023}. As a result, many such approaches are primarily evaluated in simulation and struggle to generalize to previously seen scenes, object appearances, or sensor configurations. Commonly, \ac{RL} algorithms are optimized via policy gradient methods~\cite{thomasPolicyGradientMethods2017}, which directly adjust the policy parameters $\theta$ to maximize the expected cumulative reward $J(\theta)$:

\begin{equation}
\nabla_{\theta} J(\theta) =
\mathbb{E}_{\tau \sim \pi_{\theta}}
\left[
\sum_{t=0}^{T}
\nabla_{\theta} \log \pi_{\theta}(a_t \mid o_t) \, R(\tau)
\right],
\end{equation}

where $\tau$ denotes trajectories sampled from the policy $\pi_{\theta}$, $a_t$ and $o_t$ are the action and observation at time $t$, respectively, and $R(\tau)$ is the cumulative reward obtained over trajectory $\tau$.

% ---------------------- ZSON ----------------------

\citeauthor{majumdarZSONZeroShotObjectGoal2023}~\cite{majumdarZSONZeroShotObjectGoal2023} proposed \ac{ZSON}, a zero-shot object navigation framework that leverages the shared embedding space of CLIP~\cite{radfordLearningTransferableVisual2021} to guide navigation policies trained via \ac{RL}. 
During training, the agent observes RGB images and previous actions, while the navigation goal is specified by the CLIP embedding of an image containing the target object. 
The policy is optimized using a shaped reward function:

\begin{equation}
r_t = r_{\text{success}} + r_{\text{angle-success}} - \Delta d_{tg} - \Delta a_{tg} + r_{\text{slack}},
\end{equation}

where $r_{\text{success}}$ rewards successful target localization, $r_{\text{angle-success}}$ encourages correct orientation, $\Delta d_{tg}$ and $\Delta a_{tg}$ penalize distance and angular deviation, respectively, and $r_{\text{slack}}$ promotes efficient navigation. 
At inference time, the image-based goal embedding is replaced by the CLIP embedding of a textual object description, enabling zero-shot generalization to unseen object categories. 
The action space is discrete and consists of \textit{move forward}, \textit{turn left}, \textit{turn right}, and \textit{stop}. The reward function is used exclusively during training to learn the navigation policy during inference, the agent selects actions solely based on the learned policy without
access to the reward signal.

While \ac{ZSON} demonstrates strong zero-shot object generalization, exploration behavior remains implicitly encoded in the learned policy, which makes it difficult to interpret or adapt to new scenarios. As a consequence, the agent tends to revisit visually familiar regions and lacks explicit mechanisms for systematic exploration of unknown space. Furthermore, the end-to-end \ac{RL} formulation reduces interpretability and necessitates retraining when visual conditions or environment layouts deviate from the training distribution. Nevertheless, \ac{ZSON} represents a significant step toward flexible and generalizable semantic exploration by integrating vision-language models with reinforcement learning.

% ---------------------- PIRLNav ----------------------

\citeauthor{ramrakhyaPIRLNavPretrainingImitation2023}~\cite{ramrakhyaPIRLNavPretrainingImitation2023} introduced \ac{PIRLNav}, a two-stage framework that combines behavior cloning (\ac{BC}) with \ac{RL} to improve generalization in semantic navigation tasks. In the first stage, a navigation policy is pretrained via imitation learning on large-scale human demonstrations, learning to reproduce expert actions from observations that include RGB images, pose information, and a categorical goal representation. This pretraining stage provides a strong initialization, which reduces the training time and sample complexity required for subsequent reinforcement learning. Furthermore, with respect to interpretability, the use of \ac{BC} allows for some insight into the learned behavior, as it is directly derived from human demonstrations. The pre-trained ObjectNav humand demonstraded policy is trained using supervised learning to minimize the cross-entropy loss between predicted and expert actions:

\begin{equation}
\mathcal{L}_{\text{BC}} = - \sum_{t=1}^{T} \log \pi(a_t^* \mid s_t),
\end{equation}

where \(a_t^*\) denotes the expert action at time \(t\) and \(s_t\) represents the agent's observation. The ObjectNav policy is trained with a CNN+RNN Network, in which a pre-trained vision backbone, \ac{DINO}, extracts visual features from RGB images and feeds into the nav policy network along with pose and goal information.

In the second stage, the pretrained policy is fine-tuned using \ac{RL}, where the objective is to maximize the expected discounted reward:

\begin{equation}
\pi^{*} = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} 
\left[ \sum_{t=1}^{T} \gamma^{t-1} r_t \right].
\end{equation}

where \(\tau\) denotes trajectories sampled from the policy \(\pi\), \(r_t\) is the reward at time \(t\), and \(\gamma\) is the discount factor. Although this hybrid training strategy improves sample efficiency and navigation robustness in simulation, the resulting policy remains a black-box model. 
It does not maintain an explicit semantic memory and requires retraining to adapt to new visual domains or sensor modalities.

% ---------------------- PONI ----------------------

To address the limited interpretability of end-to-end policies, \citeauthor{ramakrishnanPONIPotentialFunctions2022}~\cite{ramakrishnanPONIPotentialFunctions2022} proposed \ac{PONI}, a supervised, map-based semantic exploration framework. Rather than learning low-level actions, \ac{PONI} predicts high-level exploration objectives in the form of potential fields defined over a partial semantic map. Two complementary potentials are estimated: an area potential that encourages exploration of unknown space, and an object potential that estimates proximity to the target category. Exploration decisions are derived by scoring geometric frontiers according to a weighted combination of these potentials:

\begin{equation}
U_t(f) = \alpha \, U_t^a(f) + (1 - \alpha) \, U_t^o(f),
\end{equation}

where $\alpha$ explicitly controls the trade-off between exploration and exploitation. 
This formulation yields interpretable and stable exploration behavior and decouples high-level decision-making from low-level navigation, which is handled by a classical navigation planner.

However, \ac{PONI} relies on dense semantic annotations and assumes a fixed, closed set of object categories encountered during training.~\cite{ramakrishnanPONIPotentialFunctions2022} used Mask~R-CNN~\cite{heMaskRCNN2018} to generate semantic maps with 80 object classes from the \ac{COCO} dataset~\cite{linMicrosoftCOCOCommon2015}, which was finetuned within the Gibson dataset~\cite{xiaGibsonEnvRealWorld2018}. As a result, it does not support open-vocabulary or zero-shot object search and remains sensitive to annotation noise and domain shifts.

RL-based and supervised semantic exploration methods demonstrate the feasibility of learning navigation behavior from semantic cues, but they exhibit recurring limitations that motivate alternative approaches. These include extensive training requirements, limited interpretability, closed-set semantics, lack of persistent semantic memory, and reduced robustness to domain changes. 
These structural shortcomings have motivated recent work toward modular exploration frameworks that integrate pretrained vision-language models, which are discussed in the following section.

\section{Foundation-Model-Based Semantic Exploration}
\label{sec:sota:foundation_models}

In contrast to reinforcement learning-based approaches, which derive navigation behavior through task-specific training, a second line of work leverages large-scale pretrained \acp{VLM} as semantic priors within modular exploration systems. These approaches shift the learning burden away from navigation policy optimization toward semantic perception and reasoning, enabling zero-shot generalization to novel objects and environments without task-specific retraining.

\acp{VLM} are large pretrained image-text models that learn joint representations of visual and linguistic data from massive web-scale datasets~\cite{radfordLearningTransferableVisual2021,liGroundedLanguageImagePretraining2022}. Their core principle is to embed images and text into a shared latent space, in which semantically
related visual and linguistic concepts are mapped to nearby representations.
A \ac{VLM} defines two embedding functions,
\(
f_I(\cdot)
\)
and
\(
f_T(\cdot),
\)
which map an image \(I\) and a text prompt \(T\) to a common embedding space
\(\mathbb{R}^d\):

\begin{equation}
\mathbf{e}_I = f_I(I), \qquad \mathbf{e}_T = f_T(T),
\end{equation}

where \(\mathbf{e}_I, \mathbf{e}_T \in \mathbb{R}^d\) are high-dimensional feature vectors encoding semantic information. Text inputs are tokenized and processed using transformer-based language encoders, while visual
inputs are decomposed into patches or regions and encoded by a vision backbone (e.g., \ac{CNN} or transformer-based architectures), depending on the model design~\cite{vaswaniAttentionAllYou2023,liBLIP2BootstrappingLanguageImage2023,liGroundedLanguageImagePretraining2022}. Semantic alignment between image and text embeddings is commonly quantified using cosine similarity, which measures the angular similarity between vectors in the shared embedding space (Equation~\ref{eq:vlm_cosine_similarity}):

\begin{equation}
\label{eq:vlm_cosine_similarity}
\mathrm{sim}(I, T)
=
\frac{\mathbf{e}_I \cdot \mathbf{e}_T}
{\lVert \mathbf{e}_I \rVert \, \lVert \mathbf{e}_T \rVert}.
\end{equation}

A higher similarity score indicates stronger semantic correspondence between the visual observation
and the textual query. This representation enables open-vocabulary reasoning, as arbitrary object descriptions can be
matched against visual observations without retraining, forming the foundation for zero-shot semantic perception in exploration tasks.

The defining characteristic of foundation-model-based exploration is the explicit separation between geometric navigation and semantic understanding. Geometric structure is typically handled by classical mapping and planning components (e.g., frontier-based exploration), while semantic relevance is inferred from pretrained models such as CLIP, BLIP-2, or grounding-capable detectors~\cite{gadreCoWsPastureBaselines2022,yokoyamaVLFMVisionLanguageFrontier2023,liuGroundingDINOMarrying2024}. This modularity improves interpretability and adaptability, but introduces new challenges related to uncertainty handling, semantic consistency, and long-term memory.

Table~\ref{tab:sota:foundation_design_patterns} summarizes representative foundation-model-based exploration frameworks in five categories: the source of semantic signals, the mechanism used to fuse semantics with geometric exploration, the handling of detection confidence and uncertainty, the underlying semantic data representation, and the dominant failure causalities observed in practice.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{4pt}
    \begin{tabularx}{\textwidth}{
    |l|X|X|X|X|X|}
    \hline
    \textbf{Method} 
    & \textbf{Semantic signal source} 
    & \textbf{Fusion with geometry} 
    & \textbf{Detection confidence handling} 
    & \textbf{Semantic representation} 
    & \textbf{Primary failure causality} \\
    \hline

    \textbf{ESC~\cite{zhouESCExplorationSoft2023}}
    & Object detections + \ac{LLM} priors
    & Probabilistic frontier scoring (PSL)
    & Single-step confidence, no revision
    & Local symbolic semantic map
    & False positives amplified by reasoning priors \\
    \hline

    \textbf{CoW~\cite{gadreCoWsPastureBaselines2022}}
    & Image-text similarity \ac{CLIP}
    & No explicit geometric fusion
    & Threshold-based similarity
    & No explicit map
    & Oscillation and local minima near false positives \\
    \hline

    \textbf{SemUtil~\cite{chenHowNotTrain2023}}
    & Closed-set detections + \ac{CLIP} similarity
    & Utility map over geometric frontiers
    & No confidence decay or belief update
    & Semantic point cloud + scene graph
    & Persistent corruption from misdetections \\
    \hline

    \textbf{VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}}
    & Dense image-text similarity~\ac{BLIP}
    & Semantic value-map fused with frontier map
    & Weighted averaging over observations
    & Episodic semantic value map
    & False positives persist across episode \\
    \hline

    \end{tabularx}
    \caption{Design patterns and limitations of foundation-model-based semantic exploration frameworks.
    The table compares how different methods obtain semantic signals, integrate them with geometric exploration,
    handle uncertainty over time, and represent semantic information, revealing recurring failure modes
    that motivate the need for persistent semantic memory and belief revision.}
    \label{tab:sota:foundation_design_patterns}
\end{table}

A representative example of this paradigm is the \ac{ESC} framework proposed by \citeauthor{zhouESCExplorationSoft2023}~\cite{zhouESCExplorationSoft2023}, which augments traditional frontier-based exploration with semantic cues derived from pretrained \acp{VLM}. Specifically, \ac{ESC} combines a grounded object detector, \ac{GLIP}~\cite{liGroundedLanguageImagePretraining2022}, with a \ac{LLM} (either ChatGPT or DeBERTa) to generate semantic priors that guide exploration decisions. Frontiers are scored based on both geometric utility and semantic relevance to the target object (see Equation~\ref{eq:esc}).

\begin{equation}
\label{eq:esc}
P(F) = P(F \mid d_i^{t}, o^{t}, r^{t})
\end{equation}

\(P(F)\) denotes the probability of selecting frontier \(F\), conditioned on detected objects \(d_i^{t}\), current image observations \(o^{t}\), and the robot pose \(r^{t}\) at time \(t\). This formulation is implemented using \ac{PSL}, which fuses visual detections with language-derived priors about object co-occurrences and spatial relationships. The robot then selects the frontier with the highest combined score, balancing geometric and semantic information to improve search efficiency. For navigating toward target objects, a classical \ac{A*} planner is employed to compute collision-free paths based on the occupancy map.

~\citeauthor{zhouESCExplorationSoft2023} employs \ac{GLIP}~\cite{liGroundedLanguageImagePretraining2022} as the detection backbone to compute 2D bounding boxes, class labels, and confidence scores for objects within the robot’s \ac{FOV}. While effective in simulation, the approach introduces notable computational overhead due to repeated \ac{LLM} inference and PSL optimization. As a consequence of relying on single-step detections and static commonsense priors, errors introduced by false positives are not attenuated over time. Once a misleading semantic hypothesis is introduced, the probabilistic reasoning layer tends to reinforce rather than correct it, leading to persistent semantic bias during exploration.

Through ablation studies, \citeauthor{zhouESCExplorationSoft2023} observed that object-object and object-room relational priors can occasionally degrade performance, as commonsense relationships are inherently probabilistic rather than deterministic. Additionally, while \ac{ESC} maintains a local semantic map during navigation, it lacks mechanisms for long-term memory or belief revision. Consequently, once an incorrect detection or prior is introduced, the framework has no learned means of down-weighting or correcting it over time, which can lead to persistent semantic inconsistencies during extended exploration.

In contrast to such \ac{LLM} reasoning-based systems, \citeauthor{gadreCoWsPastureBaselines2022}~\cite{gadreCoWsPastureBaselines2022} proposed \ac{CoW}, a lightweight vision-language exploration framework that relies purely on image-text alignment from CLIP~\cite{radfordLearningTransferableVisual2021} without requiring explicit frontier detection, semantic mapping, or object segmentation. The method guides the robot toward directions with the highest cosine similarity (see Equation~\ref{eq:vlm_cosine_similarity}) between the current visual observation and the target object description.

By eliminating explicit detectors and handcrafted mapping, \ac{CoW} offers a computationally efficient and conceptually simple baseline for open-vocabulary navigation. However, this simplicity comes at the cost of robustness. The system is highly sensitive to viewpoint variations and clutter, as cosine similarity does not always correlate with true object presence. Without spatial memory or geometric reasoning, the robot may oscillate near false positives or become trapped in local minima. Moreover, because similarity scores vary across object categories, no universal threshold can be established for all targets, resulting in inconsistent stopping behavior and reduced reliability during multi-object search.

Building upon this idea of integrating semantics into classical exploration, 
\citeauthor{chenHowNotTrain2023}~\cite{chenHowNotTrain2023} introduced \ac{SemUtil}, 
a fully modular and training-free framework for object-goal navigation that combines classical SLAM-based mapping 
with pretrained perception and language models. 
In contrast to reinforcement learning or imitation learning approaches, 
\ac{SemUtil} leverages explicit geometric and semantic reasoning through three core components: 
a 2D occupancy map for frontier extraction, a semantic point cloud generated by projecting Mask~R-CNN detections into 3D space, 
and a spatial scene graph for high-level semantic reasoning. 
These three representations collectively form a structured scene model that supports geometric planning, semantic propagation, 
and reasoning about unexplored regions~\cite{chenHowNotTrain2023}.  

The central element of \ac{SemUtil} is the \textit{utility module}, 
which fuses geometric frontiers with semantic priors to determine the most promising frontier to explore next. 
For each map cell, a \textit{utility score} is computed by combining the geometric frontier characteristics, 
the CLIP-based cosine similarity between the current observation and the target object description, 
and the semantic cues from the 3D point cloud (e.g., class IDs from Mask~R-CNN). 
This results in a utility map that prioritizes frontiers both spatially and semantically,
as illustrated in Figure~3 of the original paper (showing the interaction between geometric and semantic utilities)~\cite{chenHowNotTrain2023}. \ac{SemUtil} solves the oscillation issues observed in \ac{CoW} by explicitly extracting frontiers from the occupancy map and scoring them based on their semantic utility, rather than relying solely on raw similarity scores. This structured approach enables more stable exploration behavior and reduces the likelihood of becoming trapped near false positives, which also applies to other works, which combine \acp{VLM} with frontier-based exploration~\cite{yokoyamaVLFMVisionLanguageFrontier2023}.
 
Importantly, the utility map in \ac{SemUtil} is not persistent, it is recomputed at every timestep based solely on the current observation and semantic point cloud, without maintaining a long-term memory of past detections or map updates. While this design simplifies computation and eliminates the need for training, it also limits the system’s ability to reason over time or correct previous errors. The reliance on a closed-set detector (Mask~R-CNN) restricts open-vocabulary generalization, and any incorrect detection directly corrupts the semantic point cloud, thereby distorting the frontier scoring and leading to suboptimal exploration decisions.  Furthermore, since the framework lacks belief revision or memory-based fusion, false detections persist until they leave the robot’s current field of view, reducing consistency and efficiency in long-term navigation.

Unlike \citeauthor{chenHowNotTrain2023}~\cite{chenHowNotTrain2023}, who construct a utility map from class-based detections and semantic projections that are recomputed each step, thereby risking information loss or semantic inconsistency, \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023} directly leverages pretrained vision-language models to compute semantic value maps from raw RGB observations. Rather than relying on symbolic object classes, \ac{VLFM} computes a continuous image-text similarity score between the robot’s current observation and the target text prompt using BLIP-2~\cite{liBLIP2BootstrappingLanguageImage2023}, as formulated through the cosine similarity function in Equation~\ref{eq:vlm_cosine_similarity}~\cite{gadreCoWsPastureBaselines2022}.  

The resulting similarity values are spatially projected onto a top-down occupancy grid according to the robot’s \ac{FOV}, forming a \textit{value map} that quantifies the semantic likelihood of each region leading toward the target object. To account for reduced reliability near the image periphery, a Gaussian weighting function attenuates confidence values based on angular distance from the optical axis (see Fig.~3 in the original paper~\cite{yokoyamaVLFMVisionLanguageFrontier2023}). This value map is continuously updated through a weighted averaging scheme that fuses new and previous similarity scores according to their confidence weights, enabling smooth map updates and spatial consistency across frames. \ac{BLIP-2} is not used for caption generation, but rather for retrieving text-image embeddings to compute similarity scores~\cite{liBLIP2BootstrappingLanguageImage2023,yokoyamaVLFMVisionLanguageFrontier2023}.

During exploration, \ac{VLFM} fuses this value map with a geometrically extracted frontier map to select the next exploration goal, the frontier with the highest semantic value is chosen as the next waypoint. Target object detection is performed using YOLOv7~\cite{wangYOLOv7TrainableBagoffreebies2022} for \ac{COCO}~\cite{linMicrosoftCOCOCommon2015} categories and GroundingDINO~\cite{liuGroundingDINOMarrying2024} for open-vocabulary detection. Once an object matching the target query is detected, SAM~\cite{kirillovSegmentAnything2023} is applied to generate an accurate mask, and the system transitions from exploration to goal navigation.  

This modular framework achieves state-of-the-art performance on the Gibson~\cite{xiaGibsonEnvRealWorld2018}, HM3D, and Matterport3D~\cite{changMatterport3DLearningRGBD2017} benchmarks, outperforming prior zero-shot approaches such as \ac{ESC}, \ac{SemUtil}, and \ac{CoW} in both \ac{SR} and \ac{SPL} \cite{yokoyamaVLFMVisionLanguageFrontier2023}. Despite its efficiency and interpretability, several limitations remain. \ac{VLFM} relies on a single-source detection pipeline, either GroundingDINO or YOLOv7, making it prone to false positives in open-vocabulary scenarios, which can result in premature stopping behavior. Furthermore, the value map is episodic rather than persistent: it resets after each navigation episode and does not maintain long-term semantic memory, leading to redundant revisits during multi-object search tasks. While the system operates in real time, the use of multiple large-scale pretrained models (BLIP-2~\cite{liBLIP2BootstrappingLanguageImage2023}, GroundingDINO/YOLOv7~\cite{liuGroundingDINOMarrying2024,wangYOLOv7TrainableBagoffreebies2022}, and SAM~\cite{kirillovSegmentAnything2023}) demands substantial computational resources, consuming approximately 16~GB of VRAM on an NVIDIA RTX~4090 GPU during deployment, which limits scalability on embedded robotic platforms.

Foundation-model-based exploration approaches enable zero-shot semantic navigation without the need for task-specific training and offer improved interpretability compared to learned policies. However, across all reviewed methods, semantic information is either transient, episodic, or locally scoped. None of the approaches maintain a persistent semantic belief that can be revised over time as new evidence is accumulated. This lack of long-term semantic memory leads to repeated exploration, sensitivity to false detections, and inconsistent behavior in multi-object search scenarios.

These recurring limitations motivate the development of exploration frameworks that combine
open-vocabulary semantic perception with persistent, revisable semantic memory, which is the focus of this work.

\section{Map Reconstruction and Persistent Semantic Mapping}
This section reviews state-of-the-art methods for persistent semantic mapping both exploration-driven and mapping-centric settings. Existing methods of saving semantic information within different spatial representations are discussed, along with techniques for updating and revising semantic beliefs over time, with the aim of improving efficiency in multi-object exploration tasks.

\subsection*{Persistent Semantic Mapping for Exploration}
Table~\ref{tab:sota:persistent_mapping_exploration} summarizes recent approaches to persistent semantic mapping for exploration tasks. The table highlights how different methods store semantic information, update it over time, and whether they support belief revision, revealing a common lack of mechanisms for correcting erroneous semantic memories.

\begin{table}[H]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\textwidth}{
|l|X|X|c|X|c|X|}
\hline
\textbf{Method}
& \textbf{Semantic Memory Type}
& \textbf{Spatial Representation}
& \textbf{Open-Vocab}
& \textbf{Update Mechanism}
& \textbf{Belief Revision}
& \textbf{Primary Limitation} \\
\hline

\textbf{OneMap~\cite{buschOneMapFind2025}}
& Open-Vocabulary Belief Map (\ac{CLIP} features)
& 2.5D top-down grid map
& \cmark
& Uncertainty-weighted accumulative fusion
& \xmark
& Irreversible belief fusion leads to semantic drift \\
\hline

\textbf{VLMaps~\cite{huangVisualLanguageMaps2023}}
& Dense per-cell language embeddings (\ac{CLIP}-based~\cite{radfordLearningTransferableVisual2021})
& 2.5D top-down grid map
& \cmark
& Multi-view feature averaging (accumulative fusion)
& \xmark
& Irreversible feature averaging causes semantic noise and ambiguity \\
\hline

\textbf{PIGEON~\cite{PIGEONVLMDrivenObject}}
& Language-conditioned episodic visual memory (\ac{PoI} snapshots)
& 2D geometric exploration map + episodic \ac{PoI} memory
& \cmark
& Episodic accumulation of visual evidence (per episode)
& \xmark
& No persistent semantic belief state or belief revision mechanism \\
\hline

\textbf{DualMap~\cite{jiangDualMapOnlineOpenVocabulary2025}}
& Dual semantic maps (short-term + long-term)
& 2D grid maps
& \cmark
& Dual-stream accumulative fusion
& \xmark
& Long-term map cannot correct early semantic errors \\
\hline

\end{tabularx}
\caption{Comparison of semantic memory representations for exploration.
The table highlights how different methods store semantic information, update it over time,
and whether they support belief revision, revealing a common lack of mechanisms for correcting
erroneous semantic memories.}
\label{tab:sota:persistent_mapping_exploration}
\end{table}

% ---------------------- OneMap, VLMaps ----------------------

A representative approach to building persistent open-vocabulary semantic maps during exploration
is presented by \citeauthor{buschOneMapFind2025}~\cite{buschOneMapFind2025} and
\citeauthor{huangVisualLanguageMaps2023}~\cite{huangVisualLanguageMaps2023}.
Both methods construct a 2.5D top-down grid map in which semantic information is stored at the
cell level in the form of language-aligned visual embeddings, enabling open-vocabulary querying
via image-text similarity.

In both frameworks, incoming \ac{RGB-D} observations are processed by a vision-language model to
extract dense semantic features. In \ac{OneMap}, global \ac{CLIP} image embeddings are projected
into the map using camera intrinsics and extrinsics, while \ac{VLMaps} employs language-driven
semantic segmentation (LSeg) to obtain dense per-pixel language embeddings~\cite{buschOneMapFind2025,huangVisualLanguageMaps2023}. These features are associated with corresponding grid cells in the top-down map by back-projecting depth pixels into 3D space and discretizing them onto the 2.5D grid representation.

As the robot explores, newly observed embeddings are fused with existing map entries in an
accumulative manner. \citeauthor{buschOneMapFind2025} perform uncertainty-aware recursive fusion,
in which observations with higher confidence exert greater influence on the stored semantic
representation, whereas \citeauthor{huangVisualLanguageMaps2023} apply multi-view feature averaging
without explicit uncertainty modeling. In both cases, once semantic features are integrated into the map, they are not selectively down-weighted or removed at later time steps.

Open-vocabulary object querying is performed by comparing stored map embeddings against the
embedding of a text prompt using cosine similarity (see Equation~\ref{eq:vlm_cosine_similarity}).
An object is considered detected if the similarity score in any map cell exceeds a predefined
threshold.
Because this decision relies solely on similarity values rather than explicit object detection
or instance verification, both approaches are sensitive to threshold selection and may produce
false positives in visually cluttered or ambiguous scenes.

Importantly, neither \ac{OneMap} nor \ac{VLMaps} incorporates mechanisms for belief revision or
error correction.
Once incorrect or noisy observations are fused into the map, they persist indefinitely and can
bias future exploration decisions, leading to semantic drift over time~\cite{buschOneMapFind2025,huangVisualLanguageMaps2023}.
Furthermore, the projection of semantic information onto a 2.5D grid discards vertical structure
and instance-level geometry, limiting semantic fidelity in complex environments and preventing
accurate 3D object localization for downstream tasks such as manipulation or grasp planning~\cite{guConceptGraphsOpenVocabulary3D2023}.

% Pigeon

\citeauthor{PIGEONVLMDrivenObject}~\cite{PIGEONVLMDrivenObject} introduced \textit{PIGEON}, a vision-language-model-driven exploration framework that replaces
persistent dense semantic maps with an episodic, object-centric memory abstraction.
Instead of maintaining a globally consistent semantic map, PIGEON represents the environment
as a set of semantically meaningful \textit{\acp{PoI}}, corresponding to geometrically salient
observation locations enriched with visual context.

At each \ac{PoI}, the robot stores a small set of \ac{RGB} observations captured from multiple
viewpoints~\cite{PIGEONVLMDrivenObject}.
Semantic evaluation is deferred to query time, where a vision-language model jointly reasons over the language query and the stored \ac{RGB} observations to assess the semantic relevance of each \ac{PoI}.
These relevance assessments are used to guide \ac{PoI} selection, while low-level navigation between \acp{PoI} is handled by a classical planner. Reinforcement learning is employed to fine-tune the VLM’s \ac{PoI} selection behavior, rather than to directly select navigation actions.

Formally, each \ac{PoI} $p_i$ is defined by a spatial location $\mathbf{x}_i$ and an associated
set of \ac{RGB} observations $\mathcal{I}_i = \{I_{i,1}, \dots, I_{i,K}\}$ captured from different
viewpoints at that location~\cite{PIGEONVLMDrivenObject}.
No semantic labels, object identities, or belief states are stored; semantic interpretation is
performed on demand and is not consolidated into a persistent semantic world model.

In contrast to dense mapping approaches such as \ac{OneMap}~\cite{buschOneMapFind2025} and
\ac{VLMaps}~\cite{huangVisualLanguageMaps2023}, PIGEON does not perform accumulative fusion of
semantic features into a persistent spatial representation.
As a result, it is less susceptible to semantic drift caused by irreversible belief fusion, but
does not support belief revision or long-term semantic consistency across multiple object-search
tasks~\cite{PIGEONVLMDrivenObject}.

Methodologically, PIGEON occupies an intermediate position between dense semantic mapping and
object-centric persistent representations such as DualMap~\cite{jiangDualMapOnlineOpenVocabulary2025}
and ConceptGraphs~\cite{guConceptGraphsOpenVocabulary3D2023}.
It combines episodic visual memory with query-conditioned semantic scoring and reinforcement
learning-based navigation, while deliberately avoiding persistent semantic state estimation.

% ---------------------- DualMap ----------------------

\citeauthor{jiangDualMapOnlineOpenVocabulary2025}~\cite{jiangDualMapOnlineOpenVocabulary2025}
introduced \textit{DualMap}, an object-centric framework for online open-vocabulary exploration
that explicitly separates short-term perceptual observations from long-term semantic memory.
Unlike dense feature-map approaches such as \ac{OneMap}~\cite{buschOneMapFind2025} and
\ac{VLMaps}~\cite{huangVisualLanguageMaps2023}, which store pixel-wise or cell-wise language
embeddings, DualMap reasons over discrete object instances and their spatial relations.

At each timestep, objects are detected and segmented from the RGB image using
YOLO-World~\cite{chengYOLOWorldRealTimeOpenVocabulary2024}.
For each segmented object, a visual embedding is computed from the cropped image using
CLIP’s image encoder~\cite{radfordLearningTransferableVisual2021}.
If a textual label is available, an additional text embedding is obtained from CLIP’s text encoder.
The final object-level semantic representation is computed as a weighted fusion of image and
text embeddings:

\begin{equation}
\mathbf{f}_t
=
\alpha \, \mathbf{f}^{\text{img}}_t
+
(1 - \alpha) \, \mathbf{f}^{\text{text}}_t,
\qquad \alpha = 0.7,
\label{eq:feature_fusion}
\end{equation}


where $\mathbf{f}^{\text{img}}_t$ and $\mathbf{f}^{\text{text}}_t$ denote the image-based and
text-based embeddings at time $t$, respectively.
This object-centric representation enables open-vocabulary semantic reasoning without
requiring dense per-pixel feature storage.

DualMap maintains two complementary semantic maps.
The \textit{local concrete map} stores recently observed object instances as 3D point clouds
with associated semantic embeddings, enabling rapid adaptation to new observations.
In contrast, the \textit{abstract map} serves as a long-term semantic memory and stores only
stable object instances, referred to as \textit{anchors} (e.g., tables, desks, counters),
which are unlikely to change location over time.
Smaller or movable objects are treated as \textit{volatile} and are not permanently stored
in the abstract map~\cite{buschOneMapFind2025}.

Objects are added to or updated in the maps based on a combination of semantic similarity
between embeddings and geometric overlap, measured via the 3D \ac{IoU}
between observed point clouds.
An object is updated from the local concrete map to the abstract map only when its
confidence exceeds a predefined threshold, thereby balancing adaptability with long-term
stability.
Each anchor in the abstract map maintains a list of associated volatile objects that have
been observed in its vicinity, allowing the system to reason about object co-occurrence
without permanently storing potentially transient items~\cite{buschOneMapFind2025}.

During object-goal exploration, DualMap does not directly search the entire map for the target
object. Instead, the language query is embedded using CLIP and matched against the semantic
representations of anchors and their associated volatile objects.
Anchors with high semantic relevance are prioritized as navigation goals, and the robot
navigates toward them using a classical \ac{A*} planner~\cite{kabirEnhancedRobotMotion2023} on the occupancy map. Once the target object is detected again in the local concrete map, exploration terminates.

By separating short-term perceptual memory from long-term semantic anchors, DualMap achieves more structured and interpretable language-guided navigation than dense feature-map approaches. However, semantic information in DualMap is primarily used for exploitation of previously observed anchors, while exploration itself remains purely geometry-driven. The framework does not provide a mechanism or hyperparameter to explicitly balance semantic exploration and exploitation, as semantic reasoning is only applied after anchors have been formed~\cite{jiangDualMapOnlineOpenVocabulary2025}. As a result, DualMap favors semantic exploitation over semantic exploration, which can limit its ability to actively search for objects that have not yet been associated with existing anchors.


\subsection*{Semantic Scene Reconstruction}

\begin{table}[H]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\textwidth}{
|l|X|X|c|X|c|X|}
\hline
\textbf{Method}
& \textbf{Semantic Memory Type}
& \textbf{Spatial Representation}
& \textbf{Open-Vocab}
& \textbf{Update Mechanism}
& \textbf{Belief Revision}
& \textbf{Primary Limitation} \\
\hline

\textbf{ConceptGraphs~\cite{guConceptGraphsOpenVocabulary3D2023}}
& Dense per-cell language embeddings (\ac{CLIP}-based~\cite{radfordLearningTransferableVisual2021})
& 2.5D top-down grid map
& \cmark
& Multi-view feature averaging (accumulative fusion)
& \xmark
& Irreversible feature averaging causes semantic noise and ambiguity \\
\hline

\textbf{Clio~\cite{jiangDualMapOnlineOpenVocabulary2025}}
& Dual semantic maps (short-term + long-term)
& 2D grid maps
& \cmark
& Dual-stream accumulative fusion
& \xmark
& Long-term map cannot correct early semantic errors \\
\hline

\textbf{OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}}
& Open-Vocabulary Belief Map (\ac{CLIP} features)
& 2.5D top-down grid map
& \cmark
& Uncertainty-weighted accumulative fusion
& \xmark
& Irreversible belief fusion leads to semantic drift \\
\hline

\textbf{LERF~\cite{PIGEONVLMDrivenObject}}
& Language-conditioned episodic visual memory (\ac{PoI} snapshots)
& 2D geometric exploration map + episodic \ac{PoI} memory
& \cmark
& Episodic accumulation of visual evidence (per episode)
& \xmark
& No persistent semantic belief state or belief revision mechanism \\
\hline

\end{tabularx}
\caption{Comparison of semantic memory representations for exploration.
The table highlights how different methods store semantic information, update it over time,
and whether they support belief revision, revealing a common lack of mechanisms for correcting
erroneous semantic memories.}
\label{tab:sota:semantic_scene_reconstruction_overview}
\end{table}



% \begin{itemize}
%     \item Overview of approaches to build and update semantic maps during exploration:
%     \begin{itemize}
%         \item 2D grid maps
%         \item Pointclouds
%         \item Voxel grids
%         \item Octomaps
%         \item Scene graphs
%         \item Neural Radiance Fields (NeRFs)
%         \item Feature Fields
%     \end{itemize}
%     \item Techniques for fusing sensor data into persistent 2D/3D representations:
%     \begin{itemize}
%         \item Storing Visual Embeddings (e.g., CLIP features) in 3D maps for semantic querying.
%         \item Incremental updating of semantic labels based on new observations.
%         \item Handling uncertainty and conflicting detections over time.
%     \end{itemize}
%     \item Comparison of representations (Octomaps, point clouds, voxel grids) in terms of:
%     \begin{itemize}
%         \item Memory efficiency.
%         \item Ability to store semantic labels persistently.
%     \end{itemize}
%     \item Discussion of \dots
%     \begin{itemize}
%         \item OpenFusion
%         \item Clio
%         \item GeFF
%         \item CLIP-Fields
%         \item ConceptFusion
%         \item LERF
%     \end{itemize}
%     as examples of global 3D semantic maps.
%     \item Limitations in updating or correcting the map after wrong detections.
% \end{itemize}



% \section{Object Detection and Promptable Models}
% \begin{itemize}
%     \item Review of traditional and open-vocabulary object detection methods.
%     \item Analysis of grounding-capable detectors and segmentation models for zero-shot tasks.
%     \item Specific evaluation of the following models for their suitability in semantic multi-object search:
%     \begin{itemize}
%         \item YOLO-E
%         \item GroundingDINO
%         \item MobileSAM
%         \item GroundedSAM
%         \item SEEM
%         \item OWL-ViT
%         \item MaskDINO
%         \item GLIP
%     \end{itemize}
%     \item Discussion of promptable vision-language models supporting multi-modal queries (text, image, audio).
%     \item Challenges with false positives in zero-shot settings and their implications for reliable multi-object detection.
% \end{itemize}