% chktex-file 44

\chapter{State of the Art}
\label{ch:sota}

This work introduces a hybrid semantic exploration framework that combines open-vocabulary
semantic perception with autonomous exploration and persistent spatial memory. The proposed approach integrates concepts from geometric exploration, vision-language-guided
exploration, and semantic mapping. It is motivated by recent progress in incorporating semantic information from large pretrained foundation models into classical exploration and mapping pipelines. However, these novel semantic methods still rely on design and data representation patterns derived from classic geometric exploration and mapping methods.

Semantic exploration approaches differ fundamentally in how exploration behavior is generated.
Some methods learn end-to-end navigation policies using reinforcement learning or imitation learning,
where exploration strategies are implicitly encoded in a trained policy optimized via task-specific
reward functions~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023}. Other approaches adopt modular architectures that integrate pretrained vision-language models with classical mapping and planning techniques~\cite{yokoyamaVLFMVisionLanguageFrontier2023, buschOneMapFind2025,gadreCoWsPastureBaselines2022}.

Reinforcement learning and imitation learning approaches have demonstrated promising results in
semantic object search tasks by training agents to navigate toward target objects based on high-level
semantic cues~\cite{majumdarZSONZeroShotObjectGoal2023}, primarily within simulated environments. Their main advantage lies in avoiding hand-designed exploration heuristics, as complex behaviors can be learned directly from data through reward optimization~\cite{ghasemiComprehensiveSurveyReinforcement2025}. However, such policies typically require extensive training on large datasets (e.g. Replica~\cite{straubReplicaDatasetDigital2019}, Habitat~\cite{savvaHabitatPlatformEmbodied2019}), exhibit limited generalization to unseen environments or object categories~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023}, and lack interpretability due to their black-box nature~\cite{ghasemiComprehensiveSurveyReinforcement2025}.

In contrast, modular approaches leverage semantic representations provided by large-scale
pretrained \acp{VLM} to guide exploration decisions without task-specific retraining~\cite{alamaRayFrontsOpenSetSemantic2025}.
By combining explicit geometric mapping with semantic reasoning derived from foundation models,
these systems can achieve zero-shot generalization to novel objects and environments while retaining
interpretability and adaptability~\cite{yokoyamaVLFMVisionLanguageFrontier2023}. Several works extend this paradigm by constructing persistent semantic maps that retain knowledge of previously explored areas~\cite{buschOneMapFind2025,huangVisualLanguageMaps2023}, enabling more efficient multi-object search and the integration of high-level natural language instructions~\cite{guConceptGraphsOpenVocabulary3D2023}.

Finally, object detection and promptable vision-language models play a crucial role in enabling
open-vocabulary semantic understanding for exploration tasks. Recent advances in grounding-capable
detectors and segmentation models facilitate zero-shot object recognition based on text prompts,
allowing robots to identify and localize previously unseen objects~\cite{wangYOLOERealTimeSeeing2025,liGroundedLanguageImagePretraining2022,liuGroundingDINOMarrying2024}. These models form the semantic
foundation upon which hybrid exploration systems are built, enabling flexible object
search in diverse real-world environments.

Frontier-based exploration selects navigation goals from geometric map structure (frontiers) to maximize coverage and information gain, independent of any task-specific training.
Reinforcement-learning-based semantic exploration learns a navigation policy from simulated experience (reinforcement or imitation learning) that maps observations and a goal specification to actions, implicitly encoding exploration and target-seeking behavior.
Foundation-model-based semantic exploration uses pretrained vision-language foundation models as semantic priors to score observations or candidate goals by relevance to a text query, enabling zero-shot generalization within a modular planning pipeline.

\section{Frontier-Based Exploration}

Frontier-based exploration is a standard paradigm for autonomous exploration that selects goal locations on the boundary between explored free space and unknown areas to expand the map~\cite{lluviaActiveMappingRobot2021}. A \textit{frontier} is defined as the boundary between known free space and unknown regions of the environment~\cite{lluviaActiveMappingRobot2021}. Frontier-based exploration relies on the principle that unexplored areas adjacent to known free regions provide the highest potential information gain. To identify such frontiers, the robot must maintain a global representation of the environment, typically an occupancy grid or voxel map, where each cell is classified as \textit{free}, \textit{occupied}, or \textit{unknown}, based on sensor observations from \ac{LiDAR} or RGB-D cameras. The robot then iteratively selects and navigates to frontiers to expand its knowledge of the environment.

\citeauthor{quinApproachesEfficientlyDetecting2021}~\cite{quinApproachesEfficientlyDetecting2021} evaluated three commonly used frontier extraction methods that differ primarily in computational efficiency and scalability.  
The first approach, known as the \textit{Naïve Active Area (NaïveAA)} method, evaluates every cell in the occupancy grid to determine whether it is free and has at least one unknown neighbor. Although this approach is conceptually simple and accurate for small-scale maps, it becomes computationally expensive for larger environments and often produces small, fragmented frontier clusters.

The second approach, the \textit{Wavefront Frontier Detector (WFD)}~\cite{quinApproachesEfficientlyDetecting2021,topiwalaFrontierBasedExploration2018}, improves efficiency by using a breadth-first search (BFS) to identify connected frontier regions without exhaustively scanning the entire map. Unlike the NaïveAA method, WFD directly extracts continuous frontier clusters rather than treating each frontier cell individually, significantly reducing redundant computations.

The third method, the \textit{Frontier-Tracing Frontier Detection (FTFD)}~\cite{quinApproachesEfficientlyDetecting2021}, further enhances performance by incrementally updating frontier information using only the most recent sensor observations. Instead of re-evaluating the full map, FTFD initiates a BFS from previously known frontier cells that remain within the active area and from the endpoints of the latest sensor rays. Newly visible free-space cells along the scan boundary are evaluated as potential frontiers, while outdated frontier cells that are now occupied or re-observed are removed. By restricting computation to the local scan perimeter, FTFD achieves significantly faster update rates than NaïveAA and WFD, supporting real-time frontier detection even in large-scale environments.

After frontiers have been extracted, a selection strategy determines which frontier the robot should explore next. Simple heuristics such as \textit{nearest-frontier selection} minimize travel distance but can lead to oscillatory behavior between nearby frontiers. Alternatively, selecting the \textit{largest frontier} favors unexplored regions of higher spatial extent, reducing dead-end visits but increasing traversal cost.  
To address these trade-offs, \citeauthor{bourgaultInformationBasedAdaptive2002}~\cite{bourgaultInformationBasedAdaptive2002} introduced a \textit{utility-based frontier selection} framework that combines multiple criteria, such as distance, frontier size, and expected information gain, into a unified objective function. This approach enables more balanced decision-making, improving overall exploration efficiency and map completeness. Many subsequent works have built upon this foundation, incorporating additional factors such as energy consumption, obstacle density, and dynamic environment considerations into the utility function~\cite{lluviaActiveMappingRobot2021}.

However, all these methods are primarily designed for geometric exploration without incorporating semantic understanding. As a result, they are optimized for complete map coverage rather than goal-directed exploration tasks. In scenarios where a robot must locate specific objects or regions based on semantic cues, purely geometric frontier selection often leads to inefficient search behavior and unnecessary traversal. This motivates the integration of semantic reasoning into the frontier-based exploration process, where frontiers can be prioritized not only by geometric utility but also by their semantic relevance to the task objective.

\section{RL-based Semantic Exploration}
\label{sec:sota:rl_semantic_exploration}

In contrast to geometric exploration, which aims to maximize map coverage using the shortest possible path and time, semantic exploration focuses on efficiently locating specific objects or regions of interest described in high-level semantic terms. The objective is to minimize path length and exploration time while prioritizing areas likely to contain relevant targets rather than achieving complete spatial coverage.  

Semantic exploration approaches differ fundamentally in how exploration behavior is generated and optimized. \ac{RL} and imitation learning methods have demonstrated promising results by training agents to navigate toward target objects based on high-level semantic cues~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023,ramakrishnanPONIPotentialFunctions2022}. In these approaches, an agent learns a policy that maps high-dimensional sensory observations to low-level control actions by optimizing a task-specific reward function, rather than relying on explicitly designed exploration heuristics~\cite{ghasemiComprehensiveSurveyReinforcement2025}.

In the context of semantic exploration, \ac{RL}-based methods are typically formulated as Markov Decision Processes or, more commonly in embodied navigation, as partially observable Markov Decision Processes (\acp{POMDP})~\cite{thrunProbabilisticRobotics2006}. At an abstract level, a \ac{POMDP} can be defined as a tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where $\mathcal{S}$ denotes the latent environment states, $\mathcal{A}$ the set of possible actions (e.g., move forward, turn left/right), $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ the state transition probability function, $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ the reward function, and $\gamma \in [0,1)$ the discount factor. The agent does not observe the full state directly but instead receives high-dimensional, partial observations (e.g., RGB images, depth measurements, or semantic representations), from which it must infer an internal belief about the environment~\cite{ghasemiComprehensiveSurveyReinforcement2025}.

The reward function encodes task objectives such as reaching a target object, minimizing
path length, or maintaining correct orientation, thereby implicitly shaping the agent’s
exploration behavior~\cite{ghasemiComprehensiveSurveyReinforcement2025}.
This paradigm has been successfully applied to semantic object-goal navigation, where agents
are trained to locate objects specified by high-level semantic descriptions
(e.g., object categories or language embeddings)~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023,ramakrishnanPONIPotentialFunctions2022}.

A key advantage of \ac{RL}-based semantic exploration lies in its flexibility. Complex navigation behaviors can be learned directly from interaction data without manually designing exploration strategies. However, this flexibility comes at the cost of extensive training requirements, limited interpretability, and reduced robustness to domain shifts, as policies often overfit to the visual statistics and dynamics of the training environments~\cite{ghasemiComprehensiveSurveyReinforcement2025,majumdarZSONZeroShotObjectGoal2023}. As a result, many such approaches are primarily evaluated in simulation and struggle to apply to previously unseen scenes, object appearances, or sensor configurations. Commonly, \ac{RL} algorithms are optimized via policy gradient methods~\cite{thomasPolicyGradientMethods2017}, which directly adjust the policy parameters $\theta$ to maximize the expected cumulative reward $J(\theta)$:

\begin{equation}
\nabla_{\theta} J(\theta) =
\mathbb{E}_{\tau \sim \pi_{\theta}}
\left[
\sum_{t=0}^{T}
\nabla_{\theta} \log \pi_{\theta}(a_t \mid o_t) \, R(\tau)
\right],
\end{equation}

where $\tau$ denotes trajectories sampled from the policy $\pi_{\theta}$, $a_t$ and $o_t$ are the action and observation at time $t$, respectively, and $R(\tau)$ is the cumulative reward obtained over trajectory $\tau$.

% ---------------------- ZSON ----------------------

\citeauthor{majumdarZSONZeroShotObjectGoal2023}~\cite{majumdarZSONZeroShotObjectGoal2023} proposed \ac{ZSON}, a zero-shot object navigation framework that leverages the shared embedding space of CLIP~\cite{radfordLearningTransferableVisual2021} to guide navigation policies trained via \ac{RL}. 
During training, the agent observes RGB images and previous actions, while the navigation goal is specified by the CLIP embedding of an image containing the target object. 
The policy is optimized using a shaped reward function:

\begin{equation}
r_t = r_{\text{success}} + r_{\text{angle-success}} - \Delta d_{tg} - \Delta a_{tg} + r_{\text{slack}},
\end{equation}

where $r_{\text{success}}$ rewards successful target localization, $r_{\text{angle-success}}$ encourages correct orientation, $\Delta d_{tg}$ and $\Delta a_{tg}$ penalize distance and angular deviation, respectively, and $r_{\text{slack}}$ is a negative constant, which penalizes inefficient actions per step~\cite{majumdarZSONZeroShotObjectGoal2023}. At inference time, the image-based goal embedding is replaced by the CLIP embedding of a textual object description, enabling zero-shot generalization to unseen object categories. The action space is discrete and consists of \textit{move forward}, \textit{turn left}, \textit{turn right}, and \textit{stop}. The reward function is used exclusively during training to learn the navigation policy during inference, the agent selects actions solely based on the learned policy without access to the reward signal.

While \ac{ZSON} demonstrates strong zero-shot object generalization, exploration behavior remains implicitly encoded in the learned policy, which makes it difficult to interpret or adapt to new scenarios. As a consequence, the agent tends to revisit visually familiar regions and lacks explicit mechanisms for systematic exploration of unknown space. Furthermore, the end-to-end \ac{RL} formulation reduces interpretability and necessitates retraining when visual conditions or environment layouts deviate from the training distribution. Nevertheless, \ac{ZSON} represents a significant step toward flexible and generalizable semantic exploration by integrating vision-language models with reinforcement learning.

% ---------------------- PIRLNav ----------------------

\citeauthor{ramrakhyaPIRLNavPretrainingImitation2023}~\cite{ramrakhyaPIRLNavPretrainingImitation2023} introduced \ac{PIRLNav}, a two-stage framework that combines \ac{BC} with \ac{RL} to improve generalization in semantic navigation tasks. In the first stage, a navigation policy is pretrained via imitation learning on large-scale human demonstrations, learning to reproduce expert actions from observations that include \ac{RGB} images, pose information, and a categorical goal representation. This pretraining stage provides a strong initialization, which reduces the training time and sample complexity required for subsequent reinforcement learning. The behavior cloning objective is formulated as a cross-entropy loss over expert actions (see Equation~\ref{eq:pirl_nav:bc_loss}).

A stochastic navigation policy is denoted by $\pi_\theta(a \mid s)$, parameterized by $\theta$, mapping an observation $s$ to a distribution over actions $a$. Training objectives are expressed as loss functions $\mathcal{L}(\theta)$ minimized with respect to $\theta$. The pretrained ObjectNav policy is trained using supervised learning to minimize the behavior cloning loss in Equation~\ref{eq:pirl_nav:bc_loss},

\begin{equation}
\mathcal{L}_{\text{BC}}(\theta) = - \sum_{t=1}^{T} \log \pi_\theta(a_t^* \mid s_t),
\label{eq:pirl_nav:bc_loss}
\end{equation}

where $a_t^*$ denotes the expert action at time step $t$ and $s_t$ denotes the agent observation. The ObjectNav policy is trained with a \ac{CNN}+\ac{RNN} architecture, in which a pretrained vision backbone, \ac{DINO}, extracts visual features from RGB images and feeds them into the navigation policy network along with pose and goal information.

In the second stage, the pretrained policy is fine-tuned using \ac{RL} by maximizing the expected discounted return (see Equation~\ref{eq:pirl_nav:rl_objective}):

\begin{equation}
\theta^{*} = \arg\max_{\theta} \mathbb{E}_{\tau \sim \pi_\theta}
\left[ \sum_{t=1}^{T} \gamma^{t-1} r_t \right],
\label{eq:pirl_nav:rl_objective}
\end{equation}

where $\tau = (s_1,a_1,\dots,s_T,a_T)$ denotes a trajectory generated by executing $\pi_\theta$, $r_t$ is the reward at time step $t$, and $\gamma \in (0,1]$ is the discount factor. Although this hybrid training strategy improves sample efficiency and navigation robustness in simulation, the resulting policy remains a black-box model. It does not maintain an explicit semantic memory and requires retraining to adapt to new visual domains or sensor modalities.


% ---------------------- PONI ----------------------

To address the limited interpretability of end-to-end policies, \citeauthor{ramakrishnanPONIPotentialFunctions2022}~\cite{ramakrishnanPONIPotentialFunctions2022} proposed \ac{PONI}, a supervised, map-based semantic exploration framework. Rather than learning low-level actions, \ac{PONI} predicts high-level exploration objectives in the form of potential fields defined over a partial semantic map. Two complementary potentials are estimated: an area potential that encourages exploration of unknown space $U_t^a$, and an object potential that estimates proximity to the target category $U_t^o$. Exploration decisions are derived by scoring geometric frontiers according to a weighted combination of these potentials (see Equation~\ref{eq:semutil:frontier_utility}),

\begin{equation}
U_t(f) = \alpha \, U_t^a(f) + (1 - \alpha) \, U_t^o(f)
\label{eq:semutil:frontier_utility}
\end{equation}

where $\alpha$ explicitly controls the trade-off between exploration and exploitation. 
This formulation yields interpretable and stable exploration behavior and decouples high-level decision-making from low-level navigation, which is handled by a classical navigation planner.

However, \ac{PONI} relies on dense semantic annotations and assumes a fixed, closed set of object categories encountered during training.~\cite{ramakrishnanPONIPotentialFunctions2022} used Mask~R-CNN~\cite{heMaskRCNN2018} to generate semantic maps with 80 object classes from the \ac{COCO} dataset~\cite{linMicrosoftCOCOCommon2015}, which was finetuned within the Gibson dataset~\cite{xiaGibsonEnvRealWorld2018}. As a result, it does not support open-vocabulary or zero-shot object search and remains sensitive to annotation noise and domain shifts.

RL-based and supervised semantic exploration methods demonstrate the feasibility of learning navigation behavior from semantic cues, but they exhibit recurring limitations that motivate alternative approaches. These include extensive training requirements, limited interpretability, closed-set semantics, lack of persistent semantic memory, and reduced robustness to domain changes. 
These structural shortcomings have motivated recent work toward modular exploration frameworks that integrate pretrained vision-language models, which are discussed in the following section.

\section{Foundation-Model-Based Semantic Exploration}
\label{sec:sota:foundation_models}

In contrast to reinforcement learning-based approaches, which derive navigation behavior through task-specific training, a second line of work leverages large-scale pretrained \acp{VLM} as semantic priors within modular exploration systems. These approaches shift the learning burden away from navigation policy optimization toward semantic perception and reasoning, enabling zero-shot generalization to novel objects and environments without task-specific retraining.

\acp{VLM} are large pretrained image-text models that learn joint representations of visual and linguistic data from massive web-scale datasets~\cite{radfordLearningTransferableVisual2021,liGroundedLanguageImagePretraining2022}. Their core principle is to embed images and text into a shared latent space, in which semantically
related visual and linguistic concepts are mapped to nearby representations.
A \ac{VLM} defines two embedding functions,
\(
f_I(\cdot)
\)
and
\(
f_T(\cdot),
\)
which map an image \(I\) and a text prompt \(T\) to a common embedding space
\(\mathbb{R}^d\):

\begin{equation}
\mathbf{e}_I = f_I(I), \qquad \mathbf{e}_T = f_T(T),
\end{equation}

where \(\mathbf{e}_I, \mathbf{e}_T \in \mathbb{R}^d\) are high-dimensional feature vectors encoding semantic information. Text inputs are tokenized and processed using transformer-based language encoders, while visual
inputs are decomposed into patches or regions and encoded by a vision backbone (e.g., \ac{CNN} or transformer-based architectures), depending on the model design~\cite{vaswaniAttentionAllYou2023,liBLIP2BootstrappingLanguageImage2023,liGroundedLanguageImagePretraining2022}. Semantic alignment between image and text embeddings is commonly quantified using cosine similarity, which measures the angular similarity between vectors in the shared embedding space (see Equation~\ref{eq:vlm_cosine_similarity}),

\begin{equation}
\label{eq:vlm_cosine_similarity}
\mathrm{sim}(I, T)
=
\frac{\mathbf{e}_I \cdot \mathbf{e}_T}
{\lVert \mathbf{e}_I \rVert \, \lVert \mathbf{e}_T \rVert}
\end{equation}

A higher similarity score indicates stronger semantic correspondence between the visual observation
and the textual query. This representation enables open-vocabulary reasoning, as arbitrary object descriptions can be
matched against visual observations without retraining, forming the foundation for zero-shot semantic perception in exploration tasks.

Foundation-model-based exploration is characterised by explicit separation between geometric navigation and semantic understanding. Geometric structure is typically handled by classical mapping and planning components (e.g., frontier-based exploration), while semantic relevance is inferred from pretrained models such as CLIP~\cite{radfordLearningTransferableVisual2021}, BLIP-2~\cite{liBLIP2BootstrappingLanguageImage2023}, or grounding-capable detectors~\cite{gadreCoWsPastureBaselines2022,yokoyamaVLFMVisionLanguageFrontier2023,liuGroundingDINOMarrying2024}. This modularity improves interpretability and adaptability, but introduces new challenges related to uncertainty handling, semantic consistency, and long-term memory.

Table~\ref{tab:sota:foundation_design_patterns} summarizes foundation-model-based exploration frameworks in five categories: (1) the source of semantic signals, (2) the mechanism used to fuse semantics with geometric exploration, (3) the handling of detection confidence and uncertainty, (4) the underlying semantic data representation, and (5) the dominant failure causalities observed in practice.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{4pt}
    \begin{tabularx}{\textwidth}{
        |l|
        >{\raggedright\arraybackslash}X|
        >{\raggedright\arraybackslash}X|
        >{\raggedright\arraybackslash}X|
        >{\raggedright\arraybackslash}X|
        >{\raggedright\arraybackslash}X|
    }
    \hline
    \textbf{Method} 
    & \textbf{Semantic signal source} 
    & \textbf{Fusion with geometry} 
    & \textbf{Detection confidence handling} 
    & \textbf{Semantic representation} 
    & \textbf{Primary failure causality} \\
    \hline

    \textbf{ESC~\cite{zhouESCExplorationSoft2023}}
    & Object detections + \ac{LLM} priors
    & Probabilistic frontier scoring (PSL)
    & Single-step confidence, no revision
    & Local symbolic semantic map
    & False positives amplified by reasoning priors \\
    \hline

    \textbf{CoW~\cite{gadreCoWsPastureBaselines2022}}
    & Image-text similarity \ac{CLIP}
    & No explicit geometric fusion
    & Threshold-based similarity
    & No explicit map
    & Oscillation and local minima near false positives \\
    \hline

    \textbf{SemUtil~\cite{chenHowNotTrain2023}}
    & Closed-set detections + \ac{CLIP} similarity
    & Utility map over geometric frontiers
    & No confidence decay or belief update
    & Semantic point cloud + scene graph
    & Persistent corruption from misdetections \\
    \hline

    \textbf{VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}}
    & Dense image-text similarity~\ac{BLIP}
    & Semantic value-map fused with frontier map
    & Weighted averaging over observations
    & Episodic semantic value map
    & False positives persist across episode \\
    \hline

    \end{tabularx}
    \caption{Design patterns and limitations of foundation-model-based semantic exploration frameworks.
    The table compares how different methods obtain semantic signals, integrate them with geometric exploration,
    handle uncertainty over time, and represent semantic information, revealing recurring failure modes
    that motivate the need for persistent semantic memory and belief revision.}
    \label{tab:sota:foundation_design_patterns}
\end{table}

A representative example of this paradigm is the \ac{ESC} framework proposed by \citeauthor{zhouESCExplorationSoft2023}~\cite{zhouESCExplorationSoft2023}, which augments traditional frontier-based exploration with semantic cues derived from pretrained \acp{VLM}. Specifically, \ac{ESC} combines a grounded object detector, \ac{GLIP}~\cite{liGroundedLanguageImagePretraining2022}, with a \ac{LLM} (either ChatGPT or DeBERTa) to generate semantic priors that guide exploration decisions. Frontiers are scored based on both geometric utility and semantic relevance to the target object (see Equation~\ref{eq:esc}).

\begin{equation}
\label{eq:esc}
P(F) = P(F \mid d_i^{t}, o^{t}, r^{t})
\end{equation}

\(P(F)\) denotes the probability of selecting frontier \(F\), conditioned on detected objects \(d_i^{t}\), current image observations \(o^{t}\), and the robot pose \(r^{t}\) at time \(t\). This formulation is implemented using \ac{PSL}, which fuses visual detections with language-derived priors about object co-occurrences and spatial relationships. The robot then selects the frontier with the highest combined score, balancing geometric and semantic information to improve search efficiency. For navigating toward target objects, a classical \ac{A*} planner is employed to compute collision-free paths based on the occupancy map.

~\citeauthor{zhouESCExplorationSoft2023}~\cite{zhouESCExplorationSoft2023} employs \ac{GLIP}~\cite{liGroundedLanguageImagePretraining2022} as the detection backbone to compute 2D bounding boxes, class labels, and confidence scores for objects within the robot’s \ac{FOV}. While effective in simulation, the approach introduces notable computational overhead due to repeated \ac{LLM} inference and PSL optimization. As a consequence of relying on single-step detections and static commonsense priors, errors introduced by false positives are not attenuated over time. Once a misleading semantic hypothesis is introduced, the probabilistic reasoning layer tends to reinforce rather than correct it, leading to persistent semantic bias during exploration.

Through ablation studies, \citeauthor{zhouESCExplorationSoft2023}~\cite{zhouESCExplorationSoft2023} observed that object-object and object-room relational priors can occasionally degrade performance, as commonsense relationships are inherently probabilistic rather than deterministic. Additionally, while \ac{ESC} maintains a local semantic map during navigation, it lacks mechanisms for long-term memory or belief revision. Consequently, once an incorrect detection or prior is introduced, the framework has no learned means of down-weighting or correcting it over time, which can lead to persistent semantic inconsistencies during extended exploration.

In contrast to such \ac{LLM} reasoning-based systems, \citeauthor{gadreCoWsPastureBaselines2022}~\cite{gadreCoWsPastureBaselines2022} proposed \ac{CoW}, a lightweight vision-language exploration framework that relies purely on image-text alignment from CLIP~\cite{radfordLearningTransferableVisual2021} without requiring explicit frontier detection, semantic mapping, or object segmentation. The method guides the robot toward directions with the highest cosine similarity (see Equation~\ref{eq:vlm_cosine_similarity}) between the current visual observation and the target object description.

By eliminating explicit detectors and handcrafted mapping, \ac{CoW} offers a computationally efficient and conceptually simple baseline for open-vocabulary navigation. However, this simplicity comes at the cost of robustness. The system is highly sensitive to viewpoint variations and clutter, as cosine similarity does not always correlate with true object presence. Without spatial memory or geometric reasoning, the robot may oscillate near false positives or become trapped in local minima. Moreover, because similarity scores vary across object categories, no universal threshold can be established for all targets, resulting in inconsistent stopping behavior and reduced reliability during multi-object search.

Building upon this idea of integrating semantics into classical exploration, 
\citeauthor{chenHowNotTrain2023}~\cite{chenHowNotTrain2023} introduced \ac{SemUtil}, 
a fully modular and training-free framework for object-goal navigation that combines classical SLAM-based mapping 
with pretrained perception and language models. 
In contrast to reinforcement learning or imitation learning approaches, 
\ac{SemUtil} leverages explicit geometric and semantic reasoning through three core components: 
a 2D occupancy map for frontier extraction, a semantic point cloud generated by projecting Mask~R-CNN detections into 3D space, 
and a spatial scene graph for high-level semantic reasoning. 
These three representations collectively form a structured scene model that supports geometric planning, semantic propagation, 
and reasoning about unexplored regions~\cite{chenHowNotTrain2023}.  

The central element of \ac{SemUtil} is the \textit{utility module}, 
which fuses geometric frontiers with semantic priors to determine the most promising frontier to explore next. 
For each map cell, a \textit{utility score} is computed by combining the geometric frontier characteristics, 
the CLIP-based cosine similarity between the current observation and the target object description, 
and the semantic cues from the 3D point cloud (e.g., class IDs from Mask~R-CNN). 
This results in a utility map that prioritizes frontiers both spatially and semantically,
as illustrated in Figure~3 of the original paper (showing the interaction between geometric and semantic utilities)~\cite{chenHowNotTrain2023}. \ac{SemUtil} solves the oscillation issues observed in \ac{CoW} by explicitly extracting frontiers from the occupancy map and scoring them based on their semantic utility, rather than relying solely on raw similarity scores. This structured approach enables more stable exploration behavior and reduces the likelihood of becoming trapped near false positives, which also applies to other works, which combine \acp{VLM} with frontier-based exploration~\cite{yokoyamaVLFMVisionLanguageFrontier2023}.
 
Importantly, the utility map in \ac{SemUtil} is not persistent, it is recomputed at every timestep based solely on the current observation and semantic point cloud, without maintaining a long-term memory of past detections or map updates. While this design simplifies computation and eliminates the need for training, it also limits the system’s ability to reason over time or correct previous errors. The reliance on a closed-set detector (Mask~R-CNN) restricts open-vocabulary generalization, and any incorrect detection directly corrupts the semantic point cloud, thereby distorting the frontier scoring and leading to suboptimal exploration decisions.  Furthermore, since the framework lacks belief revision or memory-based fusion, false detections persist until they leave the robot’s current field of view, reducing consistency and efficiency in long-term navigation.

In contrast to SemUtil~\cite{chenHowNotTrain2023}, \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023} derives semantic values directly from RGB observations using a pretrained VLM (BLIP-2)~\cite{liBLIP2BootstrappingLanguageImage2023}. The target relevance is the cosine similarity to the text prompt (see Equation~\ref{eq:vlm_cosine_similarity})~\cite{gadreCoWsPastureBaselines2022}.

The resulting similarity values are spatially projected onto a top-down occupancy grid according to the robot’s \ac{FOV}, forming a \textit{value map} that quantifies the semantic likelihood of each region leading toward the target object. To account for reduced reliability near the image periphery, a Gaussian weighting function attenuates confidence values based on angular distance from the optical axis (see Fig.~3 in the original paper~\cite{yokoyamaVLFMVisionLanguageFrontier2023}). This value map is continuously updated through a weighted averaging scheme that fuses new and previous similarity scores according to their confidence weights, enabling smooth map updates and spatial consistency across frames. \ac{BLIP-2} is not used for caption generation, but rather for retrieving text-image embeddings to compute similarity scores~\cite{liBLIP2BootstrappingLanguageImage2023,yokoyamaVLFMVisionLanguageFrontier2023}.

During exploration, \ac{VLFM} fuses this value map with a geometrically extracted frontier map to select the next exploration goal, the frontier with the highest semantic value is chosen as the next waypoint. Target object detection is performed using YOLOv7~\cite{wangYOLOv7TrainableBagoffreebies2022} for \ac{COCO}~\cite{linMicrosoftCOCOCommon2015} categories and GroundingDINO~\cite{liuGroundingDINOMarrying2024} for open-vocabulary detection. Once an object matching the target query is detected, SAM~\cite{kirillovSegmentAnything2023} is applied to generate an accurate mask, and the system transitions from exploration to goal navigation.  

This modular framework achieves state-of-the-art performance on the Gibson~\cite{xiaGibsonEnvRealWorld2018}, HM3D, and Matterport3D~\cite{changMatterport3DLearningRGBD2017} benchmarks, outperforming prior zero-shot approaches such as \ac{ESC}, \ac{SemUtil}, and \ac{CoW} in both \ac{SR} and \ac{SPL} \cite{yokoyamaVLFMVisionLanguageFrontier2023}. Despite its efficiency and interpretability, several limitations remain. \ac{VLFM} relies on a single-source detection pipeline, either GroundingDINO or YOLOv7, making it prone to false positives in open-vocabulary scenarios, which can result in premature stopping behavior. Furthermore, the value map is episodic rather than persistent: it resets after each navigation episode and does not maintain long-term semantic memory, leading to redundant revisits during multi-object search tasks. While the system operates in real time, the use of multiple large-scale pretrained models (BLIP-2~\cite{liBLIP2BootstrappingLanguageImage2023}, GroundingDINO/YOLOv7~\cite{liuGroundingDINOMarrying2024,wangYOLOv7TrainableBagoffreebies2022}, and SAM~\cite{kirillovSegmentAnything2023}) demands substantial computational resources, consuming approximately 16~GB of \ac{VRAM} on an NVIDIA RTX~4090 GPU during deployment, which limits scalability on embedded robotic platforms.

Foundation-model-based exploration approaches enable zero-shot semantic navigation without the need for task-specific training and offer improved interpretability compared to learned policies. However, across all reviewed methods, semantic information is either transient, episodic, or locally scoped. None of the approaches maintain a persistent semantic belief that can be revised over time as new evidence is accumulated. This lack of long-term semantic memory leads to repeated exploration, sensitivity to false detections, and inconsistent behavior in multi-object search scenarios.

These recurring limitations motivate the development of exploration frameworks that combine
open-vocabulary semantic perception with persistent, revisable semantic memory, which is the focus of this work.

\section{Persistent Semantic Mapping for Exploration}
Table~\ref{tab:sota:persistent_mapping_exploration} summarizes recent approaches to persistent semantic mapping for exploration tasks. The table highlights how different methods store semantic information, update it over time, and whether they support belief revision, revealing a common lack of mechanisms for correcting erroneous semantic memories.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{
    l |
    p{3.0cm}
    p{1.5cm}
    >{\centering\arraybackslash}p{1.2cm}
    p{2.0cm}
    >{\centering\arraybackslash}p{1.2cm}
    p{2.0cm}
    }
    \toprule
    \textbf{Method}
    & \textbf{Semantic Memory Type}
    & \textbf{Spatial Representation}
    & \textbf{Open-Vocab}
    & \textbf{Update Mechanism}
    & \textbf{Belief Revision}
    & \textbf{Primary Limitation} \\
    \midrule

    \textbf{OneMap~\cite{buschOneMapFind2025}}
    & Open-Vocabulary Belief Map (\ac{CLIP} features)
    & 2.5D top-down grid map
    & \cmark
    & Uncertainty-weighted accumulative fusion
    & \xmark
    & Irreversible belief fusion leads to semantic drift \\

    \textbf{VLMaps~\cite{huangVisualLanguageMaps2023}}
    & Dense per-cell language embeddings (\ac{CLIP}-based~\cite{radfordLearningTransferableVisual2021})
    & 2.5D top-down grid map
    & \cmark
    & Multi-view feature averaging (accumulative fusion)
    & \xmark
    & Irreversible feature averaging causes semantic noise and ambiguity \\

    \textbf{PIGEON~\cite{pengPIGEONVLMDrivenObject2025}}
    & Language-conditioned episodic visual memory (\ac{PoI} snapshots)
    & 2D geometric exploration map + episodic \ac{PoI} memory
    & \cmark
    & Episodic accumulation of visual evidence (per episode)
    & \xmark
    & No persistent semantic belief state or belief revision mechanism \\

    \textbf{DualMap~\cite{jiangDualMapOnlineOpenVocabulary2025}}
    & Dual semantic maps (short-term + long-term)
    & 2D grid maps
    & \cmark
    & Dual-stream accumulative fusion
    & \xmark
    & Long-term map cannot correct early semantic errors \\

    \bottomrule
    \end{tabular}

    \caption{Comparison of semantic memory representations for exploration.
    The table highlights how different methods store semantic information, update it over time,
    and whether they support belief revision.}
    \label{tab:sota:persistent_mapping_exploration}
\end{table}


% ---------------------- OneMap, VLMaps ----------------------

A approach to building persistent open-vocabulary semantic maps during exploration
is presented by \citeauthor{buschOneMapFind2025}~\cite{buschOneMapFind2025} and
\citeauthor{huangVisualLanguageMaps2023}~\cite{huangVisualLanguageMaps2023}.
Both methods construct a 2.5D top-down grid map in which semantic information is stored at the
cell level in the form of language-aligned visual embeddings, enabling open-vocabulary querying
via image-text similarity.

In both works, incoming RGB-D observations are processed by a vision-language model to
extract dense semantic features. In \ac{OneMap}, global \ac{CLIP} image embeddings are projected
into the map using camera intrinsics and extrinsics, while \ac{VLMaps} employs language-driven
semantic segmentation (LSeg) to obtain dense per-pixel language embeddings~\cite{buschOneMapFind2025,huangVisualLanguageMaps2023}. These features are associated with corresponding grid cells in the top-down map by back-projecting depth pixels into 3D space and discretizing them onto the 2.5D grid representation.

As the robot explores, newly observed embeddings are fused with existing map entries in an
accumulative manner. \citeauthor{buschOneMapFind2025}~\cite{buschOneMapFind2025} perform uncertainty-aware recursive fusion,
in which observations with higher confidence exert greater influence on the stored semantic
representation, whereas \citeauthor{huangVisualLanguageMaps2023}~\cite{huangVisualLanguageMaps2023} apply multi-view feature averaging without explicit uncertainty modeling. In both cases, once semantic features are integrated into the map, they are not selectively down-weighted or removed at later time steps.

Open-vocabulary object querying is performed by comparing stored map embeddings against the
embedding of a text prompt using cosine similarity (see Equation~\ref{eq:vlm_cosine_similarity}).
An object is considered detected if the similarity score in any map cell exceeds a predefined
threshold.
Because this decision relies solely on similarity values rather than explicit object detection
or instance verification, both approaches are sensitive to threshold selection and may produce
false positives in visually cluttered or ambiguous scenes.

Importantly, neither \ac{OneMap} nor \ac{VLMaps} incorporates mechanisms for belief revision or
error correction.
Once incorrect or noisy observations are fused into the map, they persist indefinitely and can
bias future exploration decisions, leading to semantic drift over time~\cite{buschOneMapFind2025,huangVisualLanguageMaps2023}.
Furthermore, the projection of semantic information onto a 2.5D grid discards vertical structure
and instance-level geometry, limiting semantic fidelity in complex environments and preventing
accurate 3D object localization for downstream tasks such as manipulation or grasp planning~\cite{guConceptGraphsOpenVocabulary3D2023}.

% Pigeon

\citeauthor{pengPIGEONVLMDrivenObject2025}~\cite{pengPIGEONVLMDrivenObject2025} introduced \textit{PIGEON}, a \ac{VLM}-driven exploration framework that replaces persistent dense semantic maps with an episodic, object-centric memory abstraction. Instead of maintaining a globally consistent semantic map, PIGEON represents the environment as a set of semantically meaningful \textit{\acp{PoI}}, corresponding to geometrically salient observation locations enriched with visual context.

At each \ac{PoI}, the robot stores a small set of \ac{RGB} observations captured from multiple
viewpoints~\cite{pengPIGEONVLMDrivenObject2025}.
Semantic evaluation is deferred to query time, where a vision-language model jointly reasons over the language query and the stored \ac{RGB} observations to assess the semantic relevance of each \ac{PoI}.
These relevance assessments are used to guide \ac{PoI} selection, while low-level navigation between \acp{PoI} is handled by a classical planner. Reinforcement learning is employed to fine-tune the VLM’s \ac{PoI} selection behavior, rather than to directly select navigation actions.

Formally, each \ac{PoI} $p_i$ is defined by a spatial location $\mathbf{x}_i$ and an associated
set of \ac{RGB} observations $\mathcal{I}_i = \{I_{i,1}, \dots, I_{i,K}\}$ captured from different
viewpoints at that location~\cite{pengPIGEONVLMDrivenObject2025}.
No semantic labels, object identities, or belief states are stored; semantic interpretation is
performed on demand and is not consolidated into a persistent semantic world model.

In contrast to dense mapping approaches such as \ac{OneMap}~\cite{buschOneMapFind2025} and
\ac{VLMaps}~\cite{huangVisualLanguageMaps2023}, PIGEON does not perform accumulative fusion of
semantic features into a persistent spatial representation.
As a result, it is less susceptible to semantic drift caused by irreversible belief fusion, but
does not support belief revision or long-term semantic consistency across multiple object-search
tasks~\cite{pengPIGEONVLMDrivenObject2025}.

Methodologically, PIGEON occupies an intermediate position between dense semantic mapping and
object-centric persistent representations such as DualMap~\cite{jiangDualMapOnlineOpenVocabulary2025}
and ConceptGraphs~\cite{guConceptGraphsOpenVocabulary3D2023}.
It combines episodic visual memory with query-conditioned semantic scoring and reinforcement
learning-based navigation, while deliberately avoiding persistent semantic state estimation.

% ---------------------- DualMap ----------------------

\citeauthor{jiangDualMapOnlineOpenVocabulary2025}~\cite{jiangDualMapOnlineOpenVocabulary2025}
introduced \textit{DualMap}, an object-centric framework for online open-vocabulary exploration
that explicitly separates short-term perceptual observations from long-term semantic memory.
Unlike dense feature-map approaches such as \ac{OneMap}~\cite{buschOneMapFind2025} and
\ac{VLMaps}~\cite{huangVisualLanguageMaps2023}, which store pixel-wise or cell-wise language
embeddings, DualMap reasons over discrete object instances and their spatial relations.

At each timestep, objects are detected and segmented from the RGB image using
YOLO-World~\cite{chengYOLOWorldRealTimeOpenVocabulary2024}.
For each segmented object, a visual embedding is computed from the cropped image using
CLIP’s image encoder~\cite{radfordLearningTransferableVisual2021}.
If a textual label is available, an additional text embedding is obtained from CLIP’s text encoder.
The final object-level semantic representation is computed as a weighted fusion of image and
text embeddings:

\begin{equation}
\mathbf{f}_t
=
\alpha \, \mathbf{f}^{\text{img}}_t
+
(1 - \alpha) \, \mathbf{f}^{\text{text}}_t,
\qquad \alpha = 0.7,
\label{eq:feature_fusion}
\end{equation}


where $\mathbf{f}^{\text{img}}_t$ and $\mathbf{f}^{\text{text}}_t$ denote the image-based and
text-based embeddings at time $t$, respectively.
This object-centric representation enables open-vocabulary semantic reasoning without
requiring dense per-pixel feature storage.

DualMap maintains two complementary semantic maps.
The \textit{local concrete map} stores recently observed object instances as 3D point clouds
with associated semantic embeddings, enabling rapid adaptation to new observations.
In contrast, the \textit{abstract map} serves as a long-term semantic memory and stores only
stable object instances, referred to as \textit{anchors} (e.g., tables, desks, counters),
which are unlikely to change location over time.
Smaller or movable objects are treated as \textit{volatile} and are not permanently stored
in the abstract map~\cite{buschOneMapFind2025}.

Objects are added to or updated in the maps based on a combination of semantic similarity
between embeddings and geometric overlap, measured via the 3D \ac{IoU}
between observed point clouds.
An object is updated from the local concrete map to the abstract map only when its
confidence exceeds a predefined threshold, thereby balancing adaptability with long-term
stability.
Each anchor in the abstract map maintains a list of associated volatile objects that have
been observed in its vicinity, allowing the system to reason about object co-occurrence
without permanently storing potentially transient items~\cite{buschOneMapFind2025}.

During object-goal exploration, DualMap does not directly search the entire map for the target
object. Instead, the language query is embedded using CLIP and matched against the semantic
representations of anchors and their associated volatile objects.
Anchors with high semantic relevance are prioritized as navigation goals, and the robot
navigates toward them using a classical \ac{A*} planner~\cite{kabirEnhancedRobotMotionBlockAStar2024} on the occupancy map. Once the target object is detected again in the local concrete map, exploration terminates.

By separating short-term perceptual memory from long-term semantic anchors, DualMap achieves more structured and interpretable language-guided navigation than dense feature-map approaches. However, semantic information in DualMap is primarily used for exploitation of previously observed anchors, while exploration itself remains purely geometry-driven. The framework does not provide a mechanism or hyperparameter to explicitly balance semantic exploration and exploitation, as semantic reasoning is only applied after anchors have been formed~\cite{jiangDualMapOnlineOpenVocabulary2025}. As a result, DualMap favors semantic exploitation over semantic exploration, which can limit its ability to actively search for objects that have not yet been associated with existing anchors.

\section{Semantic Scene Reconstruction}
\label{sec:sota:semantic_scene_reconstruction}

In order to incorporate persistent semantic memory into exploration systems, to build and maintain accurate 3D reconstructions of the environment enriched with semantic information. This section reviews state-of-the-art methods for semantic scene reconstruction, focusing on how different approaches represent spatial and semantic information, their ability to operate in real time, and whether they support object-centric reasoning and which foundation models they leverage. Table~\ref{tab:sota:semantic_scene_reconstruction_overview} summarizes key characteristics of recent semantic scene reconstruction methods.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{
        c|
        >{\centering\arraybackslash}p{2.5cm}
        >{\centering\arraybackslash}p{2.0cm}
        c 
        >{\centering\arraybackslash}p{1.5cm}
        >{\centering\arraybackslash}p{1.5cm}
    }
    \toprule
    \textbf{Method}
    & \textbf{Representation}
    & \textbf{Foundation Model}
    & \textbf{Zero-Shot}
    & \textbf{Real-Time capability}
    & \textbf{Object-Centric} \\
    \midrule

    ConceptGraphs~\cite{guConceptGraphsOpenVocabulary3D2023}
    & Points
    & OpenCLIP~\cite{chertiReproducibleScalingLaws2022}, SAM~\cite{kirillovSegmentAnything2023}
    & \cmark
    & \xmark
    & \cmark \\

    Clio~\cite{jiangDualMapOnlineOpenVocabulary2025}
    & Points (dual map)
    & CLIP-based
    & \cmark
    & \cmark
    & \cmark \\

    LERF~\cite{kerrLERFICCV2023}
    & NeRF
    & OpenCLIP~\cite{chertiReproducibleScalingLaws2022}, DINOv2~\cite{oquabDINOv2LearningRobust2024}
    & \xmark
    & \xmark
    & \xmark \\

    OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}
    & region-based TSDF
    & SEEM~\cite{zouSegmentEverythingEverywhere2023}
    & \cmark
    & \cmark$^{\dagger}$
    & \cmark \\

    RayFronts~\cite{alamaRayFrontsOpenSetSemantic2025}
    & Voxel grid + semantic ray field
    & CLIP~\cite{radfordLearningTransferableVisual2021}
    & \cmark
    & \cmark
    & \xmark \\

    OTAS~\cite{schwaigerOTASOpenvocabularyToken2025}
    & Points
    & CLIP~\cite{radfordLearningTransferableVisual2021}, SAM2~\cite{raviSAM2Segment2024}, DINOv2~\cite{oquabDINOv2LearningRobust2024}
    & \cmark
    & \cmark
    & \xmark \\

    \bottomrule
\end{tabular}
\vspace{2pt}
{\footnotesize $^{\dagger}$Real-time refers to geometric reconstruction. Semantic inference runs asynchronously or at a lower rate.}
\caption{Overview of semantic scene reconstruction methods and their core design choices. The table compares representation, foundation model, zero-shot applicability, real-time capability, and object-centricity.}
\label{tab:sota:semantic_scene_reconstruction_overview}
\end{table}



% Concept Graphs and Clio

\citeauthor{guConceptGraphsOpenVocabulary3D2023}~\cite{guConceptGraphsOpenVocabulary3D2023} proposed
\textit{ConceptGraphs}, and \citeauthor{maggioClioRealtimeTaskDriven2024}~\cite{maggioClioRealtimeTaskDriven2024}
introduced \textit{Clio}, two object-centric semantic mapping frameworks that maintain persistent
object-level representations enriched with open-vocabulary semantic information.
Both methods represent the environment in an object-centric manner by extracting objects using
open-vocabulary object detectors (e.g., GroundingDINO~\cite{liuGroundingDINOMarrying2024}) and
segmenting them with \ac{SAM}~\cite{kirillovSegmentAnything2023}, yielding a bounding box and
segmentation mask for each detected object.
Semantic information is encoded using CLIP-based embeddings.
While ConceptGraphs explicitly reconstructs per-object 3D geometry, Clio represents objects as
spatially grounded semantic entities with associated embeddings and metadata.
Clio additionally incorporates a detector confidence score into the object representation~\cite{maggioClioRealtimeTaskDriven2024}. From this point onward, the two methods differ in how object-centric representations are stored, associated, and fused into a persistent semantic map.

ConceptGraphs~\cite{guConceptGraphsOpenVocabulary3D2023} updates its semantic map by associating
newly detected objects with existing ones if the DBSCAN-filtered 3D point cloud exhibits both
high semantic similarity, measured using OpenCLIP~\cite{chertiReproducibleScalingLaws2022}, and
sufficient 3D spatial overlap, quantified via \ac{IoU}.
If these conditions are met, the new point cloud is aligned to the existing object using \ac{ICP} and
merged, while semantic embeddings are aggregated over time.
Otherwise, a new object node is added to the map.
The resulting representation is a 3D scene graph, where nodes correspond to objects and edges
encode spatial relationships (e.g., above, next to) derived from relative geometry.
Each object is subsequently captioned using a large \ac{VLM} to generate a
human-readable description of its attributes and context~\cite{guConceptGraphsOpenVocabulary3D2023}.
A \ac{LLM} then processes the set of graph nodes and their captions under a system
prompt to refine object descriptions and infer higher-level relationships between objects, by connecting spatial edges between relevant nodes. This enables a range of downstream tasks, including question answering about the scene, robot manipulation, navigation, and localization.

Clio~\cite{maggioClioRealtimeTaskDriven2024} employs a dual-level object-centric memory consisting
of a frontend instance buffer and a backend abstract object memory.
The frontend maintains a transient buffer of newly detected object instances, each associated
with a spatial estimate, a CLIP-based semantic embedding, and a detector confidence score.
These instance-level representations are updated at a high frequency and may contain redundant
or noisy observations.
The backend maintains a compact set of stable, task-relevant object abstractions that serve as
long-term semantic memory.
In Clio, promotion from instance-level observations to abstract object anchors is governed by a mathematically defined information bottleneck objective operating in semantic embedding space, which compresses redundant observations while preserving task-relevant information. This enables efficient querying by retaining only a small number of semantically meaningful object anchors~\cite{westphalGeneralizedInformationBottleneck2025}.
During language-guided navigation, semantic reasoning is performed by matching the CLIP embedding
of the language query against the abstract object representations, while low-level navigation is
handled by a classical planner operating on the geometric occupancy map. While Clio improves efficiency by retaining only task-relevant object abstractions, this task-specific design limits map reusability, as querying non-task objects requires additional map reconstruction. As noted by \citeauthor{jiangDualMapOnlineOpenVocabulary2025}~\cite{jiangDualMapOnlineOpenVocabulary2025},
DualMap addresses this limitation by adopting a hybrid segmentation strategy for holistic
open-vocabulary mapping and by replacing costly inter-object merging with lightweight
intra-object consistency checks, enabling persistent semantic memory and more efficient
online operation.

\citeauthor{kerrLERFICCV2023}~\cite{kerrLERFICCV2023} introduced
\textit{LERF}, which builds upon \ac{NeRF}~\cite{mildenhallNeRFRepresentingScenes2020} to represent a
scene as a continuous volumetric function that maps a 3D position $\mathbf{x}$ and viewing
direction $\mathbf{d}$ to color and density,
$f(\mathbf{x}, \mathbf{d}) \rightarrow (\sigma, \mathbf{c})$, learned from multi-view RGB images
with known camera poses.
While a standard \ac{NeRF} reconstructs only geometry and appearance, LERF augments the radiance
field with an additional language embedding output,
$f(\mathbf{x}, \mathbf{d}) \rightarrow (\sigma, \mathbf{c}, \mathbf{e})$, where $\mathbf{e}$ denotes
a language-aligned semantic embedding.
This embedding field is learned by distilling image-level semantic features into 3D during
training.
Specifically, CLIP~\cite{radfordLearningTransferableVisual2021} and
DINO~\cite{jiangCLIPDINOVisual2024} embeddings are extracted from each training image at multiple
spatial scales and projected into the \ac{NeRF} via multi-view consistency, allowing each 3D
location to store a stable, language-aligned semantic representation.
This additional supervision enables dense 3D relevancy maps to be generated at query time by
computing similarity between a text embedding and the learned language field.
However, LERF requires extensive offline, scene-specific \ac{NeRF} training and does not support
incremental updates or online exploration, limiting its applicability to real-time or long-term
robotic mapping scenarios.

\citeauthor{yamazakiOpenFusionRealtimeOpenVocabulary2023}~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023} presented \textit{OpenFusion}, a real-time open-vocabulary semantic mapping framework that integrates volumetric \ac{TSDF}-based reconstruction with region-level semantic perception from foundation models. OpenFusion processes incoming RGB images using \ac{SEEM}~\cite{zouSegmentEverythingEverywhere2023}, a promptable \ac{VLM} capable of zero-shot semantic segmentation based on text or image prompts. For each frame, \ac{SEEM}~\cite{zouSegmentEverythingEverywhere2023} produces soft region confidence maps along with a semantic embedding vector for each region.

Depth images and camera poses are fused into a volumetric \ac{TSDF} map to reconstruct scene geometry.
Rather than directly storing semantic embeddings per voxel, OpenFusion associates voxels with
lightweight semantic region identifiers with a dictionary mapping each region ID to its corresponding
semantic embedding.
To establish temporal consistency, the current \ac{TSDF} map is raycast from the camera pose to render
the accumulated semantic regions into the image plane.
The rendered regions are then compared with the newly observed regions using geometric overlap
and semantic similarity, and region correspondences are solved via a Jonker-Volgenant assignment
algorithm~\cite{crouseImplementing2DRectangular2016}.
Matched regions are fused by updating confidence scores, while unmatched regions are added as new
semantic entries~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}.

Semantic embeddings are stored in a global dictionary indexed by region identifiers, rather than
per voxel, reducing memory consumption while enabling efficient semantic queries.
At query time, a natural-language prompt is embedded using the same \ac{VLM}, and
cosine similarity~\ref{eq:vlm_cosine_similarity} is computed between the query embedding and the
dictionary entries.
Regions with the highest similarity scores are retrieved and localized via their associated \ac{TSDF}
geometry.
This design enables real-time, open-vocabulary semantic mapping with efficient memory usage. However, OpenFusion does not maintain explicit object instances or support object-centric or
instance-level semantic reasoning, which could potentially be addressed by additional voxel clustering strategies. Similar to DualMap~\cite{jiangDualMapOnlineOpenVocabulary2025} and Clio~\cite{maggioClioRealtimeTaskDriven2024},
OpenFusion follows a local-global fusion paradigm, where semantic information extracted from
individual observations is incrementally associated with a global map representation.
However, unlike DualMap and Clio, which explicitly maintain semantic abstractions across time,
OpenFusion performs this fusion at the level of region-aligned geometry without constructing
persistent object instances or multi-level semantic memory~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023,jiangDualMapOnlineOpenVocabulary2025,maggioClioRealtimeTaskDriven2024}.

\citeauthor{alamaRayFrontsOpenSetSemantic2025}~\cite{alamaRayFrontsOpenSetSemantic2025} introduced
\textit{RayFronts}, a real-time open-set semantic mapping framework designed to support both
fine-grained semantic scene understanding within sensor range and semantic reasoning about regions
beyond the depth perception limit.
RayFronts represents the environment using a hybrid spatial abstraction consisting of a sparse
voxel-based semantic map for observed regions and a set of semantic ray frontiers anchored at map
boundaries for unobserved space.

Given posed RGB-D observations, RayFronts first extracts dense, language-aligned visual features
using an efficient vision-language encoder based on RADIO~\cite{ranzingerAMRADIOAgglomerativeVision2024} with a SIGLIP~\cite{zhaiSigmoidLossLanguage2023} adapter.
Within the sensor range, depth measurements are fused into a sparse voxel grid, where each occupied
voxel stores a persistent semantic embedding aggregated over time via a weighted averaging scheme.
This lightweight fusion strategy prioritizes computational efficiency and online operation, in
contrast to more complex multi-stage or object-level fusion pipelines~\cite{guConceptGraphsOpenVocabulary3D2023,maggioClioRealtimeTaskDriven2024,jiangDualMapOnlineOpenVocabulary2025}.

To reason about regions beyond the depth sensing horizon, RayFronts maintains a VDB-based occupancy
map that encodes free, occupied, and unknown space.
Three-dimensional frontiers are extracted as boundary voxels separating observed and unobserved
regions.
Instead of associating a single semantic descriptor with each frontier, RayFronts introduces semantic ray frontiers, in which multiple rays are attached to each frontier voxel.
Each ray is parameterized by its origin, direction, and a language-aligned visual embedding,
capturing semantic evidence observed along that direction in image space.

Semantic rays are discretized using angular bins and incrementally fused over time, allowing
multiple distinct semantic hypotheses to coexist at the same frontier without feature collisions.
This stands in contrast to VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, which maintains a
single-query, episodic value map conditioned on a specific object prompt.
This design enables multi-object and multi-query semantic reasoning in unobserved space and supports
rough triangulation of distant semantic entities as exploration progresses.
Importantly, both voxel and ray representations store task-agnostic visual embeddings rather than
query-specific scores, allowing the semantic map to be queried at arbitrary times using text or
image prompts via cosine similarity~\cite{alamaRayFrontsOpenSetSemantic2025}.

RayFronts is explicitly planner-agnostic and does not prescribe a specific exploration strategy.
Instead, it provides a persistent semantic scene representation that can be consumed by downstream
planners for object search, exploration, or navigation in large-scale and unbounded environments.
By decoupling semantic mapping from planning, RayFronts enables flexible integration with a wide
range of exploration and decision-making frameworks while maintaining real-time performance in
outdoor settings~\cite{alamaRayFrontsOpenSetSemantic2025}.

\citeauthor{schwaigerOTASOpenvocabularyToken2025}~\cite{schwaigerOTASOpenvocabularyToken2025}
introduced \textit{OTAS} (Open-vocabulary Token Alignment for Outdoor Segmentation), a training-free
semantic segmentation and reconstruction framework designed for unstructured outdoor environments.
Unlike prior open-vocabulary mapping approaches that rely on object-centric segmentation priors,
OTAS extracts semantic structure directly from intermediate token representations of frozen vision
and vision-language foundation models.

OTAS uses DINOv2~\cite{oquabDINOv2LearningRobust2024} to generate dense visual embeddings at the image
patch level, which capture visual similarity but are not language-aligned.
To obtain language grounding, dense patch-level embeddings are extracted from CLIP~\cite{radfordLearningTransferableVisual2021},
which are language-aligned but noisy and view-dependent.
The core idea of OTAS is to cluster visually similar patches based on DINOv2 embeddings and align
these clusters with CLIP~\cite{radfordLearningTransferableVisual2021} embeddings via masked average pooling, yielding language-grounded semantic
regions without relying on object-centric segmentation~\cite{schwaigerOTASOpenvocabularyToken2025}.

Semantic queries are performed by embedding text prompts using CLIP’s text encoder and computing
cosine similarity against the cluster-level embeddings.
Optionally, a frozen mask refinement network such as SAM2~\cite{raviSAM2Segment2024} can be used to
upsample coarse relevance maps to pixel-level segmentations.
For multi-view reconstruction, language-grounded features are projected into 3D using depth and
camera poses and fused into a persistent point cloud representation, enabling open-vocabulary
querying via cosine similarity. Unlike prior object-centric mapping approaches such as
ConceptGraphs~\cite{guConceptGraphsOpenVocabulary3D2023} and Clio~\cite{maggioClioRealtimeTaskDriven2024}, OTAS does not require explicit object detection or instance segmentation, allowing it to capture amorphous or unstructured semantic entities commonly found in outdoor environments, such as vegetation, terrain types and natural landmarks~\cite{schwaigerOTASOpenvocabularyToken2025}. 

\section{Identified Research Gaps in State of the Art}

Table~\ref{tab:sota:summary} provides a overview of recent methods for open-vocabulary semantic exploration and mapping. The methods are categorized into four main groups: (a) reinforcement learning-based approaches, (b) foundation-model-based exploration, (c) foundation-model-based exploration with persistent semantic mapping, and (d) mapping-centric methods. The second column indicates the type of exploration strategy employed, while the third column highlights whether each method incorporates an explicit exploration-exploitation tradeoff mechanism, i.e., whether the balance between exploring new areas and exploiting known information can be controlled. The fourth and fifth columns denote whether the method supports zero-shot generalization to unseen environments and real-time operation, respectively.The final column indicates whether the method maintains a persistent semantic memory.

\begin{table}[htbp]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{
        l|
        >{\centering\arraybackslash}p{1.8cm}
        >{\centering\arraybackslash}p{2.2cm}
        >{\centering\arraybackslash}p{1.5cm}
        >{\centering\arraybackslash}p{1.5cm}
        >{\centering\arraybackslash}p{2.2cm}
        >{\centering\arraybackslash}p{2.2cm}
    }
    \toprule
    \textbf{Method}
    & \textbf{Exploration Type}
    & \textbf{Explicit Exploration-Exploitation Tradeoff}
    & \textbf{Zero-Shot}
    & \textbf{Real-Time}
    & \textbf{Persistent Semantic Memory} \\

    \midrule
    \multicolumn{6}{l}{\textbf{(a) Reinforcement learning-based}} \\
    \addlinespace[2pt]

    ZSON~\cite{majumdarZSONZeroShotObjectGoal2023}
    & Reinforcement Learning
    & \xmark
    & \xmark
    & \xmark~(scene specific training)
    & \xmark \\

    PONI~\cite{ramakrishnanPONIPotentialFunctions2022}
    & Reinforcement Learning
    & \cmark
    & \xmark
    & \xmark~(scene specific training)
    & \xmark \\

    \addlinespace[1pt]
    \multicolumn{6}{l}{\textbf{(b) Foundation-model-based exploration}} \\
    \addlinespace[1pt]

    VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}
    & Foundation-model
    & \xmark
    & \cmark
    & \cmark
    & \xmark \\

    \addlinespace[1pt]
    \multicolumn{6}{l}{\textbf{(c) Foundation-model + persistent semantic mapping}} \\
    \addlinespace[1pt]

    OneMap~\cite{buschOneMapFind2025}
    & Foundation-model
    & \xmark
    & \cmark
    & \cmark
    & \cmark \\

    DualMap~\cite{jiangDualMapOnlineOpenVocabulary2025}
    & Foundation-model
    & \xmark
    & \cmark
    & \cmark
    & \cmark \\

    RayFronts~\cite{alamaRayFrontsOpenSetSemantic2025}
    & Foundation-model
    & \xmark~(planner-agnostic)
    & \cmark
    & \cmark
    & \cmark \\

    \addlinespace[1pt]
    \multicolumn{6}{l}{\textbf{(d) Mapping-centric methods}} \\
    \addlinespace[1pt]

    ConceptGraphs~\cite{guConceptGraphsOpenVocabulary3D2023}
    & Mapping-centric
    & \xmark
    & \cmark
    & \xmark
    & \cmark \\

    OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}
    & Mapping-centric
    & N/A
    & \cmark
    & \cmark
    & \cmark \\

    OTAS~\cite{schwaigerOTASOpenvocabularyToken2025}
    & Mapping-centric
    & N/A
    & \cmark
    & \cmark
    & \cmark \\

    \midrule

    \textbf{SAGE} (this work)
    & Foundation-model
    & \cmark
    & \cmark
    & \cmark
    & \cmark \\

    \bottomrule
\end{tabular}
\caption{Comparison of open-vocabulary semantic exploration and mapping methods.
The table highlights differences in exploration strategy, semantic persistence,
zero-shot generalization, and explicit exploration-exploitation control.}
\label{tab:sota:summary}
\end{table}


The main limitation of reinforcement learning-based approaches lies in their limited adaptability
to unseen environments, where structural differences such as room layouts, object appearances, and
lighting conditions deviate from the training distribution~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023}.
These methods typically require extensive retraining or fine-tuning to generalize effectively,
which limits their applicability in dynamic or real-world settings.
Most importantly, RL-based approaches do not construct persistent semantic memory during exploration,
preventing the reuse of acquired knowledge across tasks or environments unless such behavior is
explicitly encoded during training~\cite{majumdarZSONZeroShotObjectGoal2023}.

Foundation-model-based exploration methods mitigate this limitation by leveraging pretrained vision-
language models to guide exploration in a zero-shot manner toward semantically relevant regions.
VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023} combines classical frontier-based exploration with
semantic value maps derived from image-text similarity, enabling efficient zero-shot navigation.
However, the semantic representation in VLFM is episodic and reset after each navigation episode, preventing long-term semantic reasoning or refinement over time.

OneMap~\cite{buschOneMapFind2025} addresses this limitation by accumulating semantic embeddings in a
probabilistic 2D map, enabling persistent semantic querying across exploration episodes. While OneMap integrates semantic similarity maps with open-vocabulary object detectors through consensus filtering, the detector output acts as a hard gating mechanism. Consequently, object hypotheses are only formed when the detector confidence exceeds a predefined threshold, preventing the accumulation of weak but consistent semantic evidence across views when detector confidence remains low.


DualMap~\cite{jiangDualMapOnlineOpenVocabulary2025} improves robustness by introducing explicit object
detection and an object-centric dual-map structure that separates short-term observations from
long-term semantic anchors.
While this design enhances semantic stability, both OneMap~\cite{buschOneMapFind2025} and DualMap~\cite{jiangDualMapOnlineOpenVocabulary2025} primarily exploit previously
observed semantic information and do not provide mechanisms to actively balance exploration and
exploitation.
Semantic reasoning is applied mainly after anchors have been formed, limiting their ability to
guide early exploration.

RayFronts~\cite{alamaRayFrontsOpenSetSemantic2025} extends semantic reasoning beyond observed space by
introducing semantic ray frontiers, enabling reasoning about unobserved regions outside the depth
sensing range.
Although this representation supports persistent semantic querying and multi-object reasoning, the
framework is explicitly planner-agnostic and does not define a concrete exploration strategy or an
exploration-exploitation control mechanism.

Mapping-centric approaches such as ConceptGraphs~\cite{guConceptGraphsOpenVocabulary3D2023},
OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}, and
OTAS~\cite{schwaigerOTASOpenvocabularyToken2025} focus on building rich semantic scene
representations but do not address exploration behavior.
These methods lack mechanisms for semantic-driven frontier selection or tunable control of
exploration based on uncertainty or memory reliability.

In summary, existing approaches either perform semantic exploration without persistent, revisable
semantic memory, or construct persistent semantic maps without explicitly guiding exploration.
No prior work jointly addresses zero-shot semantic exploration, persistent semantic memory, and an
explicit, adjustable exploration-exploitation tradeoff within a unified framework. Furthermore, existing methods rely predominantly on single-source semantic signals, lacking
multi-source confidence fusion to robustly suppress false positives during exploration.
These identified research gaps motivate the development of \textbf{\ac{SAGE}}, which aims to integrate these
capabilities into a cohesive system for robust and efficient open-vocabulary semantic exploration.

\newpage