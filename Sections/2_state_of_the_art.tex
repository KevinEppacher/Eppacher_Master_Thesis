% chktex-file 44

\chapter{State of the Art}
\label{ch:sota}

This work introduces a hybrid semantic exploration framework that combines open-vocabulary
semantic perception with autonomous exploration and persistent spatial memory, which combines the fields of geometric exploration, vision-language-guided exploration, and semantic mapping based on the successes encountered by incorporating semantic information from large pretrained foundation models into traditional exploration and mapping pipelines. However, these novel semantic methods still rely on design and data representation patterns derived from classic geometric exploration and mapping methods.

Semantic exploration approaches differ fundamentally in how exploration behavior is generated.
Some methods learn end-to-end navigation policies using reinforcement learning or imitation learning,
where exploration strategies are implicitly encoded in a trained policy optimized via task-specific
reward functions~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023}. Other approaches adopt modular architectures that integrate pretrained vision-language models with classical mapping and planning techniques~\cite{yokoyamaVLFMVisionLanguageFrontier2023, buschOneMapFind2025,gadreCoWsPastureBaselines2022}.

Reinforcement learning and imitation learning approaches have demonstrated promising results in
semantic object search tasks by training agents to navigate toward target objects based on high-level
semantic cues~\cite{majumdarZSONZeroShotObjectGoal2023}, primarily within simulated environments. Their main advantage lies in avoiding hand-designed exploration heuristics, as complex behaviors can be learned directly from data through reward optimization~\cite{ghasemiComprehensiveSurveyReinforcement2025}. However, such policies typically require extensive training on large datasets (e.g. Replica~\cite{straubReplicaDatasetDigital2019}, Habitat~\cite{savvaHabitatPlatformEmbodied2019}), exhibit limited generalization to unseen environments or object~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023} categories, and lack interpretability due to their black-box nature~\cite{ghasemiComprehensiveSurveyReinforcement2025}.

In contrast, modular approaches leverage semantic representations provided by large-scale
pretrained \acp{VLM} to guide exploration decisions without task-specific retraining~\cite{alamaRayFrontsOpenSetSemantic2025}.
By combining explicit geometric mapping with semantic reasoning derived from foundation models,
these systems can achieve zero-shot generalization to novel objects and environments while retaining
interpretability and adaptability~\cite{yokoyamaVLFMVisionLanguageFrontier2023}. Several works extend this paradigm by constructing persistent semantic maps that retain knowledge of previously explored areas~\cite{buschOneMapFind2025,huangVisualLanguageMaps2023}, enabling more efficient multi-object search and the integration of high-level natural language instructions~\cite{guConceptGraphsOpenVocabulary3D2023}.

Finally, object detection and promptable vision-language models play a crucial role in enabling
open-vocabulary semantic understanding for exploration tasks. Recent advances in grounding-capable
detectors and segmentation models facilitate zero-shot object recognition based on text prompts,
allowing robots to identify and localize previously unseen objects~\cite{wangYOLOERealTimeSeeing2025,liGroundedLanguageImagePretraining2022,liuGroundingDINOMarrying2024}. These models form the semantic
foundation upon which hybrid exploration systems are built, enabling flexible object
search in diverse real-world environments.

\section{Frontier-Based Exploration}

Frontier-based exploration is a classical and commonly used approach for autonomous mapping and navigation in unknown environments~\cite{lluviaActiveMappingRobot2021}. A \textit{frontier} is defined as the boundary between known free space and unknown regions of the environment~\cite{lluviaActiveMappingRobot2021}. Frontier-based exploration relies on the principle that unexplored areas adjacent to known free regions provide the highest potential information gain. To identify such frontiers, the robot must maintain a global representation of the environment, typically an occupancy grid or voxel map, where each cell is classified as \textit{free}, \textit{occupied}, or \textit{unknown}, based on sensor observations from \ac{LiDAR} or RGB-D cameras. The robot then iteratively selects and navigates to frontiers to expand its knowledge of the environment.

\citeauthor{quinApproachesEfficientlyDetecting2021}~\cite{quinApproachesEfficientlyDetecting2021} evaluated three commonly used frontier extraction methods that differ primarily in computational efficiency and scalability.  
The first approach, known as the \textit{Naïve Active Area (NaïveAA)} method, evaluates every cell in the occupancy grid to determine whether it is free and has at least one unknown neighbor. Although this approach is conceptually simple and accurate for small-scale maps, it becomes computationally expensive for larger environments and often produces small, fragmented frontier clusters.

The second approach, the \textit{Wavefront Frontier Detector (WFD)}~\cite{quinApproachesEfficientlyDetecting2021,topiwalaFrontierBasedExploration2018}, improves efficiency by using a breadth-first search (BFS) to identify connected frontier regions without exhaustively scanning the entire map. Unlike the NaïveAA method, WFD directly extracts continuous frontier clusters rather than treating each frontier cell individually, significantly reducing redundant computations.

The third method, the \textit{Frontier-Tracing Frontier Detection (FTFD)}~\cite{quinApproachesEfficientlyDetecting2021}, further enhances performance by incrementally updating frontier information using only the most recent sensor observations. Instead of re-evaluating the full map, FTFD initiates a BFS from previously known frontier cells that remain within the active area and from the endpoints of the latest sensor rays. Newly visible free-space cells along the scan boundary are evaluated as potential frontiers, while outdated frontier cells that are now occupied or re-observed are removed. By restricting computation to the local scan perimeter, FTFD achieves significantly faster update rates than NaïveAA and WFD, supporting real-time frontier detection even in large-scale environments.

After frontiers have been extracted, a selection strategy determines which frontier the robot should explore next. Simple heuristics such as \textit{nearest-frontier selection} minimize travel distance but can lead to oscillatory behavior between nearby frontiers. Alternatively, selecting the \textit{largest frontier} favors unexplored regions of higher spatial extent, reducing dead-end visits but increasing traversal cost.  
To address these trade-offs, \citeauthor{bourgaultInformationBasedAdaptive2002}~\cite{bourgaultInformationBasedAdaptive2002} introduced a \textit{utility-based frontier selection} framework that combines multiple criteria, such as distance, frontier size, and expected information gain, into a unified objective function. This approach enables more balanced decision-making, improving overall exploration efficiency and map completeness. Many subsequent works have built upon this foundation, incorporating additional factors such as energy consumption, obstacle density, and dynamic environment considerations into the utility function~\cite{lluviaActiveMappingRobot2021}.

However, all these methods are primarily designed for geometric exploration without incorporating semantic understanding. As a result, they are optimized for complete map coverage rather than goal-directed exploration tasks. In scenarios where a robot must locate specific objects or regions based on semantic cues, purely geometric frontier selection often leads to inefficient search behavior and unnecessary traversal. This motivates the integration of semantic reasoning into the frontier-based exploration process, where frontiers can be prioritized not only by geometric utility but also by their semantic relevance to the task objective.

\section{RL-based Semantic Exploration}
\label{sec:sota:rl_semantic_exploration}

In contrast to geometric exploration, which aims to maximize map coverage using the shortest possible path and time, semantic exploration focuses on efficiently locating specific objects or regions of interest described in high-level semantic terms. The objective is to minimize path length and exploration time while prioritizing areas likely to contain relevant targets rather than achieving complete spatial coverage.  

Semantic exploration approaches differ fundamentally in how exploration behavior is generated and optimized. \ac{RL} and imitation learning methods have demonstrated promising results by training agents to navigate toward target objects based on high-level semantic cues~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023,ramakrishnanPONIPotentialFunctions2022}. In these approaches, an agent learns a policy that maps high-dimensional sensory observations to low-level control actions by optimizing a task-specific reward function, rather than relying on explicitly designed exploration heuristics~\cite{ghasemiComprehensiveSurveyReinforcement2025}.

In the context of semantic exploration, \ac{RL}-based methods are typically formulated as Markov Decision Processes or, more commonly in embodied navigation, as partially observable Markov Decision Processes (\acp{POMDP})~\cite{thrunProbabilisticRobotics2006}. At an abstract level, a \ac{POMDP} can be defined as a tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where $\mathcal{S}$ denotes the latent environment states, $\mathcal{A}$ the set of possible actions (e.g., move forward, turn left/right), $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ the state transition probability function, $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ the reward function, and $\gamma \in [0,1)$ the discount factor. The agent does not observe the full state directly but instead receives high-dimensional, partial observations (e.g., RGB images, depth measurements, or semantic representations), from which it must infer an internal belief about the environment~\cite{ghasemiComprehensiveSurveyReinforcement2025}.

The reward function encodes task objectives such as reaching a target object, minimizing
path length, or maintaining correct orientation, thereby implicitly shaping the agent’s
exploration behavior~\cite{ghasemiComprehensiveSurveyReinforcement2025}.
This paradigm has been successfully applied to semantic object-goal navigation, where agents
are trained to locate objects specified by high-level semantic descriptions
(e.g., object categories or language embeddings)~\cite{majumdarZSONZeroShotObjectGoal2023,ramrakhyaPIRLNavPretrainingImitation2023,ramakrishnanPONIPotentialFunctions2022}.

A key advantage of \ac{RL}-based semantic exploration lies in its flexibility. Complex navigation behaviors can be learned directly from interaction data without manually designing exploration strategies. However, this flexibility comes at the cost of extensive training requirements, limited interpretability, and reduced robustness to domain shifts, as policies often overfit to the visual statistics and dynamics of the training environments~\cite{ghasemiComprehensiveSurveyReinforcement2025,majumdarZSONZeroShotObjectGoal2023}. As a result, many such approaches are primarily evaluated in simulation and struggle to generalize to previously seen scenes, object appearances, or sensor configurations. Commonly, \ac{RL} algorithms are optimized via policy gradient methods~\cite{thomasPolicyGradientMethods2017}, which directly adjust the policy parameters $\theta$ to maximize the expected cumulative reward $J(\theta)$:

\begin{equation}
\nabla_{\theta} J(\theta) =
\mathbb{E}_{\tau \sim \pi_{\theta}}
\left[
\sum_{t=0}^{T}
\nabla_{\theta} \log \pi_{\theta}(a_t \mid o_t) \, R(\tau)
\right],
\end{equation}

where $\tau$ denotes trajectories sampled from the policy $\pi_{\theta}$, $a_t$ and $o_t$ are the action and observation at time $t$, respectively, and $R(\tau)$ is the cumulative reward obtained over trajectory $\tau$.

% ---------------------- ZSON ----------------------

\citeauthor{majumdarZSONZeroShotObjectGoal2023}~\cite{majumdarZSONZeroShotObjectGoal2023} proposed \ac{ZSON}, a zero-shot object navigation framework that leverages the shared embedding space of CLIP~\cite{radfordLearningTransferableVisual2021} to guide navigation policies trained via \ac{RL}. 
During training, the agent observes RGB images and previous actions, while the navigation goal is specified by the CLIP embedding of an image containing the target object. 
The policy is optimized using a shaped reward function:

\begin{equation}
r_t = r_{\text{success}} + r_{\text{angle-success}} - \Delta d_{tg} - \Delta a_{tg} + r_{\text{slack}},
\end{equation}

where $r_{\text{success}}$ rewards successful target localization, $r_{\text{angle-success}}$ encourages correct orientation, $\Delta d_{tg}$ and $\Delta a_{tg}$ penalize distance and angular deviation, respectively, and $r_{\text{slack}}$ promotes efficient navigation. 
At inference time, the image-based goal embedding is replaced by the CLIP embedding of a textual object description, enabling zero-shot generalization to unseen object categories. 
The action space is discrete and consists of \textit{move forward}, \textit{turn left}, \textit{turn right}, and \textit{stop}. The reward function is used exclusively during training to learn the navigation policy during inference, the agent selects actions solely based on the learned policy without
access to the reward signal.

While \ac{ZSON} demonstrates strong zero-shot object generalization, exploration behavior remains implicitly encoded in the learned policy, which makes it difficult to interpret or adapt to new scenarios. As a consequence, the agent tends to revisit visually familiar regions and lacks explicit mechanisms for systematic exploration of unknown space. Furthermore, the end-to-end \ac{RL} formulation reduces interpretability and necessitates retraining when visual conditions or environment layouts deviate from the training distribution. Nevertheless, \ac{ZSON} represents a significant step toward flexible and generalizable semantic exploration by integrating vision-language models with reinforcement learning.

% ---------------------- PIRLNav ----------------------

\citeauthor{ramrakhyaPIRLNavPretrainingImitation2023}~\cite{ramrakhyaPIRLNavPretrainingImitation2023} introduced \ac{PIRLNav}, a two-stage framework that combines behavior cloning (\ac{BC}) with \ac{RL} to improve generalization in semantic navigation tasks. In the first stage, a navigation policy is pretrained via imitation learning on large-scale human demonstrations, learning to reproduce expert actions from observations that include RGB images, pose information, and a categorical goal representation. This pretraining stage provides a strong initialization, which reduces the training time and sample complexity required for subsequent reinforcement learning. Furthermore, with respect to interpretability, the use of \ac{BC} allows for some insight into the learned behavior, as it is directly derived from human demonstrations. The pre-trained ObjectNav humand demonstraded policy is trained using supervised learning to minimize the cross-entropy loss between predicted and expert actions:

\begin{equation}
\mathcal{L}_{\text{BC}} = - \sum_{t=1}^{T} \log \pi(a_t^* \mid s_t),
\end{equation}

where \(a_t^*\) denotes the expert action at time \(t\) and \(s_t\) represents the agent's observation. The ObjectNav policy is trained with a CNN+RNN Network, in which a pre-trained vision backbone, \ac{DINO}, extracts visual features from RGB images and feeds into the nav policy network along with pose and goal information.

In the second stage, the pretrained policy is fine-tuned using \ac{RL}, where the objective is to maximize the expected discounted reward:

\begin{equation}
\pi^{*} = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} 
\left[ \sum_{t=1}^{T} \gamma^{t-1} r_t \right].
\end{equation}

where \(\tau\) denotes trajectories sampled from the policy \(\pi\), \(r_t\) is the reward at time \(t\), and \(\gamma\) is the discount factor. Although this hybrid training strategy improves sample efficiency and navigation robustness in simulation, the resulting policy remains a black-box model. 
It does not maintain an explicit semantic memory and requires retraining to adapt to new visual domains or sensor modalities.

% ---------------------- PONI ----------------------

To address the limited interpretability of end-to-end policies, \citeauthor{ramakrishnanPONIPotentialFunctions2022}~\cite{ramakrishnanPONIPotentialFunctions2022} proposed \ac{PONI}, a supervised, map-based semantic exploration framework. Rather than learning low-level actions, \ac{PONI} predicts high-level exploration objectives in the form of potential fields defined over a partial semantic map. Two complementary potentials are estimated: an area potential that encourages exploration of unknown space, and an object potential that estimates proximity to the target category. Exploration decisions are derived by scoring geometric frontiers according to a weighted combination of these potentials:

\begin{equation}
U_t(f) = \alpha \, U_t^a(f) + (1 - \alpha) \, U_t^o(f),
\end{equation}

where $\alpha$ explicitly controls the trade-off between exploration and exploitation. 
This formulation yields interpretable and stable exploration behavior and decouples high-level decision-making from low-level navigation, which is handled by a classical navigation planner.

However, \ac{PONI} relies on dense semantic annotations and assumes a fixed, closed set of object categories encountered during training.~\cite{ramakrishnanPONIPotentialFunctions2022} used Mask~R-CNN~\cite{heMaskRCNN2018} to generate semantic maps with 80 object classes from the \ac{COCO} dataset~\cite{linMicrosoftCOCOCommon2015}, which was finetuned within the Gibson dataset~\cite{xiaGibsonEnvRealWorld2018}. As a result, it does not support open-vocabulary or zero-shot object search and remains sensitive to annotation noise and domain shifts.

RL-based and supervised semantic exploration methods demonstrate the feasibility of learning navigation behavior from semantic cues, but they exhibit recurring limitations that motivate alternative approaches. These include extensive training requirements, limited interpretability, closed-set semantics, lack of persistent semantic memory, and reduced robustness to domain changes. 
These structural shortcomings have motivated recent work toward modular exploration frameworks that integrate pretrained vision-language models, which are discussed in the following section.

% \begin{table}[H]
% \centering
% \footnotesize
% \renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{6pt}
% \begin{tabularx}{\textwidth}{
% |l|c|c|c|c|c|c|c}
% \hline
% \textbf{Capability / Method} &
% \textbf{DRL} &
% \textbf{BC} &
% \textbf{Supervised} &
% \textbf{CLIP} &
% \textbf{DINO} &
% \textbf{DAS} &
% \textbf{CAS} \\
% \hline

% Limited policy-level interpretability
% & \cite{majumdarZSONZeroShotObjectGoal2023}
% & \cite{ramrakhyaPIRLNavPretrainingImitation2023}
% & \xmark
% & \xmark
% & \xmark
% & \cite{majumdarZSONZeroShotObjectGoal2023}
% & \xmark \\
% \hline

% Explicit semantic memory
% & \xmark
% & \xmark
% & \xmark
% & \xmark
% & \xmark
% & \xmark
% & \xmark \\
% \hline

% Zero-shot capability
% & \xmark
% & \xmark
% & \xmark
% & \cite{majumdarZSONZeroShotObjectGoal2023}
% & \xmark
% & \xmark
% & \xmark \\
% \hline

% Explicit object detection module
% & \xmark
% & \xmark
% & \xmark
% & \xmark
% & \xmark
% & \xmark
% & \xmark \\
% \hline

% Closed-set object categories
% & \cite{ramakrishnanPONIPotentialFunctions2022}
% & \cite{ramrakhyaPIRLNavPretrainingImitation2023}
% & \cite{ramakrishnanPONIPotentialFunctions2022}
% & \xmark
% & \cite{ramrakhyaPIRLNavPretrainingImitation2023}
% & \xmark
% & \xmark \\
% \hline

% Requires retraining for domain shift
% & \cite{majumdarZSONZeroShotObjectGoal2023}
% & \cite{ramrakhyaPIRLNavPretrainingImitation2023}
% & \cite{ramakrishnanPONIPotentialFunctions2022}
% & \xmark
% & \xmark
% & \xmark
% & \xmark \\
% \hline

% Exploration strategy learned
% & \cite{majumdarZSONZeroShotObjectGoal2023}
% & \cite{ramrakhyaPIRLNavPretrainingImitation2023}
% & \xmark
% & \xmark
% & \xmark
% & \xmark
% & \xmark \\
% \hline

% Exploration strategy explicit
% & \xmark
% & \xmark
% & \cite{ramakrishnanPONIPotentialFunctions2022}
% & \xmark
% & \xmark
% & \xmark
% & \xmark \\
% \hline

% Planner-based navigation
% & \xmark
% & \xmark
% & \cite{ramakrishnanPONIPotentialFunctions2022}
% & \xmark
% & \xmark
% & \xmark
% & \xmark \\
% \hline

% \end{tabularx}
% \caption{Capability--implementation matrix for learning-based semantic exploration approaches.
% Rows indicate key system properties or limitations, while columns denote architectural or learning
% mechanisms through which these properties arise. Each cell lists representative works that
% exhibit the corresponding association.}
% \label{tab:sota:learning_capability_matrix}
% \end{table}

\section{Foundation-Model Exploration}

Therefore, recent research has shifted toward leveraging large-scale pretrained vision-language models (\acp{VLM}) that provide rich semantic representations and zero-shot generalization capabilities. By integrating these models into modular exploration frameworks, it becomes possible to overcome many of the limitations associated with learned policies while retaining the benefits of semantic reasoning for efficient multi-object search.

A representative example of this paradigm is the \ac{ESC} framework proposed by \citeauthor{zhouESCExplorationSoft2023}~\cite{zhouESCExplorationSoft2023}, which augments traditional frontier-based exploration with semantic cues derived from pretrained \acp{VLM}. Specifically, \ac{ESC} combines a grounded object detector, \ac{GLIP}~\cite{liGroundedLanguageImagePretraining2022}, with a \ac{LLM} (either ChatGPT or DeBERTa) to generate semantic priors that guide exploration decisions. Frontiers are scored based on both geometric utility and semantic relevance to the target object, as formulated in Equation~\ref{eq:esc}:

\begin{equation}
\label{eq:esc}
P(F) = P(F \mid d_i^{t}, o^{t}, r^{t}),
\end{equation}

where \(P(F)\) denotes the probability of selecting frontier \(F\), conditioned on detected objects \(d_i^{t}\), current observations \(o^{t}\), and the robot pose \(r^{t}\) at time \(t\). This formulation is implemented using Probabilistic Soft Logic (PSL), which fuses visual detections with language-derived priors about object co-occurrences and spatial relationships. The robot then selects the frontier with the highest combined score, balancing geometric and semantic information to improve search efficiency.

In practice, \ac{ESC} employs \ac{GLIP}~\cite{liGroundedLanguageImagePretraining2022} as the detection backbone to compute 2D bounding boxes, class labels, and confidence scores for objects within the robot’s \ac{FOV}. While effective in simulation, the approach introduces notable computational overhead due to repeated \ac{LLM} inference and PSL optimization. Moreover, its performance is highly dependent on the reliability of object detections-false positives can misguide frontier scoring, and incorrect commonsense priors may further bias exploration. 

Through ablation studies, \citeauthor{zhouESCExplorationSoft2023} observed that object-object and object-room relational priors can occasionally degrade performance, as commonsense relationships are inherently probabilistic rather than deterministic. Additionally, while \ac{ESC} maintains a local semantic map during navigation, it lacks mechanisms for long-term memory or belief revision. Consequently, once an incorrect detection or prior is introduced, the framework has no learned means of down-weighting or correcting it over time, which can lead to persistent semantic inconsistencies during extended exploration.

In contrast to such reasoning-heavy systems, \citeauthor{gadreCoWsPastureBaselines2022}~\cite{gadreCoWsPastureBaselines2022} proposed \ac{CoW}, a lightweight vision-language exploration framework that relies purely on image-text alignment from CLIP~\cite{radfordLearningTransferableVisual2021} without requiring explicit frontier detection, semantic mapping, or object segmentation. The method guides the robot toward directions with the highest cosine similarity between the current visual observation and the target object description, defined as:

\begin{equation}
\label{eq:cosine_similarity}
\mathrm{\cos\_sim}(\mathbf{u}, \mathbf{v})
=
\frac{\mathbf{u} \cdot \mathbf{v}}
{\lVert \mathbf{u} \rVert \, \lVert \mathbf{v} \rVert},
\end{equation}

where \(\mathbf{u}\) and \(\mathbf{v}\) denote the CLIP embeddings of the current image and the target text prompt, respectively. The robot moves forward when the similarity exceeds a predefined threshold and rotates otherwise to maximize alignment with the target.  

By eliminating explicit detectors and handcrafted mapping, \ac{CoW} offers a computationally efficient and conceptually simple baseline for open-vocabulary navigation. However, this simplicity comes at the cost of robustness. The system is highly sensitive to viewpoint variations and clutter, as cosine similarity does not always correlate with true object presence. Without spatial memory or geometric reasoning, the robot may oscillate near false positives or become trapped in local minima. Moreover, because similarity scores vary across object categories, no universal threshold can be established for all targets, resulting in inconsistent stopping behavior and reduced reliability during multi-object search.

Building upon this idea of integrating semantics into classical exploration, 
\citeauthor{chenHowNotTrain2023}~\cite{chenHowNotTrain2023} introduced \ac{SemUtil}, 
a fully modular and training-free framework for object-goal navigation that combines classical SLAM-based mapping 
with pretrained perception and language models. 
In contrast to reinforcement learning or imitation learning approaches, 
\ac{SemUtil} leverages explicit geometric and semantic reasoning through three core components: 
a 2D occupancy map for frontier extraction, a semantic point cloud generated by projecting Mask~R-CNN detections into 3D space, 
and a spatial scene graph for high-level semantic reasoning. 
These three representations collectively form a structured scene model that supports geometric planning, semantic propagation, 
and reasoning about unexplored regions~\cite{chenHowNotTrain2023}.  

The central element of \ac{SemUtil} is the \textit{utility module}, 
which fuses geometric frontiers with semantic priors to determine the most promising frontier to explore next. 
For each map cell, a \textit{utility score} is computed by combining the geometric frontier characteristics, 
the CLIP-based cosine similarity between the current observation and the target object description, 
and the semantic cues from the 3D point cloud (e.g., class IDs from Mask~R-CNN). 
This results in a utility map that prioritizes frontiers both spatially and semantically,
as illustrated in Figure~3 of the original paper (showing the interaction between geometric and semantic utilities)~\cite{chenHowNotTrain2023}.
 
Importantly, the utility map in \ac{SemUtil} is not persistent, it is recomputed at every timestep based solely on the current observation 
and semantic point cloud, without maintaining a long-term memory of past detections or map updates. 
While this design simplifies computation and eliminates the need for training, 
it also limits the system’s ability to reason over time or correct previous errors. 
The reliance on a closed-set detector (Mask~R-CNN) restricts open-vocabulary generalization, 
and any incorrect detection directly corrupts the semantic point cloud, 
thereby distorting the frontier scoring and leading to suboptimal exploration decisions.  
Furthermore, since the framework lacks belief revision or memory-based fusion, 
false detections persist until they leave the robot’s current field of view, 
reducing consistency and efficiency in long-term navigation.

Unlike \citeauthor{chenHowNotTrain2023}~\cite{chenHowNotTrain2023}, who construct a utility map from class-based detections and semantic projections that are recomputed each step, thereby risking information loss or semantic inconsistency, \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023} directly leverages pretrained vision-language models to compute semantic value maps from raw RGB observations. Rather than relying on symbolic object classes, \ac{VLFM} computes a continuous image-text similarity score between the robot’s current observation and the target text prompt using BLIP-2~\cite{liBLIP2BootstrappingLanguageImage2023}, as formulated through the cosine similarity function in Equation~\ref{eq:cosine_similarity}~\cite{gadreCoWsPastureBaselines2022}.  

The resulting similarity values are spatially projected onto a top-down occupancy grid according to the robot’s \ac{FOV}, forming a \textit{value map} that quantifies the semantic likelihood of each region leading toward the target object. To account for reduced reliability near the image periphery, a Gaussian weighting function attenuates confidence values based on angular distance from the optical axis (see Fig.~3 in the original paper~\cite{yokoyamaVLFMVisionLanguageFrontier2023}). This value map is continuously updated through a weighted averaging scheme that fuses new and previous similarity scores according to their confidence weights, enabling smooth map updates and spatial consistency across frames.  

During exploration, the \ac{VLFM} system fuses this value map with a geometrically extracted frontier map to select the next exploration goal, the frontier with the highest semantic value is chosen as the next waypoint. Target object detection is performed using YOLOv7~\cite{linMicrosoftCOCOCommon2015} for \ac{COCO} categories and GroundingDINO~\cite{liuGroundingDINOMarrying2024} for open-vocabulary detection. Once an object matching the target query is detected, SAM~\cite{kirillovSegmentAnything2023} is applied to generate an accurate mask, and the system transitions from exploration to goal navigation.  

This modular framework achieves state-of-the-art performance on the Gibson, HM3D, and Matterport3D benchmarks, outperforming prior zero-shot approaches such as \ac{ESC}, \ac{SemUtil}, and \ac{CoW} in both \ac{SR} and \ac{SPL} \cite{yokoyamaVLFMVisionLanguageFrontier2023}. Despite its efficiency and interpretability, several limitations remain. \ac{VLFM} relies on a single-source detection pipeline, either GroundingDINO or YOLOv7, making it prone to false positives in open-vocabulary scenarios, which can result in premature stopping behavior. Furthermore, the value map is episodic rather than persistent: it resets after each navigation episode and does not maintain long-term semantic memory, leading to redundant revisits during multi-object search tasks. While the system operates in real time, the use of multiple large-scale pretrained models (BLIP-2, GroundingDINO/YOLOv7, and SAM) demands substantial computational resources, consuming approximately 16~GB of VRAM on an NVIDIA RTX~4090 GPU during deployment, which limits scalability on embedded robotic platforms.

These zero-shot and training-free approaches enable real-time semantic exploration without the need for extensive retraining, making them more adaptable to diverse environments. However, they often lack persistent memory mechanisms to retain knowledge of previously explored areas, which can limit their efficiency in multi-object search tasks.

\section{Map Reconstruction and Persistent Semantic Mapping}
\subsection*{Persistent Semantic Mapping for Exploration}
\begin{itemize}
\item OneMap
\item ConceptGraphs
\item SemExp
\item VLMaps
\item Pigeon
\item CLIP-Fields
\end{itemize}


\subsection*{Semantic Scene Reconstruction}
\begin{itemize}
    \item Overview of approaches to build and update semantic maps during exploration:
    \begin{itemize}
        \item 2D grid maps
        \item Pointclouds
        \item Voxel grids
        \item Octomaps
        \item Scene graphs
        \item Neural Radiance Fields (NeRFs)
        \item Feature Fields
    \end{itemize}
    \item Techniques for fusing sensor data into persistent 2D/3D representations:
    \begin{itemize}
        \item Storing Visual Embeddings (e.g., CLIP features) in 3D maps for semantic querying.
        \item Incremental updating of semantic labels based on new observations.
        \item Handling uncertainty and conflicting detections over time.
    \end{itemize}
    \item Comparison of representations (Octomaps, point clouds, voxel grids) in terms of:
    \begin{itemize}
        \item Memory efficiency.
        \item Ability to store semantic labels persistently.
    \end{itemize}
    \item Discussion of \dots
    \begin{itemize}
        \item OpenFusion
        \item Clio
        \item GeFF
        \item CLIP-Fields
        \item ConceptFusion
        \item LERF
    \end{itemize}
    as examples of global 3D semantic maps.
    \item Limitations in updating or correcting the map after wrong detections.
\end{itemize}



\section{Object Detection and Promptable Models}
\begin{itemize}
    \item Review of traditional and open-vocabulary object detection methods.
    \item Analysis of grounding-capable detectors and segmentation models for zero-shot tasks.
    \item Specific evaluation of the following models for their suitability in semantic multi-object search:
    \begin{itemize}
        \item YOLO-E
        \item GroundingDINO
        \item MobileSAM
        \item GroundedSAM
        \item SEEM
        \item OWL-ViT
        \item MaskDINO
        \item GLIP
    \end{itemize}
    \item Discussion of promptable vision-language models supporting multi-modal queries (text, image, audio).
    \item Challenges with false positives in zero-shot settings and their implications for reliable multi-object detection.
\end{itemize}