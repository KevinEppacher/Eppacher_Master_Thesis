% chktex-file 44

\chapter{State of the Art}
\label{ch:sota}


In this chapter, the current state of research in semantic multi-object search, map reconstruction, and object detection is reviewed. The goal is to identify strengths and limitations of existing methods and establish the technological context for the proposed hybrid approach. The chapter is divided into three key areas: approaches for searching multiple objects semantically, techniques for building and maintaining persistent semantic maps, and recent advances in object detection and promptable models for open-vocabulary tasks.

\section{Geometric Exploration}

A \textit{frontier} is defined as the boundary between known free space and unknown regions of the environment~\cite{lluviaActiveMappingRobot2021}. Frontier-based exploration relies on the principle that unexplored areas adjacent to known free regions provide the highest potential information gain. To identify such frontiers, the robot must maintain a global representation of the environment, typically an occupancy grid or voxel map, where each cell is classified as \textit{free}, \textit{occupied}, or \textit{unknown}, based on sensor observations from \ac{LiDAR} or RGB-D cameras. The robot then iteratively selects and navigates to frontiers to expand its knowledge of the environment.

\citeauthor{quinApproachesEfficientlyDetecting2021}~\cite{quinApproachesEfficientlyDetecting2021} evaluated three commonly used frontier extraction methods that differ primarily in computational efficiency and scalability.  
The first approach, known as the \textit{Naïve Active Area (NaïveAA)} method, evaluates every cell in the occupancy grid to determine whether it is free and has at least one unknown neighbor. Although this approach is conceptually simple and accurate for small-scale maps, it becomes computationally expensive for larger environments and often produces small, fragmented frontier clusters.

The second approach, the \textit{Wavefront Frontier Detector (WFD)}~\cite{quinApproachesEfficientlyDetecting2021,topiwalaFrontierBasedExploration2018}, improves efficiency by using a breadth-first search (BFS) to identify connected frontier regions without exhaustively scanning the entire map. Unlike the NaïveAA method, WFD directly extracts continuous frontier clusters rather than treating each frontier cell individually, significantly reducing redundant computations.

The third method, the \textit{Frontier-Tracing Frontier Detection (FTFD)}~\cite{quinApproachesEfficientlyDetecting2021}, further enhances performance by incrementally updating frontier information using only the most recent sensor observations. Instead of re-evaluating the full map, FTFD initiates a BFS from previously known frontier cells that remain within the active area and from the endpoints of the latest sensor rays. Newly visible free-space cells along the scan boundary are evaluated as potential frontiers, while outdated frontier cells that are now occupied or re-observed are removed. By restricting computation to the local scan perimeter, FTFD achieves significantly faster update rates than NaïveAA and WFD, supporting real-time frontier detection even in large-scale environments.

After frontiers have been extracted, a selection strategy determines which frontier the robot should explore next. Simple heuristics such as \textit{nearest-frontier selection} minimize travel distance but can lead to oscillatory behavior between nearby frontiers. Alternatively, selecting the \textit{largest frontier} favors unexplored regions of higher spatial extent, reducing dead-end visits but increasing traversal cost.  
To address these trade-offs, \citeauthor{bourgaultInformationBasedAdaptive2002}~\cite{bourgaultInformationBasedAdaptive2002} introduced a \textit{utility-based frontier selection} framework that combines multiple criteria, such as distance, frontier size, and expected information gain, into a unified objective function. This approach enables more balanced decision-making, improving overall exploration efficiency and map completeness. Many subsequent works have built upon this foundation, incorporating additional factors such as energy consumption, obstacle density, and dynamic environment considerations into the utility function~\cite{lluviaActiveMappingRobot2021}.

However, all these methods are primarily designed for geometric exploration without incorporating semantic understanding. As a result, they are optimized for complete map coverage rather than goal-directed exploration tasks. In scenarios where a robot must locate specific objects or regions based on semantic cues, purely geometric frontier selection often leads to inefficient search behavior and unnecessary traversal. This motivates the integration of semantic reasoning into the frontier-based exploration process, where frontiers can be prioritized not only by geometric utility but also by their semantic relevance to the task objective.

\section{Vision-Language-Guided Exploration}

In contrast to geometric exploration, which aims to maximize map coverage using the shortest possible path and time, semantic exploration focuses on efficiently locating specific objects or regions of interest described in high-level semantic terms. The objective is to minimize path length and exploration time while prioritizing areas likely to contain relevant targets rather than achieving complete spatial coverage.  

Recent research has introduced \ac{DRL} and supervised learning approaches to train agents capable of performing semantic object search~\cite{majumdarZSONZeroShotObjectGoal2023,ramakrishnanPONIPotentialFunctions2022,ramrakhyaPIRLNavPretrainingImitation2023}.  
These methods differ in architecture and learning paradigm but share a common goal of integrating visual understanding with navigation policies.

% ---------------------- ZSON ----------------------

\citeauthor{majumdarZSONZeroShotObjectGoal2023}~\cite{majumdarZSONZeroShotObjectGoal2023} proposed \ac{ZSON}, a zero-shot object navigation framework that leverages the multimodal embedding space of CLIP~\cite{radfordLearningTransferableVisual2021} to guide exploration policies trained via reinforcement learning.  
During training, the agent’s observation space consists of previous actions and RGB images, while the navigation goal is represented by the CLIP embedding of an image containing the target object. The reward function is defined as:

\begin{equation}
r_t = r_{\text{success}} + r_{\text{angle-success}} - \Delta d_{tg} - \Delta a_{tg} + r_{\text{slack}},
\end{equation}

where $r_{\text{success}}$ provides a large positive reward for successfully locating the target object,  
$r_{\text{angle-success}}$ rewards correct orientation toward the object,  
$\Delta d_{tg}$ penalizes increased distance to the target,  
$\Delta a_{tg}$ penalizes angular deviation,  
and $r_{\text{slack}}$ applies a small negative reward to encourage efficiency.  
At inference time, the image-based embedding is replaced by the CLIP embedding of a text description of the target object, enabling zero-shot generalization to unseen objects.  
The agent’s action space includes four discrete actions: \textit{move forward}, \textit{turn left}, \textit{turn right}, and \textit{stop}.

While \ac{ZSON} demonstrates strong zero-shot generalization and effective object localization within trained domains, it lacks explicit mechanisms for structured exploration in unknown regions. Since the learned policy primarily directs the agent toward locations visually similar to previously encountered objects, it may become trapped in familiar areas and fail to discover new regions. Moreover, the reliance on \ac{DRL} training reduces interpretability and generalizability; retraining is often required when visual conditions, environment layouts, or sensor modalities differ significantly from the training distribution.

% ---------------------- PIRLNav ----------------------

Building on this line of research, \citeauthor{ramrakhyaPIRLNavPretrainingImitation2023}~\cite{ramrakhyaPIRLNavPretrainingImitation2023} introduced \ac{PIRLNav}, a two-stage pretraining framework for vision-language navigation that combines \ac{BC} and \ac{RL} to learn generalizable exploration policies. The approach first leverages large-scale human demonstrations to imitate expert navigation behavior and subsequently fine-tunes the policy using \ac{RL} to optimize long-term performance.

In the first stage, the policy network is pretrained via \ac{BC} on a dataset containing approximately 77,000 human expert demonstrations. The objective is to minimize the negative log-likelihood of actions given the corresponding observations, as defined by:

\begin{equation}
\label{eq:pirlnav_bc}
\theta^{*} = \arg\min_{\theta} \sum_{(o_t, a_t)} -\log \pi_{\theta}(a_t \mid o_t),
\end{equation}

where $\theta$ denotes the parameters of the policy network, and $(o_t, a_t)$ are observation-action pairs derived from expert trajectories.  
The observation space $o_t$ includes RGB images, GPS and compass pose information (HM3D coordinates), a one-hot encoded target object category, and an implicit temporal memory implemented via a gated recurrent unit (GRU).  
The action space is discrete and consists of \textit{move forward}, \textit{turn left}, \textit{turn right}, and \textit{stop}.  
This pretraining phase enables the agent to acquire human-like navigation behavior, producing a policy that maps high-dimensional sensory inputs to navigation commands.

In the second stage, the pretrained policy is fine-tuned using \ac{DRL} to maximize long-term rewards associated with successful object-goal navigation. The actor network is initialized with the weights obtained from the BC stage, while the critic (value function) is first trained independently to stabilize learning before both networks are optimized jointly using \ac{PPO}.  
The overall objective is to maximize the expected cumulative discounted reward over trajectories sampled from the policy:

\begin{equation}
\label{eq:pirlnav_drl}
\pi^{*} = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} 
\left[ \sum_{t=1}^{T} \gamma^{t-1} r_t \right],
\end{equation}

where $\tau$ represents trajectories of observations, actions, and rewards generated under policy $\pi$, $r_t$ is the scalar reward at time step $t$, and $\gamma$ is the discount factor.  
A reward of $r_t = 1$ is assigned when the agent stops within a defined range of the target and the object is within its \ac{FOV}. Notably, \ac{PIRLNav} does not employ an explicit object detector; instead, object recognition is implicitly encoded in the policy through visual representations learned from pretrained \ac{DINO}~\cite{jiangCLIPDINOVisual2024} features during pretraining.

While \ac{PIRLNav} achieves strong performance in simulated benchmarks, it inherits several limitations typical of end-to-end reinforcement learning systems. The policy exhibits limited interpretability and lacks explicit semantic memory, preventing long-term reasoning across multiple goals. Furthermore, retraining is required when the visual domain or sensor configuration changes significantly, and generalization strongly depends on the similarity between training and deployment environments.

% ---------------------- PONI ----------------------

To overcome the black-box nature of end-to-end policies, \citeauthor{ramakrishnanPONIPotentialFunctions2022}~\cite{ramakrishnanPONIPotentialFunctions2022} proposed \ac{PONI}, a map-based object-goal navigation framework that decouples high-level exploration decisions from low-level navigation control. \ac{PONI} learns a potential-field network that operates on a partial allocentric semantic map, where free space, occupied space, unknown regions, and detected object categories are encoded in distinct semantic channels. The network predicts two dense potential fields aligned with the current map: an \textit{area potential} \(U_t^a\), which promotes the exploration of large unexplored regions, and an \textit{object potential} \(U_t^o\), which estimates the likelihood of proximity to the target object.

The potential-field network is trained in a fully supervised, interaction-free manner using annotated semantic maps from simulated environments. During training, ground-truth area and object potentials are computed offline by exploiting complete knowledge of free space and object locations. The model is optimized to regress these potentials using a pixel-wise loss applied to frontier cells.  
At inference time, the network predicts both potentials from the current partial map without access to ground-truth information. Geometric frontiers are extracted from the occupancy map and scored by sampling the predicted potentials according to:

\begin{equation}
U_t(f) = \alpha \, U_t^a(f) + (1 - \alpha) \, U_t^o(f),
\end{equation}

where \(\alpha\) controls the trade-off between exploration and exploitation.  
The frontier with the highest score is selected as the next navigation goal, which is then reached using a deterministic path planner rather than a learned action policy.  

While \ac{PONI} achieves efficient and interpretable exploration behavior, it depends on category-level semantic inputs and assumes a fixed, closed set of object classes encountered during training. Consequently, it cannot generalize to unseen categories or open-vocabulary settings, and its reliance on dense semantic annotations limits applicability in real-world scenarios where such data are unavailable or noisy~\cite{ramakrishnanPONIPotentialFunctions2022}.

% ---------------------- Discussion ----------------------

Overall, these RL-based and supervised learning approaches illustrate the potential of combining semantic understanding with learned navigation policies but reveal persistent shortcomings that motivate the shift toward pretrained vision-language models. Key limitations include:

\begin{itemize}
    \item \textbf{Extensive training requirements:} Large datasets and prolonged training are needed, reducing adaptability to new environments.
    \item \textbf{High computational demands:} Complex architectures hinder real-time operation on embedded systems.
    \item \textbf{Closed-set semantics:} Models typically rely on a fixed set of object categories, limiting open-vocabulary reasoning.
    \item \textbf{Lack of persistent memory:} No long-term spatial or semantic memory is maintained, causing redundant exploration.
    \item \textbf{Limited interpretability:} End-to-end learning obscures decision-making and complicates integration with classical navigation frameworks.
\end{itemize}

Therefore, recent research has shifted toward leveraging large-scale pretrained vision-language models (\acp{VLM}) that provide rich semantic representations and zero-shot generalization capabilities. By integrating these models into modular exploration frameworks, it becomes possible to overcome many of the limitations associated with learned policies while retaining the benefits of semantic reasoning for efficient multi-object search.

A representative example of this paradigm is the \ac{ESC} framework proposed by \citeauthor{zhouESCExplorationSoft2023}~\cite{zhouESCExplorationSoft2023}, which augments traditional frontier-based exploration with semantic cues derived from pretrained \acp{VLM}. Specifically, \ac{ESC} combines a grounded object detector, \ac{GLIP}~\cite{liGroundedLanguageImagePretraining2022}, with a \ac{LLM} (either ChatGPT or DeBERTa) to generate semantic priors that guide exploration decisions. Frontiers are scored based on both geometric utility and semantic relevance to the target object, as formulated in Equation~\ref{eq:esc}:

\begin{equation}
\label{eq:esc}
P(F) = P(F \mid d_i^{t}, o^{t}, r^{t}),
\end{equation}

where \(P(F)\) denotes the probability of selecting frontier \(F\), conditioned on detected objects \(d_i^{t}\), current observations \(o^{t}\), and the robot pose \(r^{t}\) at time \(t\). This formulation is implemented using Probabilistic Soft Logic (PSL), which fuses visual detections with language-derived priors about object co-occurrences and spatial relationships. The robot then selects the frontier with the highest combined score, balancing geometric and semantic information to improve search efficiency.

In practice, \ac{ESC} employs \ac{GLIP} as the detection backbone to compute 2D bounding boxes, class labels, and confidence scores for objects within the robot’s \ac{FOV}. While effective in simulation, the approach introduces notable computational overhead due to repeated \ac{LLM} inference and PSL optimization. Moreover, its performance is highly dependent on the reliability of object detections-false positives can misguide frontier scoring, and incorrect commonsense priors may further bias exploration. 

Through ablation studies, \citeauthor{zhouESCExplorationSoft2023} observed that object-object and object-room relational priors can occasionally degrade performance, as commonsense relationships are inherently probabilistic rather than deterministic. Additionally, while \ac{ESC} maintains a local semantic map during navigation, it lacks mechanisms for long-term memory or belief revision. Consequently, once an incorrect detection or prior is introduced, the framework has no learned means of down-weighting or correcting it over time, which can lead to persistent semantic inconsistencies during extended exploration.

In contrast to such reasoning-heavy systems, \citeauthor{gadreCoWsPastureBaselines2022}~\cite{gadreCoWsPastureBaselines2022} proposed \ac{CoW}, a lightweight vision-language exploration framework that relies purely on image-text alignment from CLIP~\cite{radfordLearningTransferableVisual2021} without requiring explicit frontier detection, semantic mapping, or object segmentation. The method guides the robot toward directions with the highest cosine similarity between the current visual observation and the target object description, defined as:

\begin{equation}
\label{eq:cosine_similarity}
\mathrm{\cos\_sim}(\mathbf{u}, \mathbf{v})
=
\frac{\mathbf{u} \cdot \mathbf{v}}
{\lVert \mathbf{u} \rVert \, \lVert \mathbf{v} \rVert},
\end{equation}

where \(\mathbf{u}\) and \(\mathbf{v}\) denote the CLIP embeddings of the current image and the target text prompt, respectively. The robot moves forward when the similarity exceeds a predefined threshold and rotates otherwise to maximize alignment with the target.  

By eliminating explicit detectors and handcrafted mapping, \ac{CoW} offers a computationally efficient and conceptually simple baseline for open-vocabulary navigation. However, this simplicity comes at the cost of robustness. The system is highly sensitive to viewpoint variations and clutter, as cosine similarity does not always correlate with true object presence. Without spatial memory or geometric reasoning, the robot may oscillate near false positives or become trapped in local minima. Moreover, because similarity scores vary across object categories, no universal threshold can be established for all targets, resulting in inconsistent stopping behavior and reduced reliability during multi-object search.

\section{Map Reconstruction and Persistent Semantic Mapping}

\subsection*{Semantic Scene Reconstruction}
\begin{itemize}
    \item Overview of approaches to build and update semantic maps during exploration:
    \begin{itemize}
        \item 2D grid maps
        \item Pointclouds
        \item Voxel grids
        \item Octomaps
        \item Scene graphs
        \item Neural Radiance Fields (NeRFs)
        \item Feature Fields
    \end{itemize}
    \item Techniques for fusing sensor data into persistent 2D/3D representations:
    \begin{itemize}
        \item Storing Visual Embeddings (e.g., CLIP features) in 3D maps for semantic querying.
        \item Incremental updating of semantic labels based on new observations.
        \item Handling uncertainty and conflicting detections over time.
    \end{itemize}
    \item Comparison of representations (Octomaps, point clouds, voxel grids) in terms of:
    \begin{itemize}
        \item Memory efficiency.
        \item Ability to store semantic labels persistently.
    \end{itemize}
    \item Discussion of \dots
    \begin{itemize}
        \item ConceptGraphs
        \item ConceptGraph-Online
        \item OpenFusion
        \item Clio
        \item OpenScene
        \item GeFF
        \item CLIP-Fields
        \item ConceptFusion
        \item VLMaps
        \item LERF
    \end{itemize}
    as examples of global 3D semantic maps.
    \item Limitations in updating or correcting the map after wrong detections.
\end{itemize}

\subsection*{Persistent Semantic Mapping for Exploration}



\section{Object Detection and Promptable Models}
\begin{itemize}
    \item Review of traditional and open-vocabulary object detection methods.
    \item Analysis of grounding-capable detectors and segmentation models for zero-shot tasks.
    \item Specific evaluation of the following models for their suitability in semantic multi-object search:
    \begin{itemize}
        \item YOLO-E
        \item GroundingDINO
        \item MobileSAM
        \item GroundedSAM
        \item SEEM
        \item OWL-ViT
        \item MaskDINO
        \item GLIP
    \end{itemize}
    \item Discussion of promptable vision-language models supporting multi-modal queries (text, image, audio).
    \item Challenges with false positives in zero-shot settings and their implications for reliable multi-object detection.
\end{itemize}