% chktex-file 44

\chapter{Implementation}
\label{ch:implementation}
This section details the practical implementation of the proposed approach, including the simulation and real-world setup, datasets, software stack, and hardware configuration and the evaluation metrics used in the experiments.

\section{Simulation Environment}

HabitatSim~\cite{savvaHabitatPlatformEmbodied2019} has been widely adopted in embodied AI research due to its efficient rendering pipeline and support for large-scale datasets such as Matterport3D and HM3D~\cite{changMatterport3DLearningRGBD2017}. However, for this work, \ac{IsaacSim} was selected as the primary simulation platform because it provides high-fidelity photorealistic rendering, accurate physics simulation, and realistic sensor models. These properties are essential for reducing the sim-to-real gap and for ensuring that the proposed semantic exploration and fusion strategies transfer reliably to real robotic systems~\cite{zhouBuildingAICPSNVIDIA2024}.

All experiments are conducted in \ac{IsaacSim} version~5.0.0 by NVIDIA, leveraging its GPU-accelerated rendering, physically based simulation, and tight integration with ROS~2~\cite{salimpourSimtoRealTransferMobile2025}. The pre-installed \texttt{Carter} mobile robot is used as the navigation platform and is equipped with an RGB-D camera, a 2D LiDAR sensor, and wheel odometry. The robot is controlled through ROS~2 nodes communicating with IsaacSim via Fast DDS, enabling direct reuse of the navigation and perception stack employed in real-world deployment.

IsaacSim further supports importing complex indoor environments derived from Matterport3D~\cite{changMatterport3DLearningRGBD2017}, which are used as evaluation scenes in this work. To accommodate the physical footprint of the mobile robot and to ensure stable navigation behavior, all Matterport3D scenes are uniformly scaled by 50\% in each spatial dimension.

Alternative simulators such as HabitatSim~\cite{savvaHabitatPlatformEmbodied2019}, MuJoCo~\cite{zakkaMuJoCoPlayground2025}, and Ignition Gazebo~\cite{koenigDesignUseParadigms2004} were considered. However, these platforms either lack full physics-based mobile robot simulation, photorealistic sensor rendering, or seamless ROS~2 integration, which are required by the proposed system.

\section{Dataset}
\label{sec:dataset}

Prior work such as VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, CoW~\cite{gadreCoWsPastureBaselines2022}, and OneMap~\cite{buschOneMapFind2025} has demonstrated the effectiveness of Matterport3D-derived indoor environments for semantic exploration and object-goal navigation tasks. This experimental protocol closely follows the Habitat Navigation Challenge~2023~\cite{puigHabitat30CoHabitat2023}, which evaluates semantic navigation methods on the HM3D dataset~\cite{ramakrishnanHabitatMatterport3DDataset2021}.

Accordingly, this work employs Matterport3D-based indoor scenes~\cite{changMatterport3DLearningRGBD2017} as the primary source of evaluation environments. The dataset provides a diverse set of real-world indoor spaces with high-quality 3D reconstructions and semantic annotations, making it well suited for evaluating open-vocabulary semantic exploration methods.

Due to the custom simulation setup in Isaac Sim, all baseline methods are re-implemented to ensure a fair comparison under consistent conditions, following the evaluation protocol of the Habitat Navigation Challenge~2023 (see Section~\ref{sec:implementation:evaluation})~\cite{puigHabitat30CoHabitat2023}. All evaluations are conducted on the HM3Dv2 validation split, which contains scenes with diverse layouts, object distributions, and levels of complexity.

The following scenes from the Matterport3D dataset are used in this work:
\begin{itemize}
    \item \texttt{00800-TEEsavR23oF}
    \item \texttt{00813-svBbv1Pavdk}
    \item \texttt{00814-p53SfW6mjZe}
    \item \texttt{00824-Dd4bFSTQ8gi}
    \item \texttt{00848-ziup5kvtCCR}
    \item \texttt{00876-mv2HUxq3B53}
\end{itemize}

These scenes are also used by~\citeauthor{yokoyamaVLFMVisionLanguageFrontier2023}
and~\citeauthor{buschOneMapFind2025}, enabling direct and reproducible comparison of
experimental results.

\section{Used Software}

The proposed semantic exploration and fusion framework is implemented using \ac{ROS2} (Humble Hawksbill) as the middleware for inter-node communication and system integration. \ac{ROS2} provides a modular, distributed architecture that allows perception, mapping, fusion, and exploration components to operate as independent nodes while remaining tightly synchronized~\cite{macenskiImpactROS22023}.

For navigation, localization, and obstacle avoidance, the \ac{Nav2} stack is employed~\cite{macenskiDesksROSMaintainers2023}. In combination with \ac{SLAM}-Toolbox~\cite{macenskiSLAMToolboxSLAM2021}, this enables robust 2D mapping, localization, and path planning within the simulated indoor environments. Inter-process and inter-container communication is handled via FastDDS, enabling reliable distributed communication between the detection, mapping, and exploration workspaces.

Open-vocabulary object detection is realized using YOLO-E~\cite{wangYOLOERealTimeSeeing2025}, implemented via the Ultralytics framework~\cite{sapkotaUltralyticsYOLOEvolution2025}. YOLO-E is integrated as a \ac{ROS2} lifecycle node, allowing controlled initialization, activation, and shutdown during exploration episodes, and supporting real-time inference for promptable object detection.

Semantic similarity scoring between visual observations and user-defined text queries is performed using BLIP-2~\cite{liBLIP2BootstrappingLanguageImage2023}, accessed through the LAVIS library~\cite{liLAVISLibraryLanguageVision2022}. This \ac{VLM} provides cosine similarity scores that are used to populate the semantic value map and guide exploration toward semantically relevant regions.

Persistent 3D semantic mapping and memory generation are handled by OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}, which is wrapped as a dedicated \ac{ROS2} node. OpenFusion incrementally fuses RGB-D observations into a global semantic point cloud representation, serving as the long-term memory component of the proposed system.

% \begin{itemize}
%     \item ROS2-based implementation (Humble Hawksbill) as middleware.
%     \item Navigation stack: Navigation2 (Nav2) for frontier-based exploration and path planning.
%     \item DDS communication layer for distributed communication between detection, mapping, and control nodes.
%     \item Integration of promptable models (OpenFusion, BLIP-2, YOLO-E) for real-time zero-shot detection during exploration and exploitation.
% \end{itemize}

\section{Used Hardware}

All experiments are conducted on a dedicated workstation designed to support computationally demanding simulation, perception, and vision-language inference workloads. The hardware configuration is summarized as follows:
\begin{itemize}
    \item \textbf{Workstation PC}
    \begin{itemize}
        \item \textbf{CPU:} AMD Ryzen~9~5950X (16 cores, 32 threads)
        \item \textbf{Motherboard:} Gigabyte B550 Gaming X V2
        \item \textbf{GPU:} NVIDIA GeForce RTX~4090 (ASUS TUF Gaming OC, 24~GB VRAM)
        \item \textbf{Memory:} 64~GB Corsair Vengeance LPX DDR4
    \end{itemize}
\end{itemize}

The high core count of the CPU enables parallel execution of multiple \ac{ROS2}
nodes, while the RTX~4090 GPU provides sufficient compute and memory capacity for
photorealistic simulation in IsaacSim~\cite{zhouBuildingAICPSNVIDIA2024}, real-time open-vocabulary object detection~\cite{wangYOLOERealTimeSeeing2025}, and \ac{VLM}~\cite{liBLIP2BootstrappingLanguageImage2023,yamazakiOpenFusionRealtimeOpenVocabulary2023,zouSegmentEverythingEverywhere2023} inference. This setup ensures that all components of the proposed system can operate in real time without artificial bottlenecks introduced by hardware limitations.

\section{Evaluation Pipeline Overview}
\label{sec:implementation:evaluation}

To ensure comparability with prior work such as VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023} and OneMap~\cite{buschOneMapFind2025}, a dedicated evaluation pipeline is implemented that reproduces
the standard object-goal navigation metrics used in embodied \ac{AI} benchmarks~\cite{puigHabitat30CoHabitat2023}. The pipeline operates on recorded navigation trajectories and semantic maps generated during each exploration episode and is illustrated in Figure~\ref{fig:evaluation_pipeline}.

The primary evaluation metrics considered in this work are \ac{SR},\ac{SPL}, and \ac{MSR}~\cite{yokoyamaVLFMVisionLanguageFrontier2023,buschOneMapFind2025,PIGEONVLMDrivenObject}. Computing \ac{SPL} requires knowledge of the geodesic shortest path from the robot’s initial pose to the nearest instance of the target object. Since all experiments are conducted in a custom IsaacSim-based environment with uniformly scaled Matterport3D scenes, the built-in shortest-path planner provided by HabitatSim cannot be directly used.

Instead, the evaluation pipeline is composed of two main stages:  
(a) semantic environment reconstruction, and  
(b) geodesic shortest-path computation based on the reconstructed map.

As described in Chapter~\ref{ch:methods} Section~\ref{sec:methods:semantic_map_construction}, OpenFusion is employed to reconstruct a 3D semantic point cloud from the recorded RGB-D observations~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}. The Matterport3D category set is used as the closed set of semantic classes during evaluation~\cite{changMatterport3DLearningRGBD2017}. For each episode, the stored semantic point cloud is filtered according to the target object category specified by the evaluation protocol. All clusters corresponding to the target class are extracted, and their 3D centroids are computed as candidate goal locations.

The geodesic shortest path is then computed from the robot’s initial pose to the nearest target
centroid using the global planner of the \ac{ROS2} Navigation2 stack~\cite{macenskiDesksROSMaintainers2023}. Path planning is performed on the 2D occupancy grid generated by SLAM Toolbox~\cite{macenskiSLAMToolboxSLAM2021}, ensuring consistency with the localization and navigation setup used during exploration. The resulting shortest path is compared to the executed trajectory to compute \ac{SR}, \ac{SPL}, and \ac{MSR} for each episode.

In contrast to the evaluation procedure of the Habitat Navigation Challenge~2023~\cite{puigHabitat30CoHabitat2023}, which relies on HabitatSim’s internal shortest-path computation, the proposed evaluation pipeline is fully compatible with IsaacSim and the employed \ac{ROS2}-based navigation stack. This enables a fair comparison against prior methods while preserving identical metric definitions and avoiding bias introduced by simulator-specific planners.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/04_implementation/evaluation_pipeline.png}
    \caption{Evaluation pipeline used to compute \ac{SR}, \ac{SPL}, and \ac{MSR}. Recorded RGB-D observations are converted into a semantic point cloud using OpenFusion, target object clusters are extracted, and the geodesic shortest path is computed via the \ac{ROS2} Navigation2 global planner for metric evaluation~\cite{macenskiDesksROSMaintainers2023,puigHabitat30CoHabitat2023}.}
    \label{fig:evaluation_pipeline}
\end{figure}

\section{Evaluation Metrics}

This section defines the evaluation metrics used to assess the performance of the proposed semantic exploration framework and to ensure comparability with prior work in object-goal navigation. The selected metrics jointly evaluate task success, navigation efficiency, multi-object search capability, and detection robustness, thereby capturing both the quality of semantic reasoning and the reliability of perception-driven decision making. Each metric is explicitly assigned to a corresponding experiment, such that the experimental results directly address and answer the research questions defined in Chapter~\ref{ch:introduction}.

\subsection*{Experiment 1: Overall Performance Benchmarking}

The first experiment establishes a performance baseline by comparing the proposed hybrid semantic exploration framework against state-of-the-art methods on standard object-goal navigation metrics. This experiment addresses \textbf{RQ1} by quantifying the overall effectiveness of the integrated exploration–memory approach in realistic indoor environments. Results are compared against representative baselines including \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, \ac{VLMaps}~\cite{huangVisualLanguageMaps2023}, \ac{OneMap}~\cite{buschOneMapFind2025}, and Pigeon~\cite{PIGEONVLMDrivenObject}. Overall performance is evaluated using \ac{SR} and \ac{SPL}.

In this experiment, an episode is defined as a single object-goal navigation task from the HM3Dv2 validation split, in which the robot is tasked with locating a specified target object within a Matterport3D-derived indoor scene~\cite{puigHabitat30CoHabitat2023}.

The \ac{SR} metric measures the proportion of successful episodes in which the robot reaches the queried target object and is defined in Equation~\ref{eq:evaluation:sr},

\begin{equation}
    \mathrm{SR}
    =
    \frac{1}{N}
    \sum_{i=1}^{N}
    S_i
    \label{eq:evaluation:sr}
\end{equation}

where \( S_i = 1 \) if the target object is successfully reached in episode \( i \), and \( 0 \) otherwise; \( N \) denotes the total number of evaluated episodes.

Navigation efficiency is quantified using \ac{SPL}, which compares the geodesic shortest path to the actual path executed by the robot. The metric penalizes unnecessary detours and is defined only for successful episodes in Equation~\ref{eq:evaluation:spl},

\begin{equation}
    \mathrm{SPL}
    =
    \frac{1}{N}
    \sum_{i=1}^{N}
    S_i \cdot \frac{l_i}{\max(p_i, l_i)}
    \label{eq:evaluation:spl}
\end{equation}

where \( l_i \) is the geodesic shortest path length to the target, \( p_i \) is the executed path length, and \( S_i \) indicates task success for episode \( i \).

An episode is considered successful if the target object is visible in the RGB image captured by the robot’s onboard camera. The geodesic shortest path \( l_i \) is computed from the robot’s initial pose to the nearest instance of the target object. In contrast to HabitatSim-based evaluations, success is not restricted to predefined viewpoint tolerances, but instead includes any robot pose from which the target object is visually observable. All reported results were manually verified by inspecting the recorded RGB-D sequences for each episode. For every episode, the target object class is selected from the HM3Dv2 annotated category set~\cite{ramakrishnanHabitatMatterport3DDataset2021}.

\subsection*{Experiment 2: Exploration-Memory Fusion Weighting}

This experiment investigates the influence of the exploration-memory fusion weighting on navigation performance and behavioral stability, addressing \textbf{RQ2}. By systematically varying the trade-off between live semantic exploration and persistent 3D semantic memory, the experiment evaluates how different fusion configurations affect task success, navigation efficiency, and long-horizon search behavior. The primary objective is to identify which weighting configuration yields the best balance between reactivity to newly observed information and stability derived from accumulated semantic memory.

The fusion weighting parameter
\[
    \lambda_{\text{exploration}} \in [0,1]
\]
controls the relative influence of frontier-based exploration and memory-driven exploitation during graph node fusion. A value of $\lambda_{\text{exploration}} = 0$ corresponds to purely memory-driven behavior, while $\lambda_{\text{exploration}} = 1$ corresponds to purely exploration-driven behavior. The parameter is varied in increments of $0.2$. For each configuration, multiple navigation episodes are executed in the HM3Dv2 validation scenes, and performance is evaluated using \ac{SR} and \ac{SPL}.

In contrast to Experiment~1, this experiment employs multi-object search episodes in order to explicitly evaluate the benefit of persistent semantic memory across sequential object queries. Within a single episode, the robot is required to locate multiple target objects in sequence without resetting the environment or clearing the semantic map. This setting reflects realistic long-horizon search scenarios in which previously acquired semantic information can be reused to improve subsequent navigation decisions.

For evaluation, \ac{SR} and \ac{SPL} are computed per target object and aggregated across all episodes. Success for an individual target is defined identically to Experiment~1, and \ac{SPL} is computed using the geodesic shortest path from the robot’s pose at the start of each sub-task to the nearest instance of the corresponding target object.

For each episode, five target objects are selected from the HM3Dv2 annotated category set~\cite{ramakrishnanHabitatMatterport3DDataset2021} and queried in a fixed random order. A global random seed of 42 is used to ensure reproducibility across different fusion weight configurations. Similar to Experiment~1, six Matterport3D scenes from the HM3Dv2 validation split are used for evaluation, with two distinct starting positions per scene.

\subsection*{Experiment 3: Sensitivity to Semantic Map Granularity}

\begin{itemize}

    \item \textbf{Objective:}  
    Investigate how the granularity of semantic retrieval in the 3D semantic mapper affects global map quality, navigation performance, and robustness of the hybrid exploration system.  
    Specifically, this experiment evaluates whether dynamic rebalancing between exploration and memory can compensate for increased semantic noise introduced by higher retrieval depths.

    \item \textbf{Semantic Granularity Parameter:}  
    Semantic map quality is controlled through the retrieval depth \emph{top-k}, which specifies how many semantic candidates (from the VLM embedding space) are fused into each voxel:
    \begin{itemize}
        \item Low \emph{top-k}: sharper but potentially incomplete semantics.  
        \item High \emph{top-k}: denser semantics but increased noise and ambiguity.
    \end{itemize}

    \item \textbf{Dynamic Fusion Weighting:}  
    To counteract noise introduced by larger \emph{top-k} values, the exploration weight  
    \[
        \lambda_{\text{exploration}}
    \]
    is progressively increased, shifting trust toward frontier-driven exploration and away from noisy memory components.

    \item \textbf{Research Questions:}
    \begin{itemize}
        \item \textbf{RQ4a:} How do Success Rate (SR) and Success weighted by Path Length (SPL) degrade as \emph{top-k} increases while relying primarily on memory?
        \item \textbf{RQ4b:} Can adaptive rebalancing toward exploration (i.e., increasing \( \lambda_{\text{exploration}} \)) restore stable performance at higher \emph{top-k} values?
    \end{itemize}

    \item \textbf{Evaluation Metrics:}
    \begin{itemize}
        \item SR: ability to consistently reach goal objects under different semantic retrieval granularities.
        \item SPL: navigation efficiency and the influence of semantic noise on path quality.
        \item Additional qualitative assessment of map sharpness, cluster correctness, and temporal stability of semantic memory.
    \end{itemize}

    \item \textbf{Evaluation Procedure:}
    \begin{itemize}
        \item Generate semantic maps at multiple \emph{top-k} levels (e.g., 1, 3, 5, 10).
        \item For each \emph{top-k}:
        \begin{itemize}
            \item Evaluate SR and SPL under memory-dominant settings.
            \item Incrementally increase \( \lambda_{\text{exploration}} \) and re-evaluate.
        \end{itemize}
        \item Compare results to determine:
        \begin{itemize}
            \item tolerance of the system to semantic noise,
            \item optimal balance between exploration and memory at different granularity levels,
            \item interaction effects between map resolution and fusion stability.
        \end{itemize}
    \end{itemize}

    \item \textbf{Purpose:}  
    This experiment analyzes the coupling between semantic map granularity and the stability of exploration–memory fusion.  
    Results reveal how coarse or noisy semantic retrieval affects overall navigation and whether adaptive weighting can maintain robust performance in open-vocabulary mapping environments.

\end{itemize}


\subsection*{Experiment 4: Robustness to False Positives Through Multi-Source Detection Fusion}

\begin{itemize}

    \item \textbf{Objective:}  
    Evaluate how combining multiple semantic evidence sources, instance detection (YOLO-E), semantic similarity scoring (BLIP-2), and memory confidence from the 3D semantic map, improves detection robustness and suppresses false positives during exploration.

    \item \textbf{Fusion Model (Weighted Noisy-OR):}  
    The proposed fusion strategy follows a weighted Noisy-OR formulation, in which independent semantic evidence sources jointly increase the probability of a valid detection:
    \[
        S_{\text{fusion}}
        = 1 - (1 - w_d S_{\text{det}})
            (1 - w_c S_{\text{map}})
            (1 - w_m S_{\text{mem}}).
    \]
    Here,
    \begin{itemize}
        \item \( S_{\text{det}} \): YOLO-E detector confidence,
        \item \( S_{\text{map}} \): similarity score from the value map (BLIP-2),
        \item \( S_{\text{mem}} \): confidence from persistent 3D semantic memory,
        \item \( w_d, w_c, w_m \): weights defining the contribution of each source.
    \end{itemize}
    This formulation ensures that high confidence from any source can compensate for uncertainty in others while suppressing spurious detections that lack multi-source agreement.

    \item \textbf{Research Questions:}
    \begin{itemize}
        \item \textbf{RQ3a:} How does overall task performance (SR) change under different weight configurations \( (w_d, w_c, w_m) \)?
        \item \textbf{RQ3b:} How do precision, recall, F1-score, and false-positive rate vary across:
        \begin{itemize}
            \item COCO-style closed-set categories,
            \item open-vocabulary object classes,
            \item zero-shot categories not seen during detector training?
        \end{itemize}
        \item \textbf{RQ3c:} What drawbacks arise when detection thresholds are increased or when a single evidence source is overemphasised  
        (e.g., memory bias, detector hallucination, missed low-confidence but valid detections)?
    \end{itemize}

    \item \textbf{Evaluation Metrics:}
    Robustness is quantified using classification metrics derived from the confusion matrix:
    \[
        \text{Precision} = \frac{TP}{TP + FP}, \qquad
        \text{Recall} = \frac{TP}{TP + FN},
    \]
    \[
        \text{F1} = 2 \cdot 
        \frac{\text{Precision} \cdot \text{Recall}}
             {\text{Precision} + \text{Recall}},
    \]
    \[
        \text{FPR} = \frac{FP}{FP + TN}.
    \]
    Additionally, downstream Success Rate (SR) is recorded for each weight triplet \( (w_d, w_c, w_m) \) to evaluate the effect of false positives on the overall navigation pipeline.

    \item \textbf{Evaluation Procedure:}
    \begin{itemize}
        \item Evaluate a range of weight combinations \( (w_d, w_c, w_m) \) spanning detector-dominant, map-dominant, memory-dominant, and balanced regimes, whereas the detection source is mandatory (i.e., \( w_d > 0 \)).
        \item Compare against single-source baselines:
        \begin{itemize}
            \item detector-only (YOLO-E),
            \item similarity-only (BLIP-2),
            \item memory-only retrieval,
            \item the full Noisy-OR fusion strategy.
        \end{itemize}
        \item Analyse outcomes under:
        \begin{itemize}
            \item closed-set (COCO) categories,
            \item open-vocabulary targets,
            \item zero-shot targets.
        \end{itemize}
        \item Quantify how false positives propagate into:
        \begin{itemize}
            \item erroneous graph node generation,
            \item unnecessary navigation actions,
            \item degraded SR and SPL.
        \end{itemize}
    \end{itemize}

    \item \textbf{Purpose:}  
    This experiment evaluates whether multi-source, Noisy-OR-based semantic fusion provides a measurable improvement in detection robustness and false-positive suppression compared to single-source methods, thereby enabling more reliable semantic exploration in open-vocabulary indoor environments.

\end{itemize}



\subsection*{Experiment 5: Real-World System Performance:}  
\begin{itemize}
    \item SR, MSR and SP for search performance under real-world conditions.
    \item System metrics: CPU/GPU usage, FPS, inference latency.
\end{itemize}
\textit{Objective: Assess robustness, efficiency, and deployability in physical environments.}

\newpage