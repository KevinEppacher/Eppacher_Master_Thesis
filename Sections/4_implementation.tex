% chktex-file 44

\chapter{Implementation Detail}
\label{ch:implementation}

In this chapter, the implementation of the used methods is evaluated using simulations. First, the chosen simulation environment is described in Section~\ref{sec:implementation:simulation_environment}. Next, the dataset used for training and evaluation is presented in Section~\ref{sec:dataset}. Section~\ref{sec:implementation:used_software_and_hardware} outlines the software frameworks and hardware components employed in this work. Finally, the evaluation pipeline and metrics used to assess \ac{SAGE} are detailed in Sections~\ref{sec:implementation:evaluation} and~\ref{sec:implementation:metrics}, respectively.

\section{Simulation Environment}
\label{sec:implementation:simulation_environment}

HabitatSim~\cite{savvaHabitatPlatformEmbodied2019} has been widely adopted in embodied AI research due to its efficient rendering pipeline and support for large-scale datasets such as Matterport3D and HM3D~\cite{changMatterport3DLearningRGBD2017}. However, for this work, \ac{IsaacSim} was selected as the primary simulation platform because it provides high-fidelity photorealistic rendering, accurate physics simulation, and realistic sensor models. These properties are essential for reducing the sim-to-real gap and for ensuring that the proposed semantic exploration and fusion strategies transfer reliably to real robotic systems~\cite{zhouBuildingAICPSNVIDIA2024}.

All experiments are conducted in \ac{IsaacSim} version~5.0.0 by NVIDIA, leveraging its GPU-accelerated rendering, physically based simulation, and tight integration with ROS~2~\cite{salimpourSimtoRealTransferMobile2025}. The pre-installed \texttt{Carter} mobile robot is used as the navigation platform and is equipped with an RGB-D camera, a 2D LiDAR sensor, and wheel odometry. The robot is controlled through ROS~2 nodes communicating with IsaacSim via Fast DDS~\cite{leeOptimizingROS22025}, enabling direct reuse of the navigation and perception stack employed in real-world deployment.

IsaacSim further supports importing complex indoor environments derived from Matterport3D~\cite{changMatterport3DLearningRGBD2017}, which are used as evaluation scenes in this work. To accommodate the physical footprint of the mobile robot and to ensure stable navigation behavior, all Matterport3D scenes are uniformly scaled by 50\% in each spatial dimension to accommodate the physical footprint
of the mobile robot and ensure stable navigation behavior.

Alternative simulators such as HabitatSim~\cite{savvaHabitatPlatformEmbodied2019}, MuJoCo~\cite{zakkaMuJoCoPlayground2025}, and Ignition Gazebo~\cite{koenigDesignUseParadigms2004} were considered. However, these platforms either lack full physics-based mobile robot simulation, photorealistic sensor rendering, or seamless ROS~2 integration, which are required by the proposed system.

\section{Dataset}
\label{sec:dataset}

Prior work such as VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, CoW~\cite{gadreCoWsPastureBaselines2022}, and OneMap~\cite{buschOneMapFind2025} has demonstrated the effectiveness of Matterport3D-derived indoor environments for semantic exploration and object-goal navigation tasks. This experimental protocol closely follows the Habitat Navigation Challenge~2023~\cite{puigHabitat30CoHabitat2023}, which evaluates semantic navigation methods on the HM3D dataset~\cite{ramakrishnanHabitatMatterport3DDataset2021}.

Accordingly, this work employs Matterport3D-based indoor scenes~\cite{changMatterport3DLearningRGBD2017} as the primary source of evaluation environments. The dataset provides a diverse set of real-world indoor spaces with high-quality 3D reconstructions and semantic annotations, making it well suited for evaluating open-vocabulary semantic exploration methods.

Due to the custom simulation setup in Isaac Sim, all baseline methods are re-implemented to ensure a fair comparison under consistent conditions, following the evaluation protocol of the Habitat Navigation Challenge~2023 (see Section~\ref{sec:implementation:evaluation})~\cite{puigHabitat30CoHabitat2023}. All evaluations are conducted on the HM3Dv2 validation split, which contains scenes with diverse layouts, object distributions, and levels of complexity.

The following scenes from the Matterport3D dataset are used in this work:
\begin{itemize}
    \item \texttt{00800-TEEsavR23oF}
    \item \texttt{00813-svBbv1Pavdk}
    \item \texttt{00814-p53SfW6mjZe}
    \item \texttt{00824-Dd4bFSTQ8gi}
    \item \texttt{00848-ziup5kvtCCR}
    \item \texttt{00876-mv2HUxq3B53}
\end{itemize}

These scenes are also used by~\citeauthor{yokoyamaVLFMVisionLanguageFrontier2023}
and~\citeauthor{buschOneMapFind2025}, enabling direct and reproducible comparison of
experimental results.

\section{Used Software and Hardware}
\label{sec:implementation:used_software_and_hardware}

The proposed semantic exploration and fusion framework is implemented using \ac{ROS2} (Humble Hawksbill) as the middleware for inter-node communication and system integration. \ac{ROS2} provides a modular, distributed architecture that allows perception, mapping, fusion, and exploration components to operate as independent nodes while remaining tightly synchronized~\cite{macenskiImpactROS22023}.

For navigation, localization, and obstacle avoidance, the \ac{Nav2} stack is employed~\cite{macenskiDesksROSMaintainers2023}. In combination with \ac{SLAM}-Toolbox~\cite{macenskiSLAMToolboxSLAM2021}, this enables robust 2D mapping, localization, and path planning within the simulated indoor environments. Inter-process and inter-container communication is handled via FastDDS, enabling reliable distributed communication between the detection, mapping, and exploration workspaces.

Open-vocabulary object detection is realized using YOLO-E~\cite{wangYOLOERealTimeSeeing2025}, implemented via the Ultralytics framework~\cite{sapkotaUltralyticsYOLOEvolution2025}. YOLO-E is integrated as a \ac{ROS2} lifecycle node, allowing controlled initialization, activation, and shutdown during exploration episodes, and supporting real-time inference for promptable object detection.

Semantic similarity scoring between visual observations and user-defined text queries is performed using BLIP-2~\cite{liBLIP2BootstrappingLanguageImage2023}, accessed through the LAVIS library~\cite{liLAVISLibraryLanguageVision2022}. This \ac{VLM} provides cosine similarity scores that are used to populate the semantic value map and guide exploration toward semantically relevant regions.

Persistent 3D semantic mapping and memory generation are handled by OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}, which is wrapped as a dedicated \ac{ROS2} node. OpenFusion incrementally fuses RGB-D observations into a global semantic point cloud representation, serving as the long-term memory component of the proposed system.

All experiments are conducted on the following configuration:
\begin{itemize}
    \item \textbf{PC Workstation:}
    \begin{itemize}
        \item \textbf{CPU:} AMD Ryzen~9~5950X (16 cores, 32 threads)
        \item \textbf{Motherboard:} Gigabyte B550 Gaming X V2
        \item \textbf{GPU:} NVIDIA GeForce RTX~4090 (ASUS TUF Gaming OC, 24~GB \ac{VRAM})
        \item \textbf{RAM Memory:} 64~GB Corsair Vengeance LPX DDR4
    \end{itemize}
\end{itemize}

\section{Evaluation Pipeline Overview}
\label{sec:implementation:evaluation}

To ensure comparability with prior work such as VLFM~\cite{yokoyamaVLFMVisionLanguageFrontier2023} and OneMap~\cite{buschOneMapFind2025}, a dedicated evaluation pipeline is implemented that reproduces
the standard object-goal navigation metrics used in embodied \ac{AI} benchmarks~\cite{puigHabitat30CoHabitat2023}. The pipeline operates on recorded navigation trajectories and semantic maps generated during each exploration episode and is illustrated in Figure~\ref{fig:evaluation_pipeline}.

The primary evaluation metrics considered in this work are \ac{SR} and \ac{SPL}~\cite{yokoyamaVLFMVisionLanguageFrontier2023,buschOneMapFind2025,pengPIGEONVLMDrivenObject2025}. Computing \ac{SPL} requires knowledge of the geodesic shortest path from the robot’s initial pose to the nearest instance of the target object. Since all experiments are conducted in a custom IsaacSim-based environment with uniformly scaled Matterport3D scenes, the built-in shortest-path planner provided by HabitatSim cannot be directly used.

Instead, the evaluation pipeline is composed of two main stages:  
(a) semantic environment reconstruction, and  
(b) geodesic shortest-path computation based on the reconstructed map.

As described in Chapter~\ref{ch:methods} Section~\ref{sec:methods:semantic_map_construction}, OpenFusion is employed to reconstruct a 3D semantic point cloud from the recorded RGB-D observations~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}. The Matterport3D category set is used as the closed set of semantic classes during evaluation~\cite{changMatterport3DLearningRGBD2017}. For each episode, the stored semantic point cloud is filtered according to the target object category specified by the evaluation protocol. All clusters corresponding to the target class are extracted, and their 3D centroids are computed as candidate goal locations.

The geodesic shortest path is then computed from the robot’s initial pose to the nearest target
centroid using the global planner of the \ac{ROS2} Navigation2 stack~\cite{macenskiDesksROSMaintainers2023}. Path planning is performed on the 2D occupancy grid generated by SLAM Toolbox~\cite{macenskiSLAMToolboxSLAM2021}, ensuring consistency with the localization and navigation setup used during exploration. The resulting shortest path is compared to the executed trajectory to compute \ac{SR} and \ac{SPL} for each episode.

In contrast to the evaluation procedure of the Habitat Navigation Challenge~2023~\cite{puigHabitat30CoHabitat2023}, which relies on HabitatSim’s internal shortest-path computation, the proposed evaluation pipeline is fully compatible with IsaacSim and the employed \ac{ROS2}-based navigation stack. This enables a fair comparison against prior methods while preserving identical metric definitions and avoiding bias introduced by simulator-specific planners.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/04_implementation/evaluation_pipeline.png}
    \caption{Evaluation pipeline used to compute \ac{SR} and \ac{SPL}. Recorded RGB-D observations are converted into a semantic point cloud using OpenFusion, target object clusters are extracted, and the geodesic shortest path is computed via the \ac{ROS2} Navigation2 global planner for metric evaluation~\cite{macenskiDesksROSMaintainers2023,puigHabitat30CoHabitat2023}.}
    \label{fig:evaluation_pipeline}
\end{figure}

\section{Evaluation Metrics}
\label{sec:implementation:metrics}

This section defines the evaluation metrics used to assess the performance of the proposed semantic exploration framework and to ensure comparability with prior work in object-goal navigation. The selected metrics jointly evaluate task success, navigation efficiency, multi-object search capability, and detection robustness, thereby capturing both the quality of semantic reasoning and the reliability of perception-driven decision making. Each metric is explicitly assigned to a corresponding experiment, such that the experimental results directly address and answer the research questions defined in Chapter~\ref{ch:introduction}.

\subsection*{Experiment 1: Overall Performance Benchmarking}
\label{sec:implementation:metrics:experiment1}

The first experiment establishes a performance baseline by comparing the proposed hybrid semantic exploration framework against state-of-the-art methods on standard object-goal navigation metrics. This experiment addresses \textbf{RQ1} by quantifying the overall effectiveness of the integrated exploration-memory approach in realistic indoor environments. Results are compared against representative baselines including \ac{VLFM}~\cite{yokoyamaVLFMVisionLanguageFrontier2023}, \ac{VLMaps}~\cite{huangVisualLanguageMaps2023}, \ac{OneMap}~\cite{buschOneMapFind2025}, and Pigeon~\cite{pengPIGEONVLMDrivenObject2025}. Overall performance is evaluated using \ac{SR} and \ac{SPL}, which are widely adopted metrics in embodied \ac{AI} research~\cite{puigHabitat30CoHabitat2023} and adopted by comparable prior work~\cite{yokoyamaVLFMVisionLanguageFrontier2023,buschOneMapFind2025}.

In this experiment, an episode is defined as a single object-goal navigation task from the HM3Dv2 validation split, in which the robot is tasked with locating a specified target object within a Matterport3D-derived indoor scene~\cite{puigHabitat30CoHabitat2023}.

The \ac{SR} metric measures the proportion of successful episodes in which the robot reaches the queried target object and is defined in Equation~\ref{eq:evaluation:sr},

\begin{equation}
    \mathrm{SR}
    =
    \frac{1}{N}
    \sum_{i=1}^{N}
    S_i
    \label{eq:evaluation:sr}
\end{equation}

where \( S_i = 1 \) if the target object is successfully reached in episode \( i \), and \( 0 \) otherwise; \( N \) denotes the total number of evaluated episodes.

Navigation efficiency is quantified using \ac{SPL}, which compares the geodesic shortest path to the actual path executed by the robot. The metric penalizes unnecessary detours and is defined only for successful episodes in Equation~\ref{eq:evaluation:spl},

\begin{equation}
    \mathrm{SPL}
    =
    \frac{1}{N}
    \sum_{i=1}^{N}
    S_i \cdot \frac{l_i}{\max(p_i, l_i)}
    \label{eq:evaluation:spl}
\end{equation}

where \( l_i \) is the geodesic shortest path length to the target, \( p_i \) is the executed path length, and \( S_i \) indicates task success for episode \( i \).

An episode is considered successful, if the target object is visible in the RGB image captured by the robot’s onboard camera, as verified through post-hoc inspection of the recorded observations. The geodesic shortest path \( l_i \) is computed from the robot’s initial pose to the nearest instance of the target object. In contrast to HabitatSim-based evaluations, success is not restricted to predefined viewpoint tolerances, but instead includes any robot pose from which the target object is visually observable. All reported results were manually verified to confirm correct target visibility and episode termination. For every episode, the target object class is selected from the HM3Dv2 annotated category set~\cite{ramakrishnanHabitatMatterport3DDataset2021}.

\subsection*{Experiment 2: Exploration-Memory Fusion Weighting}
\label{sec:implementation:metrics:experiment2}

This experiment is designed to address \textbf{RQ2} by systematically analyzing how the balance between live semantic exploration and persistent 3D semantic memory influences navigation performance and behavioral stability. By varying the relative contribution of exploration-driven and memory-driven cues during graph node fusion, the experiment evaluates the impact of different fusion configurations on task success, navigation efficiency, and long-horizon search behavior. The primary objective is to identify a weighting configuration that provides an optimal trade-off between reactivity to newly observed information and stability derived from accumulated semantic memory.

The fusion behavior is controlled by the exploration-memory weighting parameter
\[
    \lambda_{\text{exploration}} \in [0,1],
\]
which determines the relative influence of frontier-based exploration and memory-driven exploitation during graph node fusion. A value of $\lambda_{\text{exploration}} = 0$ corresponds to purely memory-driven behavior, while $\lambda_{\text{exploration}} = 1$ corresponds to purely exploration-driven behavior. The parameter is varied in discrete increments of $0.2$. For each weighting configuration, multiple navigation episodes are executed, and performance is evaluated using the standard object-goal navigation metrics \ac{SR} and \ac{SPL} (see Equation~\ref{eq:evaluation:sr} and Equation~\ref{eq:evaluation:spl}).

To ensure consistency and reproducibility across configurations, Experiment~2 follows a fixed hierarchical evaluation protocol. Each episode is defined by a unique combination of scene, starting position, and exploration-memory weighting, and consists of multiple sequential object queries executed on a persistent semantic map without resetting the environment or clearing the accumulated memory.

\begin{table}[h!]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l c}
        \toprule
        \textbf{Evaluation Dimension} & \textbf{Configuration} \\
        \midrule
        Scenes & 6 Matterport3D scenes \\
        Starting positions per scene & 2 \\
        Multi-object episodes per starting position & 6 (one per weighting) \\
        Prompts per multi-object episode & 5 object queries \\
        Exploration-memory weight & \shortstack{Unique value per episode\\(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)} \\
        Semantic map reset & No (persistent across prompts) \\
        \bottomrule
    \end{tabular}
    \caption{Hierarchical evaluation structure used in Experiment~2. Each episode corresponds to a distinct exploration-memory weighting configuration and contains multiple sequential object queries executed on a persistent semantic map.}
    \label{tab:rq2:evaluation_structure}
\end{table}

The resulting evaluation scale for this experiment is summarized in Table~\ref{tab:rq2:evaluation_scale}. In total, 72 multi-object episodes are executed, yielding 345 individual object-level queries across all weighting configurations.

\begin{table}[h!]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l c}
        \toprule
        \textbf{Statistic} & \textbf{Value} \\
        \midrule
        Multi-object episodes & 72 \\
        Total object queries (prompts) & 345 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation scale for Experiment~2 (Exploration-Memory Fusion Weighting).}
    \label{tab:rq2:evaluation_scale}
\end{table}

In contrast to Experiment~1, which evaluates single-object navigation episodes, Experiment~2 employs multi-object search episodes to explicitly assess the benefit of persistent semantic memory across sequential object queries. Within a single episode, the robot is required to locate multiple target objects in sequence while maintaining a shared semantic map, reflecting realistic long-horizon search scenarios in which previously acquired semantic information can be reused to inform subsequent navigation decisions.

For evaluation, \ac{SR} and \ac{SPL} are computed on a per-target basis and subsequently aggregated across all episodes. Success for an individual target is defined identically to Experiment~1, and \ac{SPL} is computed using the geodesic shortest path from the robot’s pose at the start of each sub-task to the nearest instance of the corresponding target object.

For each episode, five target objects are sampled from the HM3Dv2 annotated category set~\cite{ramakrishnanHabitatMatterport3DDataset2021} and queried in a fixed random order. A global random seed of 42 is used to ensure reproducibility across different exploration-memory weighting configurations. All evaluations are conducted on the HM3Dv2 validation split, using six Matterport3D scenes with two distinct starting positions per scene.

\subsection*{Experiment 3: Sensitivity to Semantic Map Granularity}
\label{sec:implementation:metrics:experiment3}

This work employs a 3D semantic mapping approach based on OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}, which fuses \ac{VLM} embeddings into a volumetric representation. The granularity of semantic retrieval directly influences the quality of the resulting semantic map, with coarser retrievals introducing increased noise and ambiguity.

Granularity is controlled via the retrieval depth parameter top-k, which retrieves the top \( k \) most similar semantic regions from the \ac{SEEM}~\cite{zouSegmentEverythingEverywhere2023} embedding dictionary within OpenFusion~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}. Lower top-k values yield sharper but potentially incomplete semantic representations, while higher top-k values produce denser semantic maps at the cost of increased noise. The top-k parameter affects the semantic retrieval stage prior to fusion and thus indirectly controls the density of the semantic map~\cite{yamazakiOpenFusionRealtimeOpenVocabulary2023}.

To counteract noise introduced by higher top-k values, the exploration weight \( \lambda_{\text{exploration}} \) is increased across experimental configurations, shifting trust from memory-based semantic cues toward frontier-driven exploration. The top-k parameter is systematically varied from 5 to 25 in increments of 5. For each configuration, multiple navigation episodes are executed in the HM3Dv2 validation scenes, and performance is evaluated using \ac{SR} and \ac{SPL}.

To isolate the effect of semantic granularity, the starting pose is fixed across all runs. For each top-k value, two weighting configurations are evaluated:
(a) a memory-dominant setting (\( \lambda_{\text{exploration}} = 0 \)) and
(b) an exploration-dominant configuration using the optimal weighting identified
in Experiment~2 (RQ2). This design enables a direct assessment of whether increased reliance on frontier-based exploration can compensate for semantic noise introduced by higher top-k values.

The resulting evaluation scale for Experiment~3 is summarized in
Table~\ref{tab:rq3:evaluation_scale}.

\begin{table}[h!]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l c}
        \toprule
        \textbf{Statistic} & \textbf{Value} \\
        \midrule
        Multi-object episodes & 60 \\
        Total object queries (prompts) & 324 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation scale for Experiment~3 (Sensitivity to Semantic Map Granularity).}
    \label{tab:rq3:evaluation_scale}
\end{table}

\subsection*{Experiment 4: Evaluation of Multi-Source Detection Fusion}
\label{sec:implementation:metrics:experiment4}

This experiment evaluates the effectiveness of the multi-source semantic fusion strategy proposed in Chapter~\ref{ch:methods} Section~\ref{sec:methods:fusion_strategy} in fusing with a Noisy-OR model to improve detection robustness and suppress false positives during exploration, by increasing the detection thresholds, thereby addressing \textbf{RQ3}. Following prior work on open-vocabulary semantic perception, detection robustness is evaluated using confusion-matrix-based metrics such as precision, recall and F1-score, which are commonly employed to assess semantic retrieval quality in open-vocabulary settings~\cite{schwaigerOTASOpenvocabularyToken2025,wangYOLOERealTimeSeeing2025,redmonYouOnlyLook2016}

Each episode is manually annotated based on the executed navigation trajectory, the recorded YOLO-E detection overlay, and system logs, with a semantic outcome describing whether the robot correctly reached the target object, detected the wrong object, or observed the target but ignored it. Failure modes that are unrelated to the detection process itself, such as exploration timeouts, lack of motion, or insufficient proximity to the target, are excluded from
the detection evaluation and treated as neutral cases.

\begin{table}[H]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{
        c|
        >{\centering\arraybackslash}p{3.0cm}
        >{\centering\arraybackslash}p{6.0cm}
    }
    \toprule
    \textbf{Label}
    & \textbf{Condition}
    & \textbf{Interpretation} \\
    \midrule

    TP
    & Successful target reach with \( S \geq \tau \)
    & Correct detection and navigation to the target object. \\

    FP
    & Object mismatch with \( S \geq \tau \)
    & Robot selects a visually or semantically similar but incorrect object. \\

    FN
    & Threshold rejection
    & Target object is observed but not selected because all detection scores remain below \( \tau \). \\

    TN
    & Target not observed (timeout / no motion)
    & The target object was never within the camera field of view during the episode (e.g., exploration timeout or no motion). Consequently, no positive detection decision is triggered and no false positive occurs. \\

    \bottomrule
    \end{tabular}
    \caption{Abstracted confusion-matrix mapping for detection robustness evaluation.}
\end{table}

Four fusion strategies are evaluated: (a) detector-only (YOLO-E), (b) detector + semantic similarity (BLIP-2), (c) detector + memory confidence, and (d) the full Noisy-OR fusion strategy combining all three sources. All fusion strategies must contain the detector source, as the other components are complementary and cannot independently trigger detections, only reinforce them. Across each fusion strategy, all experiments are executed using a fixed detection threshold of \( \tau = 0.8 \), which defines the robot’s online decision-making behaviour and serves as the basis for manual annotation. The resulting detection confidences are then reused in a post-hoc evaluation to compute confusion matrices and classification metrics for additional thresholds \( \tau = 0.5 \) and \( 0.6 \).

Precision in this context measures the proportion of correct detections among all positive detections made by the robot and is defined in Equation~\ref{eq:evaluation:precision}:

\begin{equation}
    \mathrm{Precision}
    =
    \frac{TP}{TP + FP}
    \label{eq:evaluation:precision}
\end{equation}

where \( TP \) denotes true positives, meaning the object was correctly detected, and \( FP \) denotes false positives, meaning an incorrect object was detected. In this context, precision quantifies the system’s ability to avoid incorrect detections when making a positive identification of the target object. Recall measures the proportion of actual target objects that were correctly detected and is defined in Equation~\ref{eq:evaluation:recall},

\begin{equation}
    \mathrm{Recall}
    =
    \frac{TP}{TP + FN}
    \label{eq:evaluation:recall}
\end{equation}

where \( FN \) denotes false negatives, meaning the target object was observed but not selected because all detection confidences remained below the decision threshold \( \tau \). In this context, recall quantifies the system’s sensitivity to correctly identifying the target object when it is present in the environment. Importantly, false negatives in this experiment exclusively arise from conservative thresholding decisions and do not indicate a failure of the perception pipeline to observe the target object. The F1-score is the harmonic mean of precision and recall, providing a balanced measure of detection performance, and is defined in Equation~\ref{eq:evaluation:f1}.

\begin{equation}
    \mathrm{F1}
    =
    2 \cdot
    \frac{\mathrm{Precision} \cdot \mathrm{Recall}}
         {\mathrm{Precision} + \mathrm{Recall}}
    \label{eq:evaluation:f1}
\end{equation}

A high F1-score indicates that the system effectively balances precision and recall, achieving both accurate and sensitive detection of the target object. The \ac{FPR} quantifies the proportion of incorrect detections among all negative cases and is defined in Equation~\ref{eq:evaluation:fpr},

\begin{equation}
    \mathrm{FPR}
    =
    \frac{FP}{FP + TN}
    \label{eq:evaluation:fpr}
\end{equation}

where \( TN \) denotes true negatives, corresponding to episodes in which no detection decision
is evaluated due to exploration-related failure modes or insufficient observability. \( TN \) corresponds to neutral episodes in which no detection decision was made and is included solely for the computation of the false-positive rate. Note that $TN$ is defined at the episode level and corresponds to cases without target observability; thus, the reported $FPR$ should be interpreted as the false-commit rate under non-observability rather than a frame-level false positive rate.

The confusion matrix for single-source and multi-source fusion is primarily reported for the operational threshold \( \tau = 0.8 \), which was empirically selected to provide a balanced trade-off between sensitivity and specificity in preliminary tests. In addition, precision–recall curves are generated by sweeping the decision threshold \( \tau \) from 0 to 1 in increments of 0.01 using the same annotated episodes. This threshold sweep enables an analysis of detection robustness across different operating points without requiring additional experimental runs.

\subsection*{Experiment 5: Real-World System Performance}

This experiment defines a deployment-oriented evaluation protocol for assessing the real-world performance of the proposed semantic exploration framework on a physical mobile robot platform. The objective is to characterize system robustness, efficiency, and deployability in realistic indoor environments, thereby addressing \textbf{RQ5}.

The evaluation focuses on monitoring computational resource utilization, including CPU, GPU, and RAM usage, as well as inference latency and effective frame rate (FPS) of the perception and mapping components during live operation. Each component’s computational load is logged to enable fine-grained performance analysis.

In addition, the semantic map voxel resolution in OpenFusion is systematically reduced to analyze the trade-off between GPU memory consumption and runtime performance. This analysis is intended to assess the feasibility of deploying the system on resource-constrained robotic platforms.

\newpage