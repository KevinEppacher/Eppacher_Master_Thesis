% chktex-file 44

\chapter{Implementation}
\label{ch:implementation}
This section details the practical implementation of the proposed approach, covering the simulation and real-world setup, datasets, software stack, and hardware configuration.

\section{Simulation Environment}
\begin{itemize}
    \item Evaluation of simulation frameworks for indoor semantic navigation:
    \begin{itemize}
        \item HabitatSim: Realistic Matterport3D-based environments with semantic annotations.
        \item Isaac Sim / Isaac Lab: GPU-accelerated simulation, advanced physics, support for RTX ray tracing.
        \item MuJoCo: High-speed physics engine, limited support for complex indoor scenes.
        \item Ignition Gazebo: Modular simulator, ROS2 integration, good for real-robot transfer.
    \end{itemize}
    \item \dots
\end{itemize}

\section{Dataset}
\begin{itemize}
    \item Use of \textbf{Matterport3D} scenes for realistic indoor environments with ground truth 3D reconstruction and semantic annotations.
    \item While the Habitat Navigation Challenge 2023 defines Success Rate (SR) and Success weighted by Path Length (SPL) as standard evaluation metrics within the Habitat-Sim environment, this work extends their application to Isaac Sim.
Using Isaac Sim allows for a more physically accurate and sensor-consistent setup, incorporating realistic depth noise, lighting variation, and robot dynamics.
To ensure comparability, SR and SPL are calculated following the official Habitat definitions, maintaining consistency with prior benchmarks while improving the realism of scene interaction and perception.
\end{itemize}

\section{Used Software}
\begin{itemize}
    \item ROS2-based implementation (Humble Hawksbill) as middleware.
    \item Navigation stack: Navigation2 (Nav2) for frontier-based exploration and path planning.
    \item DDS communication layer for distributed communication between detection, mapping, and control nodes.
    \item Integration of promptable models (OpenFusion, BLIP-2, YOLO-E) for real-time zero-shot detection during exploration and exploitation.
\end{itemize}

\section{Used Hardware}
\begin{itemize}
    \item \textbf{PC:}
    \begin{itemize}
        \item CPU: AMD Ryzen 9 5950X 16-Core Processor
        \item Motherboard: B550 Gaming X V2
        \item GPU: ASUS TUF Gaming RTX 4090 24GB OC Edition
        \item RAM: 64GB Corsair Vengeance LPX DDR4
    \end{itemize}
    \item \textbf{Real Robot:} Configuration and components to be determined (TurtleBot Waffle).
\end{itemize}

\section{Evaluation Metrics}
This section defines the evaluation metrics used throughout the experiments and assigns them to each corresponding experiment.

\textbf{Evaluation Pipeline Overview}

\begin{itemize}
    \item \textbf{Semantic Map Generation:} OpenFusion performs semantic segmentation of RGB-D input using the Matterport3D class list. Each segment is assigned its most likely class label from the detection model.
    
    \item \textbf{Manual Correction:} Incorrectly labeled segments can be manually relabeled within a dedicated ROS~2 node for semantic correction.
    
    \item \textbf{Data Storage:} OpenFusion saves both the 3D semantic pointcloud and the corresponding 2D SLAM map for each episode. All experiment data follows the \texttt{sage\_datasets/matterport\_isaac} directory structure.
    
    \item \textbf{Evaluation Initialization:} During evaluation, the saved maps and class definitions are loaded together with a list of target objects (e.g., ``bed'', ``toilet'', ``chair'').
    
    \item \textbf{Class Filtering and Centroid Extraction:} The evaluator node filters the semantic pointcloud according to the target classes and extracts the 3D centroids of matching clusters.
    
    \item \textbf{Path Planning:} The shortest-path planner computes the geodesic-optimal path from the robot’s current pose to the nearest centroid of the selected target class, with the Global Path Planner from ROS2 Navigation2.
    
    \item \textbf{Metric Computation:} The evaluator node compares the planned and executed trajectories to compute Success Rate (SR), Success weighted by Path Length (SPL), and Multi-Object Success Rate (MSR).
    
    \item \textbf{Result Storage:} Evaluation metrics, trajectories, and intermediate results are stored per episode for analysis and benchmarking.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/04_implementation/evaluation_pipeline.png}
    \caption{Evaluation pipeline for benchmarking SR, SPL, and MSR in the semantic exploration framework.}
    \label{fig:evaluation_pipeline}
\end{figure}

\subsection*{Experiment 1: Overall Performance Benchmarking}

\begin{itemize}
    \item Compare SR, SPL and MSR across different baseline methods and the proposed hybrid approach.
    \item Baselines include:
    \begin{itemize}
        \item \ac{VLFM} \cite{shah2023vlm},
        \item \ac{VLMaps} \cite{liu2023vlmaps},
        \item \ac{OneMap} \cite{yang20233don},
        \item \ac{Pigeon} \cite{qi2023pigeon}.
    \end{itemize}
    \item Every scene within the ObjectNav HM3D v2 validation split:
    % \item (wget https://dl.fbaipublicfiles.com/habitat/data/datasets/objectnav/hm3d/v2/objectnav_hm3d_v2.zip)
    \item Within this dataset, each scene contains a set of episodes with the starting pose and target object specified.
    \item Due to the custom nature of the Isaac Sim environment, all baselines are re-implemented to ensure fair comparison under similar conditions.
    \item Limitations with IsaacSims Environment setup:
    \begin{itemize}
        \item Starting Pose Variability
        \item Amount of episodes per scene (5 per scene per floor with each 5 sub-episodes for multi-object search)
        \item Different requirements:
        \begin{itemize}
            \item IsaacSim: Photorealistic rendering, physics simulation, realistic sensor noise
            \item HabitatSim: Optimized for fast navigation and large-scale datasets
        \end{itemize}
    \end{itemize}
\end{itemize}

\begin{itemize}
    \item \textbf{Success Rate (SR):}  
    Measures the proportion of tasks in which the robot successfully reaches the queried single goal object. This metric reflects the system’s ability to semantically ground a user-specified object and to navigate toward it reliably. It serves as a fundamental indicator of task success and is essential for evaluating overall system effectiveness in basic search scenarios.
    \textit{Evaluation against:} \ac{VLFM}, \ac{VLMaps}, \ac{OneMap}, \ac{GeFF}

    \[
    \text{SR} = \frac{1}{N} \sum_{i=1}^{N} S_i
    \]
    where \( S_i = 1 \) if the goal was reached in episode \( i \), and \( 0 \) otherwise; \( N \) is the total number of episodes.


    \item \textbf{Path Efficiency (SPL):}  
    SPL measures the efficiency of successful navigation by comparing the shortest possible path to the actual path taken. It is defined only for successful runs and penalizes overly long trajectories. In the context of semantic exploration, SPL provides insight into how effectively the system prioritizes relevant regions and minimizes detours when searching for target objects.

    \[
    \text{SPL} = \frac{1}{N} \sum_{i=1}^{N} S_i \cdot \frac{l_i}{\max(p_i, l_i)}
    \]
    where \( S_i \) is the success indicator for episode \( i \), \( l_i \) is the shortest path length to the goal, \( p_i \) is the actual path length taken, and \( N \) is the total number of episodes.

    \item \textbf{Multi-Object Success Rate (MSR):}  
    The average number of successfully found objects per episode (\textit{Progress, PR}) captures partial success in multi-goal navigation. SPL is computed separately for each object in sequence, conditioned on the success of the previous one. This highlights the system’s ability to reuse semantic map information and improve efficiency across successive targets.

    \[
    \text{PR} = \frac{1}{N} \sum_{i=1}^{N} C_i
    \]
    where \( C_i \) is the number of successfully found objects in episode \( i \), and \( N \) is the total number of episodes.

\end{itemize}

\subsection*{Experiment 2: Exploration--Memory Fusion Weighting}

\begin{itemize}

    \item \textbf{Objective:}  
    Evaluate how varying the weighting between live semantic exploration and persistent 3D semantic memory influences navigation performance, stability, and overall search efficiency in the hybrid framework.

    \item \textbf{Fusion Parameter:}  
    The trade-off between exploration and memory is controlled through a scalar weighting parameter
    \[
        \lambda_{\text{exploration}} \in [0, 1],
    \]
    which determines the relative influence of frontier-based exploration versus memory-driven exploitation during graph node fusion.

    \item \textbf{Research Questions:}
    \begin{itemize}
        \item \textbf{RQ2a:} How do Success Rate (SR) and Success weighted by Path Length (SPL) vary as the weighting shifts from exploration (\(\lambda \rightarrow 1\)) toward memory-driven behavior (\(\lambda \rightarrow 0\))?
        \item \textbf{RQ2b:} Which weighting configuration yields the best trade-off between reactivity (fast adaptation to newly observed information) and stability (robust semantic localization using persistent memory)?
    \end{itemize}

    \item \textbf{Evaluation Procedure:}
    \begin{itemize}
        \item Multiple runs are conducted across a range of \(\lambda_{\text{exploration}}\) values.
        \item Performance is measured using the metrics SR and SPL.
    \end{itemize}

    \item \textbf{Expected Outcome:}  
    This experiment highlights whether hybrid fusion provides measurable benefits over purely exploration-driven or purely memory-driven behavior, and identifies the optimal balance for multi-object search tasks.

\end{itemize}

\subsection*{Experiment 3: Sensitivity to Semantic Map Granularity}

\begin{itemize}

    \item \textbf{Objective:}  
    Investigate how the granularity of semantic retrieval in the 3D semantic mapper affects global map quality, navigation performance, and robustness of the hybrid exploration system.  
    Specifically, this experiment evaluates whether dynamic rebalancing between exploration and memory can compensate for increased semantic noise introduced by higher retrieval depths.

    \item \textbf{Semantic Granularity Parameter:}  
    Semantic map quality is controlled through the retrieval depth \emph{top-k}, which specifies how many semantic candidates (from the VLM embedding space) are fused into each voxel:
    \begin{itemize}
        \item Low \emph{top-k}: sharper but potentially incomplete semantics.  
        \item High \emph{top-k}: denser semantics but increased noise and ambiguity.
    \end{itemize}

    \item \textbf{Dynamic Fusion Weighting:}  
    To counteract noise introduced by larger \emph{top-k} values, the exploration weight  
    \[
        \lambda_{\text{exploration}}
    \]
    is progressively increased, shifting trust toward frontier-driven exploration and away from noisy memory components.

    \item \textbf{Research Questions:}
    \begin{itemize}
        \item \textbf{RQ4a:} How do Success Rate (SR) and Success weighted by Path Length (SPL) degrade as \emph{top-k} increases while relying primarily on memory?
        \item \textbf{RQ4b:} Can adaptive rebalancing toward exploration (i.e., increasing \( \lambda_{\text{exploration}} \)) restore stable performance at higher \emph{top-k} values?
    \end{itemize}

    \item \textbf{Evaluation Metrics:}
    \begin{itemize}
        \item SR: ability to consistently reach goal objects under different semantic retrieval granularities.
        \item SPL: navigation efficiency and the influence of semantic noise on path quality.
        \item Additional qualitative assessment of map sharpness, cluster correctness, and temporal stability of semantic memory.
    \end{itemize}

    \item \textbf{Evaluation Procedure:}
    \begin{itemize}
        \item Generate semantic maps at multiple \emph{top-k} levels (e.g., 1, 3, 5, 10).
        \item For each \emph{top-k}:
        \begin{itemize}
            \item Evaluate SR and SPL under memory-dominant settings.
            \item Incrementally increase \( \lambda_{\text{exploration}} \) and re-evaluate.
        \end{itemize}
        \item Compare results to determine:
        \begin{itemize}
            \item tolerance of the system to semantic noise,
            \item optimal balance between exploration and memory at different granularity levels,
            \item interaction effects between map resolution and fusion stability.
        \end{itemize}
    \end{itemize}

    \item \textbf{Purpose:}  
    This experiment analyzes the coupling between semantic map granularity and the stability of exploration–memory fusion.  
    Results reveal how coarse or noisy semantic retrieval affects overall navigation and whether adaptive weighting can maintain robust performance in open-vocabulary mapping environments.

\end{itemize}


\subsection*{Experiment 4: Robustness to False Positives Through Multi-Source Detection Fusion}

\begin{itemize}

    \item \textbf{Objective:}  
    Evaluate how combining multiple semantic evidence sources, instance detection (YOLO-E), semantic similarity scoring (BLIP-2), and memory confidence from the 3D semantic map, improves detection robustness and suppresses false positives during exploration.

    \item \textbf{Fusion Model (Weighted Noisy-OR):}  
    The proposed fusion strategy follows a weighted Noisy-OR formulation, in which independent semantic evidence sources jointly increase the probability of a valid detection:
    \[
        S_{\text{fusion}}
        = 1 - (1 - w_d S_{\text{det}})
            (1 - w_c S_{\text{map}})
            (1 - w_m S_{\text{mem}}).
    \]
    Here,
    \begin{itemize}
        \item \( S_{\text{det}} \): YOLO-E detector confidence,
        \item \( S_{\text{map}} \): similarity score from the value map (BLIP-2),
        \item \( S_{\text{mem}} \): confidence from persistent 3D semantic memory,
        \item \( w_d, w_c, w_m \): weights defining the contribution of each source.
    \end{itemize}
    This formulation ensures that high confidence from any source can compensate for uncertainty in others while suppressing spurious detections that lack multi-source agreement.

    \item \textbf{Research Questions:}
    \begin{itemize}
        \item \textbf{RQ3a:} How does overall task performance (SR) change under different weight configurations \( (w_d, w_c, w_m) \)?
        \item \textbf{RQ3b:} How do precision, recall, F1-score, and false-positive rate vary across:
        \begin{itemize}
            \item COCO-style closed-set categories,
            \item open-vocabulary object classes,
            \item zero-shot categories not seen during detector training?
        \end{itemize}
        \item \textbf{RQ3c:} What drawbacks arise when detection thresholds are increased or when a single evidence source is overemphasised  
        (e.g., memory bias, detector hallucination, missed low-confidence but valid detections)?
    \end{itemize}

    \item \textbf{Evaluation Metrics:}
    Robustness is quantified using classification metrics derived from the confusion matrix:
    \[
        \text{Precision} = \frac{TP}{TP + FP}, \qquad
        \text{Recall} = \frac{TP}{TP + FN},
    \]
    \[
        \text{F1} = 2 \cdot 
        \frac{\text{Precision} \cdot \text{Recall}}
             {\text{Precision} + \text{Recall}},
    \]
    \[
        \text{FPR} = \frac{FP}{FP + TN}.
    \]
    Additionally, downstream Success Rate (SR) is recorded for each weight triplet \( (w_d, w_c, w_m) \) to evaluate the effect of false positives on the overall navigation pipeline.

    \item \textbf{Evaluation Procedure:}
    \begin{itemize}
        \item Evaluate a range of weight combinations \( (w_d, w_c, w_m) \) spanning detector-dominant, map-dominant, memory-dominant, and balanced regimes, whereas the detection source is mandatory (i.e., \( w_d > 0 \)).
        \item Compare against single-source baselines:
        \begin{itemize}
            \item detector-only (YOLO-E),
            \item similarity-only (BLIP-2),
            \item memory-only retrieval,
            \item the full Noisy-OR fusion strategy.
        \end{itemize}
        \item Analyse outcomes under:
        \begin{itemize}
            \item closed-set (COCO) categories,
            \item open-vocabulary targets,
            \item zero-shot targets.
        \end{itemize}
        \item Quantify how false positives propagate into:
        \begin{itemize}
            \item erroneous graph node generation,
            \item unnecessary navigation actions,
            \item degraded SR and SPL.
        \end{itemize}
    \end{itemize}

    \item \textbf{Purpose:}  
    This experiment evaluates whether multi-source, Noisy-OR-based semantic fusion provides a measurable improvement in detection robustness and false-positive suppression compared to single-source methods, thereby enabling more reliable semantic exploration in open-vocabulary indoor environments.

\end{itemize}



\subsection*{Experiment 5: Real-World System Performance:}  
\begin{itemize}
    \item SR, MSR and SP for search performance under real-world conditions.
    \item System metrics: CPU/GPU usage, FPS, inference latency.
\end{itemize}
\textit{Objective: Assess robustness, efficiency, and deployability in physical environments.}