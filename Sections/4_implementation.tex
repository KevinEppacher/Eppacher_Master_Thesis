\chapter{Implementation}
This section details the practical implementation of the proposed approach, covering the simulation and real-world setup, datasets, software stack, and hardware configuration.

\section{Simulation Environment}
\begin{itemize}
    \item Evaluation of simulation frameworks for indoor semantic navigation:
    \begin{itemize}
        \item HabitatSim: Realistic Matterport3D-based environments with semantic annotations.
        \item Isaac Sim / Isaac Lab: GPU-accelerated simulation, advanced physics, support for RTX ray tracing.
        \item MuJoCo: High-speed physics engine, limited support for complex indoor scenes.
        \item Ignition Gazebo: Modular simulator, ROS2 integration, good for real-robot transfer.
    \end{itemize}
    \item \dots
\end{itemize}

\section{Dataset}
\begin{itemize}
    \item Use of \textbf{Matterport3D} scenes for realistic indoor environments with ground truth 3D reconstruction and semantic annotations.
    \item Incorporation of the \textbf{Habitat Navigation Challenge 2023} tasks to benchmark exploration and navigation performance (SR, SPL).
\end{itemize}

\section{Used Software}
\begin{itemize}
    \item ROS2-based implementation (Humble Hawksbill) as middleware.
    \item Navigation stack: Navigation2 (Nav2) for frontier-based exploration and path planning.
    \item DDS communication layer for distributed communication between detection, mapping, and control nodes.
    \item Custom RobotDriver for interfacing with real robot hardware.
    \item Integration of promptable models (SEEM, GroundingDINO, etc.) for real-time zero-shot detection during exploration and exploitation.
    \item \dots
\end{itemize}

\section{Used Hardware}
\begin{itemize}
    \item \textbf{PC:}
    \begin{itemize}
        \item CPU: AMD Ryzen 9 5950X 16-Core Processor
        \item Motherboard: B550 Gaming X V2
        \item GPU: ASUS TUF Gaming RTX 4090 24GB OC Edition
        \item RAM: 64GB Corsair Vengeance LPX DDR4
    \end{itemize}
    \item \textbf{Real Robot:} Configuration and components to be determined (tbd).
\end{itemize}

\section{Evaluation Metrics}
This section defines the evaluation metrics used throughout the experiments and assigns them to each corresponding experiment.

\begin{itemize}
    \item \textbf{Experiment 1 – Success Rate (SR):}  
    Ratio of successfully reached single goal objects.  
    \textit{Evaluation against:} \ac{VLFM}, \ac{VLMaps}, \ac{OneMap}, \ac{GeFF}

    \item \textbf{Experiment 2 – Path Efficiency (SPL):}  
    Success weighted by path length (SPL).  
    \textit{Used to assess navigational efficiency of successful trajectories.}

    \item \textbf{Experiment 3 – Multi-Object Success Rate (MSR):}  
    Percentage of tasks where all queried objects are found.  
    \textit{Important for evaluating real-world multi-object search performance.}

    \item \textbf{Experiment 4 – Ablation: Memory Component (OpenFusion):}  
    MSR change with vs. without OpenFusion (i.e. only \ac{VLFM}).  
    \textit{Purpose: Understand OpenFusion’s contribution in the hybrid setup.}

    \item \textbf{Experiment 5 – Robustness to False Positives (Fusion Strategy):}  
    \textbf{Metrics:} Semantic Precision and False Positive Rate.  
    \textit{Description: Evaluates how fusion between \ac{SEEM} and OpenFusion mitigates false detections under occlusion and ambiguous inputs.}

    \item \textbf{Experiment 6 – Real-World System Performance:}  
    \textbf{Metrics:}
    \begin{itemize}
        \item SR, MSR, SPL – for search performance under real-world conditions.
        \item System metrics – CPU/GPU usage, FPS, inference latency.
    \end{itemize}
    \textit{Objective: Assess robustness, efficiency, and deployability in physical environments.}

    \item \textbf{Experiment 7 – Vision-Language Model Comparison (SEEM vs. BLIP2):}  
    \textbf{Metrics:} Cosine Similarity Consistency, SR, SPL, Memory Footprint.  
    \textit{Purpose: Compare SEEM and BLIP2 in the context of frontier scoring and value map generation.}
\end{itemize}